{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Rollouts - Kubernetes Progressive Delivery Controller \u00b6 What is Argo Rollouts? \u00b6 Argo Rollouts is a Kubernetes controller and set of CRDs which provide advanced deployment capabilities such as blue-green, canary, canary analysis, experimentation, and progressive delivery features to Kubernetes. Argo Rollouts (optionally) integrates with ingress controllers and service meshes, leveraging their traffic shaping abilities to gradually shift traffic to the new version during an update. Additionally, Rollouts can query and interpret metrics from various providers to verify key KPIs and drive automated promotion or rollback during an update. Here is a demonstration video (click to watch on Youtube): Why Argo Rollouts? \u00b6 The native Kubernetes Deployment Object supports the RollingUpdate strategy which provides a basic set of safety guarantees (readiness probes) during an update. However the rolling update strategy faces many limitations: Few controls over the speed of the rollout Inability to control traffic flow to the new version Readiness probes are unsuitable for deeper, stress, or one-time checks No ability to query external metrics to verify an update Can halt the progression, but unable to automatically abort and rollback the update For these reasons, in large scale high-volume production environments, a rolling update is often considered too risky of an update procedure since it provides no control over the blast radius, may rollout too aggressively, and provides no automated rollback upon failures. Controller Features \u00b6 Blue-Green update strategy Canary update strategy Fine-grained, weighted traffic shifting Automated rollbacks and promotions Manual judgement Customizable metric queries and analysis of business KPIs Ingress controller integration: NGINX, ALB Service Mesh integration: Istio, Linkerd, SMI Simultaneous usage of multiple providers: SMI + NGINX, Istio + ALB, etc. Metric provider integration: Prometheus, Wavefront, Kayenta, Web, Kubernetes Jobs, Datadog, New Relic, Graphite Quick Start \u00b6 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml Follow the full getting started guide to walk through creating and then updating a rollout object. How does it work? \u00b6 Similar to the deployment object , the Argo Rollouts controller will manage the creation, scaling, and deletion of ReplicaSets . These ReplicaSets are defined by the spec.template field inside the Rollout resource, which uses the same pod template as the deployment object. When the spec.template is changed, that signals to the Argo Rollouts controller that a new ReplicaSet will be introduced. The controller will use the strategy set within the spec.strategy field in order to determine how the rollout will progress from the old ReplicaSet to the new ReplicaSet. Once that new ReplicaSet is scaled up (and optionally passes an Analysis ), the controller will mark it as \"stable\". If another change occurs in the spec.template during a transition from a stable ReplicaSet to a new ReplicaSet (i.e. you change the application version in the middle of a rollout), then the previously new ReplicaSet will be scaled down, and the controller will try to progress the ReplicasSet that reflects the updated spec.template field. There is more information on the behaviors of each strategy in the spec section. Use cases of Argo Rollouts \u00b6 A user wants to run last-minute functional tests on the new version before it starts to serve production traffic. With the BlueGreen strategy, Argo Rollouts allows users to specify a preview service and an active service. The Rollout will configure the preview service to send traffic to the new version while the active service continues to receive production traffic. Once a user is satisfied, they can promote the preview service to be the new active service. ( example ) Before a new version starts receiving live traffic, a generic set of steps need to be executed beforehand. With the BlueGreen Strategy, the user can bring up the new version without it receiving traffic from the active service. Once those steps finish executing, the rollout can cut over traffic to the new version. A user wants to give a small percentage of the production traffic to a new version of their application for a couple of hours. Afterward, they want to scale down the new version and look at some metrics to determine if the new version is performant compared to the old version. Then they will decide if they want to roll out the new version for all of the production traffic or stick with the current version. With the canary strategy, the rollout can scale up a ReplicaSet with the new version to receive a specified percentage of traffic, wait for a specified amount of time, set the percentage back to 0, and then wait to rollout out to service all of the traffic once the user is satisfied. ( example ) A user wants to slowly give the new version more production traffic. They start by giving it a small percentage of the live traffic and wait a while before giving the new version more traffic. Eventually, the new version will receive all the production traffic. With the canary strategy, the user specifies the percentages they want the new version to receive and the amount of time to wait between percentages. ( example ) A user wants to use the normal Rolling Update strategy from the deployment. If a user uses the canary strategy with no steps, the rollout will use the max surge and max unavailable values to roll to the new version. ( example ) Examples \u00b6 You can see more examples of Rollouts at: The example directory The Argo Rollouts Demo application","title":"Overview"},{"location":"#argo-rollouts-kubernetes-progressive-delivery-controller","text":"","title":"Argo Rollouts - Kubernetes Progressive Delivery Controller"},{"location":"#what-is-argo-rollouts","text":"Argo Rollouts is a Kubernetes controller and set of CRDs which provide advanced deployment capabilities such as blue-green, canary, canary analysis, experimentation, and progressive delivery features to Kubernetes. Argo Rollouts (optionally) integrates with ingress controllers and service meshes, leveraging their traffic shaping abilities to gradually shift traffic to the new version during an update. Additionally, Rollouts can query and interpret metrics from various providers to verify key KPIs and drive automated promotion or rollback during an update. Here is a demonstration video (click to watch on Youtube):","title":"What is Argo Rollouts?"},{"location":"#why-argo-rollouts","text":"The native Kubernetes Deployment Object supports the RollingUpdate strategy which provides a basic set of safety guarantees (readiness probes) during an update. However the rolling update strategy faces many limitations: Few controls over the speed of the rollout Inability to control traffic flow to the new version Readiness probes are unsuitable for deeper, stress, or one-time checks No ability to query external metrics to verify an update Can halt the progression, but unable to automatically abort and rollback the update For these reasons, in large scale high-volume production environments, a rolling update is often considered too risky of an update procedure since it provides no control over the blast radius, may rollout too aggressively, and provides no automated rollback upon failures.","title":"Why Argo Rollouts?"},{"location":"#controller-features","text":"Blue-Green update strategy Canary update strategy Fine-grained, weighted traffic shifting Automated rollbacks and promotions Manual judgement Customizable metric queries and analysis of business KPIs Ingress controller integration: NGINX, ALB Service Mesh integration: Istio, Linkerd, SMI Simultaneous usage of multiple providers: SMI + NGINX, Istio + ALB, etc. Metric provider integration: Prometheus, Wavefront, Kayenta, Web, Kubernetes Jobs, Datadog, New Relic, Graphite","title":"Controller Features"},{"location":"#quick-start","text":"kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml Follow the full getting started guide to walk through creating and then updating a rollout object.","title":"Quick Start"},{"location":"#how-does-it-work","text":"Similar to the deployment object , the Argo Rollouts controller will manage the creation, scaling, and deletion of ReplicaSets . These ReplicaSets are defined by the spec.template field inside the Rollout resource, which uses the same pod template as the deployment object. When the spec.template is changed, that signals to the Argo Rollouts controller that a new ReplicaSet will be introduced. The controller will use the strategy set within the spec.strategy field in order to determine how the rollout will progress from the old ReplicaSet to the new ReplicaSet. Once that new ReplicaSet is scaled up (and optionally passes an Analysis ), the controller will mark it as \"stable\". If another change occurs in the spec.template during a transition from a stable ReplicaSet to a new ReplicaSet (i.e. you change the application version in the middle of a rollout), then the previously new ReplicaSet will be scaled down, and the controller will try to progress the ReplicasSet that reflects the updated spec.template field. There is more information on the behaviors of each strategy in the spec section.","title":"How does it work?"},{"location":"#use-cases-of-argo-rollouts","text":"A user wants to run last-minute functional tests on the new version before it starts to serve production traffic. With the BlueGreen strategy, Argo Rollouts allows users to specify a preview service and an active service. The Rollout will configure the preview service to send traffic to the new version while the active service continues to receive production traffic. Once a user is satisfied, they can promote the preview service to be the new active service. ( example ) Before a new version starts receiving live traffic, a generic set of steps need to be executed beforehand. With the BlueGreen Strategy, the user can bring up the new version without it receiving traffic from the active service. Once those steps finish executing, the rollout can cut over traffic to the new version. A user wants to give a small percentage of the production traffic to a new version of their application for a couple of hours. Afterward, they want to scale down the new version and look at some metrics to determine if the new version is performant compared to the old version. Then they will decide if they want to roll out the new version for all of the production traffic or stick with the current version. With the canary strategy, the rollout can scale up a ReplicaSet with the new version to receive a specified percentage of traffic, wait for a specified amount of time, set the percentage back to 0, and then wait to rollout out to service all of the traffic once the user is satisfied. ( example ) A user wants to slowly give the new version more production traffic. They start by giving it a small percentage of the live traffic and wait a while before giving the new version more traffic. Eventually, the new version will receive all the production traffic. With the canary strategy, the user specifies the percentages they want the new version to receive and the amount of time to wait between percentages. ( example ) A user wants to use the normal Rolling Update strategy from the deployment. If a user uses the canary strategy with no steps, the rollout will use the max surge and max unavailable values to roll to the new version. ( example )","title":"Use cases of Argo Rollouts"},{"location":"#examples","text":"You can see more examples of Rollouts at: The example directory The Argo Rollouts Demo application","title":"Examples"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 Before You Start \u00b6 Argo Rollouts is written in Golang. If you do not have a good grounding in Go, try out the tutorial . Pre-requisites \u00b6 Install: docker golang kubectl kustomize minikube or Docker for Desktop Kustomize is required for unit tests ( make test is using it), so you must install it locally if you wish to make code contributions to Argo Rollouts. Argo Rollout additionally uses the following tools golangci-lint to lint the project. protoc and swagger-codegen to generate proto related files yarn to build the UI Run the following commands to install them: # macOS brew install golangci-lint # linux go get -u github.com/golangci/golangci-lint/cmd/golangci-lint Brew users can quickly install the lot: brew install go kubectl kustomize golangci-lint protobuf swagger-codegen Set up environment variables (e.g. is ~/.bashrc ): export GOPATH = ~/go export PATH = $PATH : $GOPATH /bin Checkout the code: go get -u github.com/argoproj/argo-rollouts cd ~/go/src/github.com/argoproj/argo-rollouts Building \u00b6 go.mod is used, so the go build/test commands automatically install the needed dependencies The make controller command will build the controller. make codegen - Runs the code generator that creates the informers, client, lister, and deepcopies from the types.go and modifies the open-api spec. Running Controller Locally \u00b6 It is much easier to run and debug if you run Argo Rollout in your local machine than in the Kubernetes cluster. cd ~/go/src/github.com/argoproj/argo-rollouts go run ./cmd/rollouts-controller/main.go Running Unit Tests \u00b6 To run unit tests: make test Running E2E tests \u00b6 The end-to-end tests need to run against a kubernetes cluster with the Argo Rollouts controller running. The rollout controller can be started with the command: make start-e2e Then run the e2e tests: make test-e2e Tips \u00b6 You can run the tests using a different kubeconfig by setting the KUBECONFIG environment variable: KUBECONFIG = ~/.kube/minikube make start-e2e KUBECONFIG = ~/.kube/minikube make test-e2e To run a specific e2e test, set the E2E_TEST_OPTIONS environment variable to specify the test (or test regex): make test-e2e E2E_TEST_OPTIONS = \"-testify.m ^TestRolloutRestart $ \" The e2e tests are designed to run as quickly as possible, eliminating readiness and termination delays. However, it is often desired to artificially slow down the tests for debugging purposes, as well as to understand what the test is doing. To delay startup and termination of pods, set the E2E_POD_DELAY to an integer value in seconds. This environment variable is often coupled with E2E_TEST_OPTIONS to debug and slow down a specific test. make test-e2e E2E_POD_DELAY = 10 Increasing the timeout. The E2E tests time out waiting on conditions to be met within 60 seconds. If debugging the rollout controller, it may be useful to increase this timeout while say sitting at a debugger breakpoint: make test-e2e E2E_WAIT_TIMEOUT = 999999 The e2e tests leverage a feature of the controller allowing the controller to be sharded with a user-specific \"instance id\" label. This allows the tests to operate only on rollouts with the specified label, and prevents any other controllers (including the system rollout controller), from also operating on the same set of rollouts. This value can be changed (from the default of argo-rollouts-e2e ), using the E2E_INSTANCE_ID environment variable: make start-e2e E2E_INSTANCE_ID = foo make test-e2e E2E_INSTANCE_ID = foo Alternatively, the e2e tests can be run against the system controller (i.e. without an instance id): make start-e2e E2E_INSTANCE_ID = '' Running Local Containers \u00b6 You may need to run containers locally, so here's how: Create login to Docker Hub, then login. docker login Add your username as the environment variable, e.g. to your ~/.bash_profile : export IMAGE_NAMESPACE = argoproj Build the images: DOCKER_PUSH = true make image Update the manifests: make manifests Install the manifests: kubectl -n argo-rollouts apply -f manifests/install.yaml Upgrading Kubernetes Libraries \u00b6 Argo Rollouts has a dependency on the kubernetes/kubernetes repo for some of the functionality that has not been pushed into the other kubernetes repositories yet. In order to import the kubernetes/kubernetes repo, all of the associated repos have to pinned to the correct version specified by the kubernetes/kubernetes release. The ./hack/update-k8s-dependencies.sh updates all the dependencies to the those correct versions. Documentation Changes \u00b6 Modify contents in docs/ directory. Preview changes in your browser by visiting http://localhost:8000 after running: make serve-docs To publish changes, run: make release-docs","title":"Contribution Guide"},{"location":"CONTRIBUTING/#contributing","text":"","title":"Contributing"},{"location":"CONTRIBUTING/#before-you-start","text":"Argo Rollouts is written in Golang. If you do not have a good grounding in Go, try out the tutorial .","title":"Before You Start"},{"location":"CONTRIBUTING/#pre-requisites","text":"Install: docker golang kubectl kustomize minikube or Docker for Desktop Kustomize is required for unit tests ( make test is using it), so you must install it locally if you wish to make code contributions to Argo Rollouts. Argo Rollout additionally uses the following tools golangci-lint to lint the project. protoc and swagger-codegen to generate proto related files yarn to build the UI Run the following commands to install them: # macOS brew install golangci-lint # linux go get -u github.com/golangci/golangci-lint/cmd/golangci-lint Brew users can quickly install the lot: brew install go kubectl kustomize golangci-lint protobuf swagger-codegen Set up environment variables (e.g. is ~/.bashrc ): export GOPATH = ~/go export PATH = $PATH : $GOPATH /bin Checkout the code: go get -u github.com/argoproj/argo-rollouts cd ~/go/src/github.com/argoproj/argo-rollouts","title":"Pre-requisites"},{"location":"CONTRIBUTING/#building","text":"go.mod is used, so the go build/test commands automatically install the needed dependencies The make controller command will build the controller. make codegen - Runs the code generator that creates the informers, client, lister, and deepcopies from the types.go and modifies the open-api spec.","title":"Building"},{"location":"CONTRIBUTING/#running-controller-locally","text":"It is much easier to run and debug if you run Argo Rollout in your local machine than in the Kubernetes cluster. cd ~/go/src/github.com/argoproj/argo-rollouts go run ./cmd/rollouts-controller/main.go","title":"Running Controller Locally"},{"location":"CONTRIBUTING/#running-unit-tests","text":"To run unit tests: make test","title":"Running Unit Tests"},{"location":"CONTRIBUTING/#running-e2e-tests","text":"The end-to-end tests need to run against a kubernetes cluster with the Argo Rollouts controller running. The rollout controller can be started with the command: make start-e2e Then run the e2e tests: make test-e2e","title":"Running E2E tests"},{"location":"CONTRIBUTING/#tips","text":"You can run the tests using a different kubeconfig by setting the KUBECONFIG environment variable: KUBECONFIG = ~/.kube/minikube make start-e2e KUBECONFIG = ~/.kube/minikube make test-e2e To run a specific e2e test, set the E2E_TEST_OPTIONS environment variable to specify the test (or test regex): make test-e2e E2E_TEST_OPTIONS = \"-testify.m ^TestRolloutRestart $ \" The e2e tests are designed to run as quickly as possible, eliminating readiness and termination delays. However, it is often desired to artificially slow down the tests for debugging purposes, as well as to understand what the test is doing. To delay startup and termination of pods, set the E2E_POD_DELAY to an integer value in seconds. This environment variable is often coupled with E2E_TEST_OPTIONS to debug and slow down a specific test. make test-e2e E2E_POD_DELAY = 10 Increasing the timeout. The E2E tests time out waiting on conditions to be met within 60 seconds. If debugging the rollout controller, it may be useful to increase this timeout while say sitting at a debugger breakpoint: make test-e2e E2E_WAIT_TIMEOUT = 999999 The e2e tests leverage a feature of the controller allowing the controller to be sharded with a user-specific \"instance id\" label. This allows the tests to operate only on rollouts with the specified label, and prevents any other controllers (including the system rollout controller), from also operating on the same set of rollouts. This value can be changed (from the default of argo-rollouts-e2e ), using the E2E_INSTANCE_ID environment variable: make start-e2e E2E_INSTANCE_ID = foo make test-e2e E2E_INSTANCE_ID = foo Alternatively, the e2e tests can be run against the system controller (i.e. without an instance id): make start-e2e E2E_INSTANCE_ID = ''","title":"Tips"},{"location":"CONTRIBUTING/#running-local-containers","text":"You may need to run containers locally, so here's how: Create login to Docker Hub, then login. docker login Add your username as the environment variable, e.g. to your ~/.bash_profile : export IMAGE_NAMESPACE = argoproj Build the images: DOCKER_PUSH = true make image Update the manifests: make manifests Install the manifests: kubectl -n argo-rollouts apply -f manifests/install.yaml","title":"Running Local Containers"},{"location":"CONTRIBUTING/#upgrading-kubernetes-libraries","text":"Argo Rollouts has a dependency on the kubernetes/kubernetes repo for some of the functionality that has not been pushed into the other kubernetes repositories yet. In order to import the kubernetes/kubernetes repo, all of the associated repos have to pinned to the correct version specified by the kubernetes/kubernetes release. The ./hack/update-k8s-dependencies.sh updates all the dependencies to the those correct versions.","title":"Upgrading Kubernetes Libraries"},{"location":"CONTRIBUTING/#documentation-changes","text":"Modify contents in docs/ directory. Preview changes in your browser by visiting http://localhost:8000 after running: make serve-docs To publish changes, run: make release-docs","title":"Documentation Changes"},{"location":"FAQ/","text":"FAQ \u00b6 General \u00b6 Does Argo Rollouts depend on Argo CD or any other Argo project? \u00b6 Argo Rollouts is a standalone project. Even though it works great with Argo CD and other Argo projects, it can be used on its own for Progressive Delivery scenarios. More specifically, Argo Rollouts does NOT require that you also have installed Argo CD on the same cluster. How does Argo Rollouts integrate with Argo CD? \u00b6 Argo CD understands the health of Argo Rollouts resources via Argo CD\u2019s Lua health check . These Health checks understand when the Argo Rollout objects are Progressing, Suspended, Degraded, or Healthy. Additionally, Argo CD has Lua based Resource Actions that can mutate an Argo Rollouts resource (i.e. unpause a Rollout). As a result, an operator can build automation to react to the states of the Argo Rollouts resources. For example, if a Rollout created by Argo CD is paused, Argo CD detects that and marks the Application as suspended. Once the new version is verified to be good, the operator can use Argo CD\u2019s resume resource action to unpause the Rollout so it can continue to make progress. Can we run the Argo Rollouts kubectl plugin commands via Argo CD? \u00b6 Argo CD supports running Lua scripts to modify resource kinds (i.e. suspending a CronJob by setting the .spec.suspend to true). These Lua Scripts can be configured in the argocd-cm ConfigMap or upstreamed to the Argo CD's resource_customizations directory. These custom actions have two Lua scripts: one to modify the said resource and another to detect if the action can be executed (i.e. A user should not be able to resuming a unpaused Rollout). Argo CD allows users to execute these actions via the UI or CLI. In the CLI, a user (or a CI system) can run argocd app actions run <APP_NAME> <ACTION> This command executes the action listed on the application listed. In the UI, a user can click the hamburger button of a resource and the available actions will appear in a couple of seconds. The user can click and confirm that action to execute it. Currently, the Rollout action has two available custom actions in Argo CD: resume and restart. Resume unpauses a Rollout with a PauseCondition Restart: Sets the RestartAt and causes all the pods to be restarted. Does Argo Rollout require a Service Mesh like Istio? \u00b6 Argo Rollouts does not require a service mesh or ingress controller to be used. In the absence of a traffic routing provider, Argo Rollouts manages the replica counts of the canary/stable ReplicaSets to achieve the desired canary weights. Normal Kubernetes Service routing (via kube-proxy) is used to split traffic between the ReplicaSets. Does Argo Rollout require we follow GitOps in my organization? \u00b6 Argo Rollouts is a Kubernetes controller that will react to any manifest change regardless of how the manifest was changed. The manifest can be changed by a Git commit, an API call, another controller or even a manual kubectl command. You can use Argo Rollouts with any traditional CI/CD solution that does not follow the GitOps approach. Can we run the Argo Rollouts controller in HA mode? \u00b6 Yes. A k8s cluster can run multiple replicas of Argo-rollouts controllers to achieve HA. To enable this feature, run the controller with --leader-elect flag and increase the number of replicas in the controller's deployment manifest. The implementation is based on the k8s client-go's leaderelection package . This implementation is tolerant to arbitrary clock skew among replicas. The level of tolerance to skew rate can be configured by setting --leader-election-lease-duration and --leader-election-renew-deadline appropriately. Please refer to the package documentation for details. Rollouts \u00b6 Which deployment strategies does Argo Rollouts support? \u00b6 Argo Rollouts supports BlueGreen, Canary, and Rolling Update. Additionally, Progressive Delivery features can be enabled on top of the blue-green/canary update, which further provides advanced deployment such as automated analysis and rollback. Does the Rollout object follow the provided strategy when it is first created? \u00b6 As with Deployments, Rollouts does not follow the strategy parameters on the initial deploy. The controller tries to get the Rollout into a steady state as fast as possible by creating a fully scaled up ReplicaSet from the provided .spec.template . Once the Rollout has a stable ReplicaSet to transition from, the controller starts using the provided strategy to transition the previous ReplicaSet to the desired ReplicaSet. How does BlueGreen rollback work? \u00b6 A BlueGreen Rollout keeps the old ReplicaSet up and running for 30 seconds or the value of the scaleDownDelaySeconds. The controller tracks the remaining time before scaling down by adding an annotation called argo-rollouts.argoproj.io/scale-down-deadline to the old ReplicaSet. If the user applies the old Rollout manifest before the old ReplicaSet scales down, the controller does something called a fast rollback. The controller immediately switches the active service\u2019s selector back to the old ReplicaSet\u2019s rollout-pod-template-hash and removes the scaled down annotation from that ReplicaSet. The controller does not do any of the normal operations when trying to introduce a new version since it is trying to revert as fast as possible. A non-fast-track rollback occurs when the scale down annotation has past and the old ReplicaSet has been scaled down. In this case, the Rollout treats the ReplicaSet like any other new ReplicaSet and follows the usual procedure for deploying a new ReplicaSet. What is the argo-rollouts.argoproj.io/managed-by-rollouts annotation? \u00b6 Argo Rollouts adds an argo-rollouts.argoproj.io/managed-by-rollouts annotation to Services and Ingresses that the controller modifies. They are used when the Rollout managing these resources is deleted and the controller tries to revert them back into their previous state. How can I deploy multiple services in a single step and roll them back according to their dependencies? \u00b6 The Rollout specification focuses on a single application/deployment. Argo Rollouts knows nothing about application dependencies. If you want to deploy multiple applications together in a smart way (e.g. automatically rollback a frontend if backend deployment fails) you need to write your own solution on top of Argo Rollouts. In most cases, you would need one Rollout resource for each application that you are deploying. Ideally you should also make your services backwards and forwards compatible (i.e. frontend should be able to work with both backend-preview and backend-active). Experiments \u00b6 Why doesn't my Experiment end? \u00b6 An Experiment\u2019s duration is controlled by the .spec.duration field and the analyses created for the Experiment. The .spec.duration indicates how long the ReplicaSets created by the Experiment should run. Once the duration passes, the experiment scales down the ReplicaSets it created and marks the AnalysisRuns successful unless the requiredForCompletion field is used in the Experiment. If enabled, the ReplicaSets are still scaled-down, but the Experiment does not finish until the Analysis Run finishes. Additionally, the .spec.duration is an optional field. If it\u2019s left unset, and the Experiment creates no AnalysisRuns, the ReplicaSets run indefinitely. The Experiment creates AnalysisRuns without the requiredForCompletion field, the Experiment fails only when the AnalysisRun created fails or errors out. If the requiredForCompletion field is set, the Experiment only marks itself as Successful and scales down the created ReplicaSets when the AnalysisRun finishes Successfully. Additionally, an Experiment ends if the .spec.terminate field is set to true regardless of the state of the Experiment. Analysis \u00b6 Why doesn't my AnalysisRun end? \u00b6 The AnalysisRun\u2019s duration is controlled by the metrics specified. Each Metric can specify an interval, count, and various limits (ConsecutiveErrorLimit, InconclusiveLimit, FailureLimit). If the interval is omitted, the AnalysisRun takes a single measurement. The count indicates how many measurements should be taken and causes the AnalysisRun to run indefinitely if omitted. The ConsecutiveErrorLimit, InconclusiveLimit, and FailureLimit define the thresholds allowed before putting the rollout into a completed state. Additionally, an AnalysisRun ends if the .spec.terminate field is set to true regardless of the state of the AnalysisRun. What is the difference between failures and errors? \u00b6 Failures are when the failure condition evaluates to true or an AnalysisRun without a failure condition evaluates the success condition to false. Errors are when the controller has any kind of issue with taking a measurement (i.e. invalid Prometheus URL).","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#general","text":"","title":"General"},{"location":"FAQ/#does-argo-rollouts-depend-on-argo-cd-or-any-other-argo-project","text":"Argo Rollouts is a standalone project. Even though it works great with Argo CD and other Argo projects, it can be used on its own for Progressive Delivery scenarios. More specifically, Argo Rollouts does NOT require that you also have installed Argo CD on the same cluster.","title":"Does Argo Rollouts depend on Argo CD or any other Argo project?"},{"location":"FAQ/#how-does-argo-rollouts-integrate-with-argo-cd","text":"Argo CD understands the health of Argo Rollouts resources via Argo CD\u2019s Lua health check . These Health checks understand when the Argo Rollout objects are Progressing, Suspended, Degraded, or Healthy. Additionally, Argo CD has Lua based Resource Actions that can mutate an Argo Rollouts resource (i.e. unpause a Rollout). As a result, an operator can build automation to react to the states of the Argo Rollouts resources. For example, if a Rollout created by Argo CD is paused, Argo CD detects that and marks the Application as suspended. Once the new version is verified to be good, the operator can use Argo CD\u2019s resume resource action to unpause the Rollout so it can continue to make progress.","title":"How does Argo Rollouts integrate with Argo CD?"},{"location":"FAQ/#can-we-run-the-argo-rollouts-kubectl-plugin-commands-via-argo-cd","text":"Argo CD supports running Lua scripts to modify resource kinds (i.e. suspending a CronJob by setting the .spec.suspend to true). These Lua Scripts can be configured in the argocd-cm ConfigMap or upstreamed to the Argo CD's resource_customizations directory. These custom actions have two Lua scripts: one to modify the said resource and another to detect if the action can be executed (i.e. A user should not be able to resuming a unpaused Rollout). Argo CD allows users to execute these actions via the UI or CLI. In the CLI, a user (or a CI system) can run argocd app actions run <APP_NAME> <ACTION> This command executes the action listed on the application listed. In the UI, a user can click the hamburger button of a resource and the available actions will appear in a couple of seconds. The user can click and confirm that action to execute it. Currently, the Rollout action has two available custom actions in Argo CD: resume and restart. Resume unpauses a Rollout with a PauseCondition Restart: Sets the RestartAt and causes all the pods to be restarted.","title":"Can we run the Argo Rollouts kubectl plugin commands via Argo CD?"},{"location":"FAQ/#does-argo-rollout-require-a-service-mesh-like-istio","text":"Argo Rollouts does not require a service mesh or ingress controller to be used. In the absence of a traffic routing provider, Argo Rollouts manages the replica counts of the canary/stable ReplicaSets to achieve the desired canary weights. Normal Kubernetes Service routing (via kube-proxy) is used to split traffic between the ReplicaSets.","title":"Does Argo Rollout require a Service Mesh like Istio?"},{"location":"FAQ/#does-argo-rollout-require-we-follow-gitops-in-my-organization","text":"Argo Rollouts is a Kubernetes controller that will react to any manifest change regardless of how the manifest was changed. The manifest can be changed by a Git commit, an API call, another controller or even a manual kubectl command. You can use Argo Rollouts with any traditional CI/CD solution that does not follow the GitOps approach.","title":"Does Argo Rollout require we follow GitOps in my organization?"},{"location":"FAQ/#can-we-run-the-argo-rollouts-controller-in-ha-mode","text":"Yes. A k8s cluster can run multiple replicas of Argo-rollouts controllers to achieve HA. To enable this feature, run the controller with --leader-elect flag and increase the number of replicas in the controller's deployment manifest. The implementation is based on the k8s client-go's leaderelection package . This implementation is tolerant to arbitrary clock skew among replicas. The level of tolerance to skew rate can be configured by setting --leader-election-lease-duration and --leader-election-renew-deadline appropriately. Please refer to the package documentation for details.","title":"Can we run the Argo Rollouts controller in HA mode?"},{"location":"FAQ/#rollouts","text":"","title":"Rollouts"},{"location":"FAQ/#which-deployment-strategies-does-argo-rollouts-support","text":"Argo Rollouts supports BlueGreen, Canary, and Rolling Update. Additionally, Progressive Delivery features can be enabled on top of the blue-green/canary update, which further provides advanced deployment such as automated analysis and rollback.","title":"Which deployment strategies does Argo Rollouts support?"},{"location":"FAQ/#does-the-rollout-object-follow-the-provided-strategy-when-it-is-first-created","text":"As with Deployments, Rollouts does not follow the strategy parameters on the initial deploy. The controller tries to get the Rollout into a steady state as fast as possible by creating a fully scaled up ReplicaSet from the provided .spec.template . Once the Rollout has a stable ReplicaSet to transition from, the controller starts using the provided strategy to transition the previous ReplicaSet to the desired ReplicaSet.","title":"Does the Rollout object follow the provided strategy when it is first created?"},{"location":"FAQ/#how-does-bluegreen-rollback-work","text":"A BlueGreen Rollout keeps the old ReplicaSet up and running for 30 seconds or the value of the scaleDownDelaySeconds. The controller tracks the remaining time before scaling down by adding an annotation called argo-rollouts.argoproj.io/scale-down-deadline to the old ReplicaSet. If the user applies the old Rollout manifest before the old ReplicaSet scales down, the controller does something called a fast rollback. The controller immediately switches the active service\u2019s selector back to the old ReplicaSet\u2019s rollout-pod-template-hash and removes the scaled down annotation from that ReplicaSet. The controller does not do any of the normal operations when trying to introduce a new version since it is trying to revert as fast as possible. A non-fast-track rollback occurs when the scale down annotation has past and the old ReplicaSet has been scaled down. In this case, the Rollout treats the ReplicaSet like any other new ReplicaSet and follows the usual procedure for deploying a new ReplicaSet.","title":"How does BlueGreen rollback work?"},{"location":"FAQ/#what-is-the-argo-rolloutsargoprojiomanaged-by-rollouts-annotation","text":"Argo Rollouts adds an argo-rollouts.argoproj.io/managed-by-rollouts annotation to Services and Ingresses that the controller modifies. They are used when the Rollout managing these resources is deleted and the controller tries to revert them back into their previous state.","title":"What is the argo-rollouts.argoproj.io/managed-by-rollouts annotation?"},{"location":"FAQ/#how-can-i-deploy-multiple-services-in-a-single-step-and-roll-them-back-according-to-their-dependencies","text":"The Rollout specification focuses on a single application/deployment. Argo Rollouts knows nothing about application dependencies. If you want to deploy multiple applications together in a smart way (e.g. automatically rollback a frontend if backend deployment fails) you need to write your own solution on top of Argo Rollouts. In most cases, you would need one Rollout resource for each application that you are deploying. Ideally you should also make your services backwards and forwards compatible (i.e. frontend should be able to work with both backend-preview and backend-active).","title":"How can I deploy multiple services in a single step and roll them back according to their dependencies?"},{"location":"FAQ/#experiments","text":"","title":"Experiments"},{"location":"FAQ/#why-doesnt-my-experiment-end","text":"An Experiment\u2019s duration is controlled by the .spec.duration field and the analyses created for the Experiment. The .spec.duration indicates how long the ReplicaSets created by the Experiment should run. Once the duration passes, the experiment scales down the ReplicaSets it created and marks the AnalysisRuns successful unless the requiredForCompletion field is used in the Experiment. If enabled, the ReplicaSets are still scaled-down, but the Experiment does not finish until the Analysis Run finishes. Additionally, the .spec.duration is an optional field. If it\u2019s left unset, and the Experiment creates no AnalysisRuns, the ReplicaSets run indefinitely. The Experiment creates AnalysisRuns without the requiredForCompletion field, the Experiment fails only when the AnalysisRun created fails or errors out. If the requiredForCompletion field is set, the Experiment only marks itself as Successful and scales down the created ReplicaSets when the AnalysisRun finishes Successfully. Additionally, an Experiment ends if the .spec.terminate field is set to true regardless of the state of the Experiment.","title":"Why doesn't my Experiment end?"},{"location":"FAQ/#analysis","text":"","title":"Analysis"},{"location":"FAQ/#why-doesnt-my-analysisrun-end","text":"The AnalysisRun\u2019s duration is controlled by the metrics specified. Each Metric can specify an interval, count, and various limits (ConsecutiveErrorLimit, InconclusiveLimit, FailureLimit). If the interval is omitted, the AnalysisRun takes a single measurement. The count indicates how many measurements should be taken and causes the AnalysisRun to run indefinitely if omitted. The ConsecutiveErrorLimit, InconclusiveLimit, and FailureLimit define the thresholds allowed before putting the rollout into a completed state. Additionally, an AnalysisRun ends if the .spec.terminate field is set to true regardless of the state of the AnalysisRun.","title":"Why doesn't my AnalysisRun end?"},{"location":"FAQ/#what-is-the-difference-between-failures-and-errors","text":"Failures are when the failure condition evaluates to true or an AnalysisRun without a failure condition evaluates the success condition to false. Errors are when the controller has any kind of issue with taking a measurement (i.e. invalid Prometheus URL).","title":"What is the difference between failures and errors?"},{"location":"architecture/","text":"Architecture \u00b6 Here is an overview of all the components that take part in a deployment managed by Argo Rollouts. Argo Rollouts controller \u00b6 This is the main controller that monitors the cluster for events and reacts whenever a resource of type Rollout is changed. The controller will read all the details of the rollout and bring the cluster to the same state as described in the rollout definition. Note that Argo Rollouts will not tamper with or respond to any changes that happen on normal Deployment Resources . This means that you can install Argo Rollouts in a cluster that is also deploying applications with alternative methods. To install the controller in your cluster and get started with Progressive Delivery, see the Installation page . Rollout resource \u00b6 The Rollout resource is a custom Kubernetes resource introduced and managed by Argo Rollouts. It is mostly compatible with the native Kubernetes Deployment resource but with extra fields that control the stages, thresholds and methods of advanced deployment methods such as canaries and blue/green deployments. Note that the Argo Rollouts controller will only respond to those changes that happen in Rollout sources. It will do nothing for normal deployment resources. This means that you need to migrate your Deployments to Rollouts if you want to manage them with Argo Rollouts. You can see all possible options of a Rollout in the full specification page . Replica sets for old and new version \u00b6 These are instances of the standard Kubernetes ReplicaSet resources . Argo Rollouts puts some extra metadata on them in order to keep track of the different versions that are part of an application. Note also that the replica sets that take part in a Rollout are fully managed by the controller in an automatic way. You should not tamper with them with external tools. Ingress/Service \u00b6 This is the mechanism that traffic from live users enters your cluster and is redirected to the appropriate version. Argo Rollouts use the standard Kubernetes service resource , but with some extra metadata needed for management. Argo Rollouts is very flexible on networking options. First of all you can have different services during a Rollout, that go only to the new version, only to the old version or both. Specifically for Canary deployments, Argo Rollouts supports several service mesh and ingress solutions for splitting traffic with specific percentages instead of simple balancing based on pod counts and it is possible to use multiple routing providers simultaneously. AnalysisTemplate and AnalysisRun \u00b6 Analysis is the capability to connect a Rollout to your metrics provider and define specific thresholds for certain metrics that will decide if an update is successful or not. For each analysis you can define one or more metric queries along with their expected results. A Rollout will progress on its own if metric queries are good, rollback automatically if metrics show failure and pause the rollout if metrics cannot provide a success/failure answer. For performing an analysis, Argo Rollouts includes two custom Kubernetes resources: AnalysisTemplate and AnalysisRun . AnalysisTemplate contains instructions on what metrics to query. The actual result that is attached to a Rollout is the AnalysisRun custom resource. You can define an AnalysisTemplate on a specific Rollout or globally on the cluster to be shared by multiple rollouts as a ClusterAnalysisTemplate . The AnalysisRun resource is scoped on a specific rollout. Note that using an analysis and metrics in a Rollout is completely optional. You can manually pause and promote a rollout or use other external methods (e.g. smoke tests) via the API or the CLI. You don't need a metric solution just to use Argo Rollouts. You can also mix both automated (i.e. analysis based) and manual steps in a Rollout. Apart from metrics, you can also decide the success of a rollout by running a Kubernetes job or running a webhook . Metric providers \u00b6 Argo Rollouts includes native integration for several popular metrics providers that you can use in the Analysis resources to automatically promote or rollback a rollout. See the documentation of each provider for specific setup options. CLI and UI (Not shown in the diagram) \u00b6 You can view and manage Rollouts with the Argo Rollouts CLI or the integrated UI . Both are optional.","title":"Architecture"},{"location":"architecture/#architecture","text":"Here is an overview of all the components that take part in a deployment managed by Argo Rollouts.","title":"Architecture"},{"location":"architecture/#argo-rollouts-controller","text":"This is the main controller that monitors the cluster for events and reacts whenever a resource of type Rollout is changed. The controller will read all the details of the rollout and bring the cluster to the same state as described in the rollout definition. Note that Argo Rollouts will not tamper with or respond to any changes that happen on normal Deployment Resources . This means that you can install Argo Rollouts in a cluster that is also deploying applications with alternative methods. To install the controller in your cluster and get started with Progressive Delivery, see the Installation page .","title":"Argo Rollouts controller"},{"location":"architecture/#rollout-resource","text":"The Rollout resource is a custom Kubernetes resource introduced and managed by Argo Rollouts. It is mostly compatible with the native Kubernetes Deployment resource but with extra fields that control the stages, thresholds and methods of advanced deployment methods such as canaries and blue/green deployments. Note that the Argo Rollouts controller will only respond to those changes that happen in Rollout sources. It will do nothing for normal deployment resources. This means that you need to migrate your Deployments to Rollouts if you want to manage them with Argo Rollouts. You can see all possible options of a Rollout in the full specification page .","title":"Rollout resource"},{"location":"architecture/#replica-sets-for-old-and-new-version","text":"These are instances of the standard Kubernetes ReplicaSet resources . Argo Rollouts puts some extra metadata on them in order to keep track of the different versions that are part of an application. Note also that the replica sets that take part in a Rollout are fully managed by the controller in an automatic way. You should not tamper with them with external tools.","title":"Replica sets for old and new version"},{"location":"architecture/#ingressservice","text":"This is the mechanism that traffic from live users enters your cluster and is redirected to the appropriate version. Argo Rollouts use the standard Kubernetes service resource , but with some extra metadata needed for management. Argo Rollouts is very flexible on networking options. First of all you can have different services during a Rollout, that go only to the new version, only to the old version or both. Specifically for Canary deployments, Argo Rollouts supports several service mesh and ingress solutions for splitting traffic with specific percentages instead of simple balancing based on pod counts and it is possible to use multiple routing providers simultaneously.","title":"Ingress/Service"},{"location":"architecture/#analysistemplate-and-analysisrun","text":"Analysis is the capability to connect a Rollout to your metrics provider and define specific thresholds for certain metrics that will decide if an update is successful or not. For each analysis you can define one or more metric queries along with their expected results. A Rollout will progress on its own if metric queries are good, rollback automatically if metrics show failure and pause the rollout if metrics cannot provide a success/failure answer. For performing an analysis, Argo Rollouts includes two custom Kubernetes resources: AnalysisTemplate and AnalysisRun . AnalysisTemplate contains instructions on what metrics to query. The actual result that is attached to a Rollout is the AnalysisRun custom resource. You can define an AnalysisTemplate on a specific Rollout or globally on the cluster to be shared by multiple rollouts as a ClusterAnalysisTemplate . The AnalysisRun resource is scoped on a specific rollout. Note that using an analysis and metrics in a Rollout is completely optional. You can manually pause and promote a rollout or use other external methods (e.g. smoke tests) via the API or the CLI. You don't need a metric solution just to use Argo Rollouts. You can also mix both automated (i.e. analysis based) and manual steps in a Rollout. Apart from metrics, you can also decide the success of a rollout by running a Kubernetes job or running a webhook .","title":"AnalysisTemplate and AnalysisRun"},{"location":"architecture/#metric-providers","text":"Argo Rollouts includes native integration for several popular metrics providers that you can use in the Analysis resources to automatically promote or rollback a rollout. See the documentation of each provider for specific setup options.","title":"Metric providers"},{"location":"architecture/#cli-and-ui-not-shown-in-the-diagram","text":"You can view and manage Rollouts with the Argo Rollouts CLI or the integrated UI . Both are optional.","title":"CLI and UI (Not shown in the diagram)"},{"location":"best-practices/","text":"Best Practices \u00b6 This document describes some best practices, tips and tricks when using Argo Rollouts. Ingress desired/stable host routes \u00b6 For various reasons, it is often desired that external services are able to reach the desired pods (aka canary/preview) or stable pods specifically, without the possibility of traffic arbitrarily being split between the two versions. Some use cases include: The new version of the service is able to be reach internally/privately (e.g. for manual verification), before exposing it externally. An external CI/CD pipeline runs tests against the blue-green preview stack before it is promoted to production. Running tests which compare the behavior of old version against the new version. If you are using an Ingress to route traffic to the service, additional host rules can be added to the ingress rules so that it is possible to specifically reach to the desired (canary/preview) pods or stable pods. apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : guestbook spec : rules : # host rule to only reach the desired pods (aka canary/preview) - host : guestbook-desired.argoproj.io http : paths : - backend : serviceName : guestbook-desired servicePort : 443 path : /* # host rule to only reach the stable pods - host : guestbook-stable.argoproj.io http : paths : - backend : serviceName : guestbook-stable servicePort : 443 path : /* # default rule which omits host, and will split traffic between desired vs. stable - http : paths : - backend : serviceName : guestbook-root servicePort : 443 path : /* The above technique has the a benefit in that it would not incur additional cost of allocating additional load balancers.","title":"Best Practices"},{"location":"best-practices/#best-practices","text":"This document describes some best practices, tips and tricks when using Argo Rollouts.","title":"Best Practices"},{"location":"best-practices/#ingress-desiredstable-host-routes","text":"For various reasons, it is often desired that external services are able to reach the desired pods (aka canary/preview) or stable pods specifically, without the possibility of traffic arbitrarily being split between the two versions. Some use cases include: The new version of the service is able to be reach internally/privately (e.g. for manual verification), before exposing it externally. An external CI/CD pipeline runs tests against the blue-green preview stack before it is promoted to production. Running tests which compare the behavior of old version against the new version. If you are using an Ingress to route traffic to the service, additional host rules can be added to the ingress rules so that it is possible to specifically reach to the desired (canary/preview) pods or stable pods. apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : guestbook spec : rules : # host rule to only reach the desired pods (aka canary/preview) - host : guestbook-desired.argoproj.io http : paths : - backend : serviceName : guestbook-desired servicePort : 443 path : /* # host rule to only reach the stable pods - host : guestbook-stable.argoproj.io http : paths : - backend : serviceName : guestbook-stable servicePort : 443 path : /* # default rule which omits host, and will split traffic between desired vs. stable - http : paths : - backend : serviceName : guestbook-root servicePort : 443 path : /* The above technique has the a benefit in that it would not incur additional cost of allocating additional load balancers.","title":"Ingress desired/stable host routes"},{"location":"concepts/","text":"Concepts \u00b6 Rollout \u00b6 A Rollout is Kubernetes workload resource which is equivalent to a Kubernetes Deployment object. It is intended to replace a Deployment object in scenarios when more advanced deployment or progressive delivery functionality is needed. A Rollout provides the following features which a Kubernetes Deployment cannot: blue-green deployments canary deployments integration with ingress controllers and service meshes for advanced traffic routing integration with metric providers for blue-green & canary analysis automated promotion or rollback based on successful or failed metrics Progressive Delivery \u00b6 Progressive delivery is the process of releasing updates of a product in a controlled and gradual manner, thereby reducing the risk of the release, typically coupling automation and metric analysis to drive the automated promotion or rollback of the update. Progressive delivery is often described as an evolution of continuous delivery, extending the speed benefits made in CI/CD to the deployment process. This is accomplished by limiting the exposure of the new version to a subset of users, observing and analyzing for correct behavior, then progressively increasing the exposure to a broader and wider audience while continuously verifying correctness. Deployment Strategies \u00b6 While the industry has used a consistent terminology to describe various deployment strategies, the implementations of these strategies tend to differ across tooling. To make it clear how the Argo Rollouts will behave, here are the descriptions of the various deployment strategies implementations offered by the Argo Rollouts. Rolling Update \u00b6 A RollingUpdate slowly replaces the old version with the new version. As the new version comes up, the old version is scaled down in order to maintain the overall count of the application. This is the default strategy of the Deployment object. Recreate \u00b6 A Recreate deployment deletes the old version of the application before bring up the new version. As a result, this ensures that two versions of the application never run at the same time, but there is downtime during the deployment. Blue-Green \u00b6 A Blue-Green deployment (sometimes referred to as a Red-Black) has both the new and old version of the application deployed at the same time. During this time, only the old version of the application will receive production traffic. This allows the developers to run tests against the new version before switching the live traffic to the new version. Canary \u00b6 A Canary deployment exposes a subset of users to the new version of the application while serving the rest of the traffic to the old version. Once the new version is verified to be correct, the new version can gradually replace the old version. Ingress controllers and service meshes such as NGINX and Istio, enable more sophisticated traffic shaping patterns for canarying than what is natively available (e.g. achieving very fine-grained traffic splitting, or splitting based on HTTP headers). The picture above shows a canary with two stages (10% and 33% of traffic goes to new version) but this is just an example. With Argo Rollouts you can define the exact number of stages and percentages of traffic according to your use case.","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#rollout","text":"A Rollout is Kubernetes workload resource which is equivalent to a Kubernetes Deployment object. It is intended to replace a Deployment object in scenarios when more advanced deployment or progressive delivery functionality is needed. A Rollout provides the following features which a Kubernetes Deployment cannot: blue-green deployments canary deployments integration with ingress controllers and service meshes for advanced traffic routing integration with metric providers for blue-green & canary analysis automated promotion or rollback based on successful or failed metrics","title":"Rollout"},{"location":"concepts/#progressive-delivery","text":"Progressive delivery is the process of releasing updates of a product in a controlled and gradual manner, thereby reducing the risk of the release, typically coupling automation and metric analysis to drive the automated promotion or rollback of the update. Progressive delivery is often described as an evolution of continuous delivery, extending the speed benefits made in CI/CD to the deployment process. This is accomplished by limiting the exposure of the new version to a subset of users, observing and analyzing for correct behavior, then progressively increasing the exposure to a broader and wider audience while continuously verifying correctness.","title":"Progressive Delivery"},{"location":"concepts/#deployment-strategies","text":"While the industry has used a consistent terminology to describe various deployment strategies, the implementations of these strategies tend to differ across tooling. To make it clear how the Argo Rollouts will behave, here are the descriptions of the various deployment strategies implementations offered by the Argo Rollouts.","title":"Deployment Strategies"},{"location":"concepts/#rolling-update","text":"A RollingUpdate slowly replaces the old version with the new version. As the new version comes up, the old version is scaled down in order to maintain the overall count of the application. This is the default strategy of the Deployment object.","title":"Rolling Update"},{"location":"concepts/#recreate","text":"A Recreate deployment deletes the old version of the application before bring up the new version. As a result, this ensures that two versions of the application never run at the same time, but there is downtime during the deployment.","title":"Recreate"},{"location":"concepts/#blue-green","text":"A Blue-Green deployment (sometimes referred to as a Red-Black) has both the new and old version of the application deployed at the same time. During this time, only the old version of the application will receive production traffic. This allows the developers to run tests against the new version before switching the live traffic to the new version.","title":"Blue-Green"},{"location":"concepts/#canary","text":"A Canary deployment exposes a subset of users to the new version of the application while serving the rest of the traffic to the old version. Once the new version is verified to be correct, the new version can gradually replace the old version. Ingress controllers and service meshes such as NGINX and Istio, enable more sophisticated traffic shaping patterns for canarying than what is natively available (e.g. achieving very fine-grained traffic splitting, or splitting based on HTTP headers). The picture above shows a canary with two stages (10% and 33% of traffic goes to new version) but this is just an example. With Argo Rollouts you can define the exact number of stages and percentages of traffic according to your use case.","title":"Canary"},{"location":"dashboard/","text":"UI Dashboard \u00b6 The Argo Rollouts Kubectl plugin can serve a local UI Dashboard to visualize your Rollouts. To start it, run kubectl argo rollouts dashboard in the namespace that contains your Rollouts. Then visit localhost:3100 to view the user interface. List view \u00b6 Individual Rollout view \u00b6","title":"Dashboard"},{"location":"dashboard/#ui-dashboard","text":"The Argo Rollouts Kubectl plugin can serve a local UI Dashboard to visualize your Rollouts. To start it, run kubectl argo rollouts dashboard in the namespace that contains your Rollouts. Then visit localhost:3100 to view the user interface.","title":"UI Dashboard"},{"location":"dashboard/#list-view","text":"","title":"List view"},{"location":"dashboard/#individual-rollout-view","text":"","title":"Individual Rollout view"},{"location":"getting-started/","text":"Getting Started \u00b6 This guide will demonstrate various concepts and features of Argo Rollouts by going through deployment, upgrade, promotion, and abortion of a Rollout. Requirements \u00b6 Kubernetes cluster with argo-rollouts controller installed (see install guide ) kubectl with argo-rollouts plugin installed (see install guide ) 1. Deploying a Rollout \u00b6 First we deploy a Rollout resource and a Kubernetes Service targeting that Rollout. The example Rollout in this guide utilizes a canary update strategy which sends 20% of traffic to the canary, followed by a manual promotion, and finally gradual automated traffic increases for the remainder of the upgrade. This behavior is described in the following portion of the Rollout spec: spec : replicas : 5 strategy : canary : steps : - setWeight : 20 - pause : {} - setWeight : 40 - pause : { duration : 10 } - setWeight : 60 - pause : { duration : 10 } - setWeight : 80 - pause : { duration : 10 } Run the following command to deploy the initial Rollout and Service: kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml Initial creations of any Rollout will immediately scale up the replicas to 100% (skipping any canary upgrade steps, analysis, etc...) since there was no upgrade that occurred. The Argo Rollouts kubectl plugin allows you to visualize the Rollout, its related resources (ReplicaSets, Pods, AnalysisRuns), and presents live state changes as they occur. To watch the rollout as it deploys, run the get rollout --watch command from plugin: kubectl argo rollouts get rollout rollouts-demo --watch 2. Updating a Rollout \u00b6 Next it is time to perform an update. Just as with Deployments, any change to the Pod template field ( spec.template ) results in a new version (i.e. ReplicaSet) to be deployed. Updating a Rollout involves modifying the rollout spec, typically changing the container image field with a new version, and then running kubectl apply against the new manifest. As a convenience, the rollouts plugin provides a set image command, which performs these steps against the live rollout object in-place. Run the following command to update the rollouts-demo Rollout with the \"yellow\" version of the container: kubectl argo rollouts set image rollouts-demo \\ rollouts-demo = argoproj/rollouts-demo:yellow During a rollout update, the controller will progress through the steps defined in the Rollout's update strategy. The example rollout sets a 20% traffic weight to the canary, and pauses the rollout indefinitely until user action is taken to unpause/promote the rollout. After updating the image, watch the rollout again until it reaches the paused state: kubectl argo rollouts get rollout rollouts-demo --watch When the demo rollout reaches the second step, we can see from the plugin that the Rollout is in a paused state, and now has 1 of 5 replicas running the new version of the pod template, and 4 of 5 replicas running the old version. This equates to the 20% canary weight as defined by the setWeight: 20 step. 3. Promoting a Rollout \u00b6 The rollout is now in a paused state. When a Rollout reaches a pause step with no duration, it will remain in a paused state indefinitely until it is resumed/promoted. To manually promote a rollout to the next step, run the promote command of the plugin: kubectl argo rollouts promote rollouts-demo After promotion, Rollout will proceed to execute the remaining steps. The remaining rollout steps in our example are fully automated, so the Rollout will eventually complete steps until it has has fully transitioned to the new version. Watch the rollout again until it has completed all steps: kubectl argo rollouts get rollout rollouts-demo --watch Tip The promote command also supports the ability to skip all remaining steps and analysis with the --full flag. Once all steps complete successfully, the new ReplicaSet is marked as the \"stable\" ReplicaSet. Whenever a rollout is aborted during an update, either automatically via a failed canary analysis, or manually by a user, the Rollout will fall back to the \"stable\" version. 4. Aborting a Rollout \u00b6 Next we will learn how to manually abort a rollout during an update. First, deploy a new \"red\" version of the container using the set image command, and wait for the rollout to reach the paused step again: kubectl argo rollouts set image rollouts-demo \\ rollouts-demo = argoproj/rollouts-demo:red This time, instead of promoting the rollout to the next step, we will abort the update, so that it falls back to the \"stable\" version. The plugin provides an abort command as a way to manually abort a rollout at any time during an update: kubectl argo rollouts abort rollouts-demo When a rollout is aborted, it will scale up the \"stable\" version of the ReplicaSet (in this case the yellow image), and scale down any other versions. Although the stable version of the ReplicaSet may be running and is healthy, the overall rollout is still considered Degraded , since the desired version (the red image) is not the version which is actually running. In order to make Rollout considered Healthy again and not Degraded, it is necessary to change the desired state back to the previous, stable version. This typically involves running kubectl apply against the previous Rollout spec. In our case, we can simply re-run the set image command using the previous, \"yellow\" image. kubectl argo rollouts set image rollouts-demo \\ rollouts-demo=argoproj/rollouts-demo:yellow After running this command, you should notice that the Rollout immediately becomes Healthy, and there is no activity with regards to new ReplicaSets becoming created. When a Rollout has not yet reached its desired state (e.g. it was aborted, or in the middle of an update), and the stable manifest were re-applied, the Rollout detects this as a rollback and not a update, and will fast-track the deployment of the stable ReplicaSet by skipping analysis, and the steps. Summary \u00b6 In this guide, we have learned basic capabilities of Argo Rollouts, including: Deploying a rollout Performing a canary update Manual promotion Manual abortion The Rollout in this basic example did not utilize a ingress controller or service mesh provider to route traffic. Instead, it used normal Kubernetes Service networking (i.e. kube-proxy) to achieve an approximate canary weight, based on the closest ratio of new to old replica counts. As a result, this Rollout had a limitation in that it could only achieve a minimum canary weight of 20%, by scaling 1 of 5 pods to run the new version. In order to achieve much finer grained canaries, an ingress controller or service mesh is necessary. Follow one of the traffic routing guides to see how Argo Rollouts can leverage a networking provider to achieve more advanced traffic shaping. ALB Guide Ambassador Guide Istio Guide Multiple Providers Guide NGINX Guide SMI Guide","title":"Basic Usage"},{"location":"getting-started/#getting-started","text":"This guide will demonstrate various concepts and features of Argo Rollouts by going through deployment, upgrade, promotion, and abortion of a Rollout.","title":"Getting Started"},{"location":"getting-started/#requirements","text":"Kubernetes cluster with argo-rollouts controller installed (see install guide ) kubectl with argo-rollouts plugin installed (see install guide )","title":"Requirements"},{"location":"getting-started/#1-deploying-a-rollout","text":"First we deploy a Rollout resource and a Kubernetes Service targeting that Rollout. The example Rollout in this guide utilizes a canary update strategy which sends 20% of traffic to the canary, followed by a manual promotion, and finally gradual automated traffic increases for the remainder of the upgrade. This behavior is described in the following portion of the Rollout spec: spec : replicas : 5 strategy : canary : steps : - setWeight : 20 - pause : {} - setWeight : 40 - pause : { duration : 10 } - setWeight : 60 - pause : { duration : 10 } - setWeight : 80 - pause : { duration : 10 } Run the following command to deploy the initial Rollout and Service: kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml Initial creations of any Rollout will immediately scale up the replicas to 100% (skipping any canary upgrade steps, analysis, etc...) since there was no upgrade that occurred. The Argo Rollouts kubectl plugin allows you to visualize the Rollout, its related resources (ReplicaSets, Pods, AnalysisRuns), and presents live state changes as they occur. To watch the rollout as it deploys, run the get rollout --watch command from plugin: kubectl argo rollouts get rollout rollouts-demo --watch","title":"1. Deploying a Rollout"},{"location":"getting-started/#2-updating-a-rollout","text":"Next it is time to perform an update. Just as with Deployments, any change to the Pod template field ( spec.template ) results in a new version (i.e. ReplicaSet) to be deployed. Updating a Rollout involves modifying the rollout spec, typically changing the container image field with a new version, and then running kubectl apply against the new manifest. As a convenience, the rollouts plugin provides a set image command, which performs these steps against the live rollout object in-place. Run the following command to update the rollouts-demo Rollout with the \"yellow\" version of the container: kubectl argo rollouts set image rollouts-demo \\ rollouts-demo = argoproj/rollouts-demo:yellow During a rollout update, the controller will progress through the steps defined in the Rollout's update strategy. The example rollout sets a 20% traffic weight to the canary, and pauses the rollout indefinitely until user action is taken to unpause/promote the rollout. After updating the image, watch the rollout again until it reaches the paused state: kubectl argo rollouts get rollout rollouts-demo --watch When the demo rollout reaches the second step, we can see from the plugin that the Rollout is in a paused state, and now has 1 of 5 replicas running the new version of the pod template, and 4 of 5 replicas running the old version. This equates to the 20% canary weight as defined by the setWeight: 20 step.","title":"2. Updating a Rollout"},{"location":"getting-started/#3-promoting-a-rollout","text":"The rollout is now in a paused state. When a Rollout reaches a pause step with no duration, it will remain in a paused state indefinitely until it is resumed/promoted. To manually promote a rollout to the next step, run the promote command of the plugin: kubectl argo rollouts promote rollouts-demo After promotion, Rollout will proceed to execute the remaining steps. The remaining rollout steps in our example are fully automated, so the Rollout will eventually complete steps until it has has fully transitioned to the new version. Watch the rollout again until it has completed all steps: kubectl argo rollouts get rollout rollouts-demo --watch Tip The promote command also supports the ability to skip all remaining steps and analysis with the --full flag. Once all steps complete successfully, the new ReplicaSet is marked as the \"stable\" ReplicaSet. Whenever a rollout is aborted during an update, either automatically via a failed canary analysis, or manually by a user, the Rollout will fall back to the \"stable\" version.","title":"3. Promoting a Rollout"},{"location":"getting-started/#4-aborting-a-rollout","text":"Next we will learn how to manually abort a rollout during an update. First, deploy a new \"red\" version of the container using the set image command, and wait for the rollout to reach the paused step again: kubectl argo rollouts set image rollouts-demo \\ rollouts-demo = argoproj/rollouts-demo:red This time, instead of promoting the rollout to the next step, we will abort the update, so that it falls back to the \"stable\" version. The plugin provides an abort command as a way to manually abort a rollout at any time during an update: kubectl argo rollouts abort rollouts-demo When a rollout is aborted, it will scale up the \"stable\" version of the ReplicaSet (in this case the yellow image), and scale down any other versions. Although the stable version of the ReplicaSet may be running and is healthy, the overall rollout is still considered Degraded , since the desired version (the red image) is not the version which is actually running. In order to make Rollout considered Healthy again and not Degraded, it is necessary to change the desired state back to the previous, stable version. This typically involves running kubectl apply against the previous Rollout spec. In our case, we can simply re-run the set image command using the previous, \"yellow\" image. kubectl argo rollouts set image rollouts-demo \\ rollouts-demo=argoproj/rollouts-demo:yellow After running this command, you should notice that the Rollout immediately becomes Healthy, and there is no activity with regards to new ReplicaSets becoming created. When a Rollout has not yet reached its desired state (e.g. it was aborted, or in the middle of an update), and the stable manifest were re-applied, the Rollout detects this as a rollback and not a update, and will fast-track the deployment of the stable ReplicaSet by skipping analysis, and the steps.","title":"4. Aborting a Rollout"},{"location":"getting-started/#summary","text":"In this guide, we have learned basic capabilities of Argo Rollouts, including: Deploying a rollout Performing a canary update Manual promotion Manual abortion The Rollout in this basic example did not utilize a ingress controller or service mesh provider to route traffic. Instead, it used normal Kubernetes Service networking (i.e. kube-proxy) to achieve an approximate canary weight, based on the closest ratio of new to old replica counts. As a result, this Rollout had a limitation in that it could only achieve a minimum canary weight of 20%, by scaling 1 of 5 pods to run the new version. In order to achieve much finer grained canaries, an ingress controller or service mesh is necessary. Follow one of the traffic routing guides to see how Argo Rollouts can leverage a networking provider to achieve more advanced traffic shaping. ALB Guide Ambassador Guide Istio Guide Multiple Providers Guide NGINX Guide SMI Guide","title":"Summary"},{"location":"installation/","text":"Installation \u00b6 Controller Installation \u00b6 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml This will create a new namespace, argo-rollouts , where Argo Rollouts controller will run. Tip If you are using another namspace name, please update install.yaml clusterrolebinding's serviceaccount namespace name. Tip When installing Argo Rollouts on Kubernetes v1.14 or lower, the CRD manifests must be kubectl applied with the --validate=false option. This is caused by use of new CRD fields introduced in v1.15, which are rejected by default in lower API servers. Tip On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com You can find released container images of the controller at Quay.io . There are also old releases at Dockerhub, but since the introduction of rate limiting, the Argo project has moved to Quay. Kubectl Plugin Installation \u00b6 The kubectl plugin is optional, but is convenient for managing and visualizing rollouts from the command line. Brew \u00b6 brew install argoproj/tap/kubectl-argo-rollouts Manual \u00b6 Install Argo Rollouts Kubectl plugin with curl. curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-darwin-amd64 For Linux dist, replace darwin with linux Make the kubectl-argo-rollouts binary executable. chmod +x ./kubectl-argo-rollouts-darwin-amd64 Move the binary into your PATH. sudo mv ./kubectl-argo-rollouts-darwin-amd64 /usr/local/bin/kubectl-argo-rollouts Test to ensure the version you installed is up-to-date: kubectl argo rollouts version Shell auto completion \u00b6 The CLI can export shell completion code for several shells. For bash, ensure you have bash completions installed and enabled. To access completions in your current shell, run $ source <(kubectl-argo-rollouts completion bash) . Alternatively, write it to a file and source in .bash_profile . The completion command supports bash, zsh, fish and powershell. See the completion command documentation for more details. Using the CLI with Docker \u00b6 The CLI is also available as a container image at https://quay.io/repository/argoproj/kubectl-argo-rollouts You can run it like any other Docker image or use it in any CI platform that supports Docker images. docker run quay.io/argoproj/kubectl-argo-rollouts:master version","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#controller-installation","text":"kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml This will create a new namespace, argo-rollouts , where Argo Rollouts controller will run. Tip If you are using another namspace name, please update install.yaml clusterrolebinding's serviceaccount namespace name. Tip When installing Argo Rollouts on Kubernetes v1.14 or lower, the CRD manifests must be kubectl applied with the --validate=false option. This is caused by use of new CRD fields introduced in v1.15, which are rejected by default in lower API servers. Tip On GKE, you will need grant your account the ability to create new cluster roles: kubectl create clusterrolebinding YOURNAME-cluster-admin-binding --clusterrole = cluster-admin --user = YOUREMAIL@gmail.com You can find released container images of the controller at Quay.io . There are also old releases at Dockerhub, but since the introduction of rate limiting, the Argo project has moved to Quay.","title":"Controller Installation"},{"location":"installation/#kubectl-plugin-installation","text":"The kubectl plugin is optional, but is convenient for managing and visualizing rollouts from the command line.","title":"Kubectl Plugin Installation"},{"location":"installation/#brew","text":"brew install argoproj/tap/kubectl-argo-rollouts","title":"Brew"},{"location":"installation/#manual","text":"Install Argo Rollouts Kubectl plugin with curl. curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-darwin-amd64 For Linux dist, replace darwin with linux Make the kubectl-argo-rollouts binary executable. chmod +x ./kubectl-argo-rollouts-darwin-amd64 Move the binary into your PATH. sudo mv ./kubectl-argo-rollouts-darwin-amd64 /usr/local/bin/kubectl-argo-rollouts Test to ensure the version you installed is up-to-date: kubectl argo rollouts version","title":"Manual"},{"location":"installation/#shell-auto-completion","text":"The CLI can export shell completion code for several shells. For bash, ensure you have bash completions installed and enabled. To access completions in your current shell, run $ source <(kubectl-argo-rollouts completion bash) . Alternatively, write it to a file and source in .bash_profile . The completion command supports bash, zsh, fish and powershell. See the completion command documentation for more details.","title":"Shell auto completion"},{"location":"installation/#using-the-cli-with-docker","text":"The CLI is also available as a container image at https://quay.io/repository/argoproj/kubectl-argo-rollouts You can run it like any other Docker image or use it in any CI platform that supports Docker images. docker run quay.io/argoproj/kubectl-argo-rollouts:master version","title":"Using the CLI  with Docker"},{"location":"migrating/","text":"Migrating to Rollouts \u00b6 There are ways to migrate to Rollout: Convert an existing Deployment resource to a Rollout resource. Reference an existing Deployment from a Rollout using workloadRef field. Convert Deployment to Rollout \u00b6 When converting a Deployment to a Rollout, it involves changing three fields: Replacing the apiVersion from apps/v1 to argoproj.io/v1alpha1 Replacing the kind from Deployment to Rollout Replacing the deployment strategy with a blue-green or canary strategy Below is an example of a Rollout resource using the canary strategy. apiVersion : argoproj.io/v1alpha1 # Changed from apps/v1 kind : Rollout # Changed from Deployment metadata : name : rollouts-demo spec : selector : matchLabels : app : rollouts-demo template : metadata : labels : app : rollouts-demo spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue ports : - containerPort : 8080 strategy : canary : # Changed from rollingUpdate or recreate steps : - setWeight : 20 - pause : {} Warning When migrating a Deployment which is already serving live production traffic, a Rollout should run next to the Deployment before deleting the Deployment or scaling down the Deployment. Not following this approach might result in downtime . It also allows for the Rollout to be tested before deleting the original Deployment. Reference Deployment From Rollout \u00b6 Instead of removing Deployment you can scale-down in to zero and reference from the Rollout resource: Create a Rollout resource. Reference an existing Deployment using workloadRef field. Scale-down existing Deployment by changing replicas field of an existing Deployment to zero. To perform an update, the change should be made to the Pod template field of the Deployment. Below is an example of a Rollout resource referencing a Deployment. apiVersion : argoproj.io/v1alpha1 # Create a rollout resource kind : Rollout metadata : name : rollout-ref-deployment spec : replicas : 5 workloadRef : # Reference an existing Deployment using workloadRef field apiVersion : apps/v1 kind : Deployment name : rollout-ref-deployment strategy : canary : steps : - setWeight : 20 - pause : { duration : 10s } --- apiVersion : apps/v1 kind : Deployment metadata : labels : app.kubernetes.io/instance : rollout-canary name : rollout-ref-deployment spec : replicas : 0 # Scale down existing deployment selector : matchLabels : app : rollout-ref-deployment template : metadata : labels : app : rollout-ref-deployment spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue imagePullPolicy : Always ports : - containerPort : 8080 Consider following if your Deployment runs in production: Running Rollout and Deployment side-by-side After creation Rollout will spinup required number of Pods side-by-side with the Deployment Pods. Rollout won't try to manage existing Deployment Pods. That means you can safely update add Rollout to the production environment without any interruption but you are going to run twice more Pods during migration. Argo-rollouts controller patches the spec of rollout object with an annotation of rollout.argoproj.io/workload-generation , which equals the generation of referenced deployment. Users can detect if the rollout matches desired generation of deployment by checking the workloadObservedGeneration in the rollout status. Traffic Management During Migration The Rollout offers traffic management functionality that manages routing rules and flows the traffic to different versions of an application. For example Blue-Green deployment strategy manipulates Kubernetes Service selector and direct production traffic to \"green\" instances only. If you are using this feature then Rollout switches production traffic to Pods that it manages. The switch happens only when the required number of Pod is running and healthy so it is safe in production as well. However, if you want to be extra careful then consider creating a temporal Service or Ingress object to validate Rollout behavior. Once testing is done delete temporal Service/Ingress and switch rollout to production one.","title":"Migrating"},{"location":"migrating/#migrating-to-rollouts","text":"There are ways to migrate to Rollout: Convert an existing Deployment resource to a Rollout resource. Reference an existing Deployment from a Rollout using workloadRef field.","title":"Migrating to Rollouts"},{"location":"migrating/#convert-deployment-to-rollout","text":"When converting a Deployment to a Rollout, it involves changing three fields: Replacing the apiVersion from apps/v1 to argoproj.io/v1alpha1 Replacing the kind from Deployment to Rollout Replacing the deployment strategy with a blue-green or canary strategy Below is an example of a Rollout resource using the canary strategy. apiVersion : argoproj.io/v1alpha1 # Changed from apps/v1 kind : Rollout # Changed from Deployment metadata : name : rollouts-demo spec : selector : matchLabels : app : rollouts-demo template : metadata : labels : app : rollouts-demo spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue ports : - containerPort : 8080 strategy : canary : # Changed from rollingUpdate or recreate steps : - setWeight : 20 - pause : {} Warning When migrating a Deployment which is already serving live production traffic, a Rollout should run next to the Deployment before deleting the Deployment or scaling down the Deployment. Not following this approach might result in downtime . It also allows for the Rollout to be tested before deleting the original Deployment.","title":"Convert Deployment to Rollout"},{"location":"migrating/#reference-deployment-from-rollout","text":"Instead of removing Deployment you can scale-down in to zero and reference from the Rollout resource: Create a Rollout resource. Reference an existing Deployment using workloadRef field. Scale-down existing Deployment by changing replicas field of an existing Deployment to zero. To perform an update, the change should be made to the Pod template field of the Deployment. Below is an example of a Rollout resource referencing a Deployment. apiVersion : argoproj.io/v1alpha1 # Create a rollout resource kind : Rollout metadata : name : rollout-ref-deployment spec : replicas : 5 workloadRef : # Reference an existing Deployment using workloadRef field apiVersion : apps/v1 kind : Deployment name : rollout-ref-deployment strategy : canary : steps : - setWeight : 20 - pause : { duration : 10s } --- apiVersion : apps/v1 kind : Deployment metadata : labels : app.kubernetes.io/instance : rollout-canary name : rollout-ref-deployment spec : replicas : 0 # Scale down existing deployment selector : matchLabels : app : rollout-ref-deployment template : metadata : labels : app : rollout-ref-deployment spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue imagePullPolicy : Always ports : - containerPort : 8080 Consider following if your Deployment runs in production: Running Rollout and Deployment side-by-side After creation Rollout will spinup required number of Pods side-by-side with the Deployment Pods. Rollout won't try to manage existing Deployment Pods. That means you can safely update add Rollout to the production environment without any interruption but you are going to run twice more Pods during migration. Argo-rollouts controller patches the spec of rollout object with an annotation of rollout.argoproj.io/workload-generation , which equals the generation of referenced deployment. Users can detect if the rollout matches desired generation of deployment by checking the workloadObservedGeneration in the rollout status. Traffic Management During Migration The Rollout offers traffic management functionality that manages routing rules and flows the traffic to different versions of an application. For example Blue-Green deployment strategy manipulates Kubernetes Service selector and direct production traffic to \"green\" instances only. If you are using this feature then Rollout switches production traffic to Pods that it manages. The switch happens only when the required number of Pod is running and healthy so it is safe in production as well. However, if you want to be extra careful then consider creating a temporal Service or Ingress object to validate Rollout behavior. Once testing is done delete temporal Service/Ingress and switch rollout to production one.","title":"Reference Deployment From Rollout"},{"location":"releasing/","text":"Releasing \u00b6 Tag a commit to release from using semantic versioning (e.g. v1.0.0-rc1) Visit the Release GitHub Action and enter the tag. When the action completes, visit the generated draft Github releases and enter the details about the release: Getting started (copy from previous release and new version) Changelog Update stable tag: git tag stable --force && git push $REPO stable --force Update Brew formula: git clone git@github.com:argoproj/homebrew-tap.git cd homebrew-tap git pull ./update.sh kubectl-argo-rollouts $VERSION git commit -am \"Update kubectl-argo-rollouts to $VERSION \" git push Verify \u00b6 Install locally using the command below and follow the Getting Started Guide : kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/download/ ${ VERSION } /install.yaml Check the Kubectl Argo Rollout plugin: brew upgrade kubectl-argo-rollouts kubectl argo rollouts version","title":"Releasing"},{"location":"releasing/#releasing","text":"Tag a commit to release from using semantic versioning (e.g. v1.0.0-rc1) Visit the Release GitHub Action and enter the tag. When the action completes, visit the generated draft Github releases and enter the details about the release: Getting started (copy from previous release and new version) Changelog Update stable tag: git tag stable --force && git push $REPO stable --force Update Brew formula: git clone git@github.com:argoproj/homebrew-tap.git cd homebrew-tap git pull ./update.sh kubectl-argo-rollouts $VERSION git commit -am \"Update kubectl-argo-rollouts to $VERSION \" git push","title":"Releasing"},{"location":"releasing/#verify","text":"Install locally using the command below and follow the Getting Started Guide : kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/download/ ${ VERSION } /install.yaml Check the Kubectl Argo Rollout plugin: brew upgrade kubectl-argo-rollouts kubectl argo rollouts version","title":"Verify"},{"location":"roadmap/","text":"Roadmap \u00b6 The item listed here are proposed items for Argo Rollouts and are subject to change. To see where items may fall into releases, visit the github milestones and notice if the item appears in the milestone description. Roadmap Weight Verification Istio Canary using DestinationRule subsets Webhook Notifications Rollback Window Header Based Routing Shadow Traffic v1.0 \u00b6 Weight Verification \u00b6 Issue #701 When Argo Rollouts adjusts a canary weight, it currently assumes that the adjustment was made and moves on to the next step. However, for some traffic routing providers, this change can take a long time to take effect (or possibly never even made) since external factors may cause the change to become delayed. This proposal is to add verification to the traffic routers so that after a setWeight step, the rollout controller could verify that the weight took effect before moving on to the next step. This is especially important for the ALB ingress controller which are affected by things like rate limiting, the ALB ingress controller not running, etc... Istio Canary using DestinationRule subsets \u00b6 Issue #617 Currently, Rollouts supports only host-level traffic splitting using two Kubernetes Services. For some use cases (e.g. east-west canarying intra-cluster), this pattern not desirable and traffic splitting should be achieved using two Istio DestinationRule Subsets instead. Workload Referencing \u00b6 Issue #676 Currently, the Rollout spec contains both the deployment strategy (e.g. blueGreen/canary), as well as the pod template. This proposal is to support a way to reference the pod template definition from another group/kind (e.g. a Deployment, PodTemplate) so that the rollout strategy could be separated from the workload definition. This is motivated by the following use cases: CRDs (e.g. Rollouts) are not supported well in kustomize, and strategic merge patches simply don't work as expected with a Rollout because lists will be replaced and not merged. By referencing a native Kubernetes kind, kustomize would work expectedly against the k8s native referenced object, which is the portion of the spec that users typically want to customize overlays against. During a migration from a Deployment to a Rollout, it has been inconvenient for users to duplicate the entire Deployment spec to a Rollout, and keeping them always in sync during the transition. By referencing the definition, we would be able to eliminate the possibility of pod template spec duplication. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook-canary spec : replicas : 5 workloadRef : apiVersion : apps/v1 kind : Deployment name : guestbook strategy : canary : steps : - setWeight : 20 - pause : { duration : 1h } - setWeight : 40 - pause : { duration : 1h } - setWeight : 60 - pause : { duration : 1h } - setWeight : 80 - pause : { duration : 1h } v1.1+ \u00b6 Webhook Notifications \u00b6 Issue #369 When a rollout transitions state, such as an aborted rollout due to failed analysis, there is no mechanism to notify an external system about the failure. Instead, users must currently put in place something to monitor the rollout, and notice the condition to take action. Monitoring a rollout is not always an option, since it requires that the external system have access to the Kubernetes API server. A webhook notification feature of Rollouts would allow a push-based model where the Rollout controller itself would push an event to an external system, in the form of a webhook/cloud event. Rollback Windows \u00b6 Issue #574 Currently, when an older Rollout manifest is re-applied, the controller treats it the same as a spec change, and will execute the full list of steps, and perform analysis too. There are two exceptions to this rule: 1. the controller detects if it is moving back to a blue-green ReplicaSet which exists and is still scaled up (within its scaleDownDelay) 2. the controller detects it is moving back to the canary's \"stable\" ReplicaSet, and the upgrade had not yet completed. It is often undesirable to re-run analysis and steps for a rollout, when the desired behavior is to rollback as soon as possible. To help with this, a rollback window feature would allow users a window indicate to the controller to Header Based Routing \u00b6 Issue #474 Users who are using Rollout with a service mesh, may want to leverage some of its more advanced features, such as routing traffic via headers instead of purely by percentage. Header based routing provides the ability to route traffic based on a header, instead of a percentage of traffic. This allows more flexibility when canarying, such as providing session stickiness, or only exposing a subset of users with a HTTP cookie or user-agent. Shadow Traffic \u00b6 Issue #474 Some service meshes provide the ability to \"shadow\" live production traffic. A feature in rollouts could provide a canary step to shadow traffic to the canary stack, to see how it responds to the real-world data.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"The item listed here are proposed items for Argo Rollouts and are subject to change. To see where items may fall into releases, visit the github milestones and notice if the item appears in the milestone description. Roadmap Weight Verification Istio Canary using DestinationRule subsets Webhook Notifications Rollback Window Header Based Routing Shadow Traffic","title":"Roadmap"},{"location":"roadmap/#v10","text":"","title":"v1.0"},{"location":"roadmap/#weight-verification","text":"Issue #701 When Argo Rollouts adjusts a canary weight, it currently assumes that the adjustment was made and moves on to the next step. However, for some traffic routing providers, this change can take a long time to take effect (or possibly never even made) since external factors may cause the change to become delayed. This proposal is to add verification to the traffic routers so that after a setWeight step, the rollout controller could verify that the weight took effect before moving on to the next step. This is especially important for the ALB ingress controller which are affected by things like rate limiting, the ALB ingress controller not running, etc...","title":"Weight Verification"},{"location":"roadmap/#istio-canary-using-destinationrule-subsets","text":"Issue #617 Currently, Rollouts supports only host-level traffic splitting using two Kubernetes Services. For some use cases (e.g. east-west canarying intra-cluster), this pattern not desirable and traffic splitting should be achieved using two Istio DestinationRule Subsets instead.","title":"Istio Canary using DestinationRule subsets"},{"location":"roadmap/#workload-referencing","text":"Issue #676 Currently, the Rollout spec contains both the deployment strategy (e.g. blueGreen/canary), as well as the pod template. This proposal is to support a way to reference the pod template definition from another group/kind (e.g. a Deployment, PodTemplate) so that the rollout strategy could be separated from the workload definition. This is motivated by the following use cases: CRDs (e.g. Rollouts) are not supported well in kustomize, and strategic merge patches simply don't work as expected with a Rollout because lists will be replaced and not merged. By referencing a native Kubernetes kind, kustomize would work expectedly against the k8s native referenced object, which is the portion of the spec that users typically want to customize overlays against. During a migration from a Deployment to a Rollout, it has been inconvenient for users to duplicate the entire Deployment spec to a Rollout, and keeping them always in sync during the transition. By referencing the definition, we would be able to eliminate the possibility of pod template spec duplication. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook-canary spec : replicas : 5 workloadRef : apiVersion : apps/v1 kind : Deployment name : guestbook strategy : canary : steps : - setWeight : 20 - pause : { duration : 1h } - setWeight : 40 - pause : { duration : 1h } - setWeight : 60 - pause : { duration : 1h } - setWeight : 80 - pause : { duration : 1h }","title":"Workload Referencing"},{"location":"roadmap/#v11","text":"","title":"v1.1+"},{"location":"roadmap/#webhook-notifications","text":"Issue #369 When a rollout transitions state, such as an aborted rollout due to failed analysis, there is no mechanism to notify an external system about the failure. Instead, users must currently put in place something to monitor the rollout, and notice the condition to take action. Monitoring a rollout is not always an option, since it requires that the external system have access to the Kubernetes API server. A webhook notification feature of Rollouts would allow a push-based model where the Rollout controller itself would push an event to an external system, in the form of a webhook/cloud event.","title":"Webhook Notifications"},{"location":"roadmap/#rollback-windows","text":"Issue #574 Currently, when an older Rollout manifest is re-applied, the controller treats it the same as a spec change, and will execute the full list of steps, and perform analysis too. There are two exceptions to this rule: 1. the controller detects if it is moving back to a blue-green ReplicaSet which exists and is still scaled up (within its scaleDownDelay) 2. the controller detects it is moving back to the canary's \"stable\" ReplicaSet, and the upgrade had not yet completed. It is often undesirable to re-run analysis and steps for a rollout, when the desired behavior is to rollback as soon as possible. To help with this, a rollback window feature would allow users a window indicate to the controller to","title":"Rollback Windows"},{"location":"roadmap/#header-based-routing","text":"Issue #474 Users who are using Rollout with a service mesh, may want to leverage some of its more advanced features, such as routing traffic via headers instead of purely by percentage. Header based routing provides the ability to route traffic based on a header, instead of a percentage of traffic. This allows more flexibility when canarying, such as providing session stickiness, or only exposing a subset of users with a HTTP cookie or user-agent.","title":"Header Based Routing"},{"location":"roadmap/#shadow-traffic","text":"Issue #474 Some service meshes provide the ability to \"shadow\" live production traffic. A feature in rollouts could provide a canary step to shadow traffic to the canary stack, to see how it responds to the real-world data.","title":"Shadow Traffic"},{"location":"security/","text":"Security \u00b6 Reporting Vulnerabilities \u00b6 Please report security vulnerabilities by e-mailing: Jesse_Suen@intuit.com Alexander_Matyushentsev@intuit.com Edward_Lee@intuit.com","title":"Security"},{"location":"security/#security","text":"","title":"Security"},{"location":"security/#reporting-vulnerabilities","text":"Please report security vulnerabilities by e-mailing: Jesse_Suen@intuit.com Alexander_Matyushentsev@intuit.com Edward_Lee@intuit.com","title":"Reporting Vulnerabilities"},{"location":"analysis/cloudwatch/","text":"CloudWatch Metrics \u00b6 Important Available since v1.1.0 A CloudWatch using GetMetricData can be used to obtain measurements for analysis. Setup \u00b6 You can use CloudWatch Metrics if you have used to EKS or not. This analysis is required IAM permission for cloudwatch:GetMetricData and you need to define AWS_REGION in Deployment for argo-rollouts . EKS \u00b6 If you create new cluster on EKS, you can attach cluster IAM role or attach IAM roles for service accounts . If you have already cluster on EKS, you can attach IAM roles for service accounts . not EKS \u00b6 You need to define access key and secret key. apiVersion : v1 kind : Secret metadata : name : cloudwatch-secret type : Opaque stringData : AWS_ACCESS_KEY_ID : <aws-access-key-id> AWS_SECRET_ACCESS_KEY : <aws-secret-access-key> AWS_REGION : <aws-region> apiVersion : apps/v1 kind : Deployment metadata : name : argo-rollouts spec : template : spec : containers : - name : argo-rollouts env : - name : AWS_ACCESS_KEY_ID valueFrom : secretKeyRef : name : cloudwatch-secret key : AWS_ACCESS_KEY_ID - name : AWS_SECRET_ACCESS_KEY valueFrom : secretKeyRef : name : cloudwatch-secret key : AWS_SECRET_ACCESS_KEY - name : AWS_REGION valueFrom : secretKeyRef : name : cloudwatch-secret key : AWS_REGION Configuration \u00b6 metricDataQueries - GetMetricData query: MetricDataQuery interval - optional interval, e.g. 30m, default: 5m apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : metrics : - name : success-rate interval : 1m successCondition : \"len(result[0].Values) >= 5 and all(result[0].Values, {# <= 0.01})\" failureLimit : 3 provider : cloudWatch : interval : 30m metricDataQueries : - { \"id\" : \"rate\" , \"expression\" : \"errors / requests\" } - { \"id\" : \"errors\" , \"metricStat\" : { \"metric\" : { \"namespace\" : \"app\" , \"metricName\" : \"errors\" }, \"period\" : 300 , \"stat\" : \"Sum\" , \"unit\" : \"Count\" }, \"returnData\" : false } - { \"id\" : \"requests\" , \"metricStat\" : { \"metric\" : { \"namespace\" : \"app\" , \"metricName\" : \"requests\" }, \"period\" : 300 , \"stat\" : \"Sum\" , \"unit\" : \"Count\" }, \"returnData\" : false } debug \u00b6 You can confirm the results value in AnalysisRun . $ kubectl get analysisrun/rollouts-name-xxxxxxxxxx-xx -o yaml ( snip ) status: metricResults: - count: 2 failed: 1 measurements: - finishedAt: \"2021-09-08T17:29:14Z\" phase: Failed startedAt: \"2021-09-08T17:29:13Z\" value: '[[0.0029476787030213707 0.006100422336931018 0.01020408163265306 0.007932573128408527 0.00589622641509434 0.006339144215530904]]' - finishedAt: \"2021-09-08T17:30:14Z\" phase: Successful startedAt: \"2021-09-08T17:30:14Z\" value: '[[0.004484304932735426 0.0058374494836102376 0.006736068585425597 0.008444444444444444 0.006859756097560976 0.0045385779122541605]]' name: success-rate phase: Running successful: 1 phase: Running startedAt: \"2021-09-08T17:29:14Z\"","title":"CloudWatch"},{"location":"analysis/cloudwatch/#cloudwatch-metrics","text":"Important Available since v1.1.0 A CloudWatch using GetMetricData can be used to obtain measurements for analysis.","title":"CloudWatch Metrics"},{"location":"analysis/cloudwatch/#setup","text":"You can use CloudWatch Metrics if you have used to EKS or not. This analysis is required IAM permission for cloudwatch:GetMetricData and you need to define AWS_REGION in Deployment for argo-rollouts .","title":"Setup"},{"location":"analysis/cloudwatch/#eks","text":"If you create new cluster on EKS, you can attach cluster IAM role or attach IAM roles for service accounts . If you have already cluster on EKS, you can attach IAM roles for service accounts .","title":"EKS"},{"location":"analysis/cloudwatch/#not-eks","text":"You need to define access key and secret key. apiVersion : v1 kind : Secret metadata : name : cloudwatch-secret type : Opaque stringData : AWS_ACCESS_KEY_ID : <aws-access-key-id> AWS_SECRET_ACCESS_KEY : <aws-secret-access-key> AWS_REGION : <aws-region> apiVersion : apps/v1 kind : Deployment metadata : name : argo-rollouts spec : template : spec : containers : - name : argo-rollouts env : - name : AWS_ACCESS_KEY_ID valueFrom : secretKeyRef : name : cloudwatch-secret key : AWS_ACCESS_KEY_ID - name : AWS_SECRET_ACCESS_KEY valueFrom : secretKeyRef : name : cloudwatch-secret key : AWS_SECRET_ACCESS_KEY - name : AWS_REGION valueFrom : secretKeyRef : name : cloudwatch-secret key : AWS_REGION","title":"not EKS"},{"location":"analysis/cloudwatch/#configuration","text":"metricDataQueries - GetMetricData query: MetricDataQuery interval - optional interval, e.g. 30m, default: 5m apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : metrics : - name : success-rate interval : 1m successCondition : \"len(result[0].Values) >= 5 and all(result[0].Values, {# <= 0.01})\" failureLimit : 3 provider : cloudWatch : interval : 30m metricDataQueries : - { \"id\" : \"rate\" , \"expression\" : \"errors / requests\" } - { \"id\" : \"errors\" , \"metricStat\" : { \"metric\" : { \"namespace\" : \"app\" , \"metricName\" : \"errors\" }, \"period\" : 300 , \"stat\" : \"Sum\" , \"unit\" : \"Count\" }, \"returnData\" : false } - { \"id\" : \"requests\" , \"metricStat\" : { \"metric\" : { \"namespace\" : \"app\" , \"metricName\" : \"requests\" }, \"period\" : 300 , \"stat\" : \"Sum\" , \"unit\" : \"Count\" }, \"returnData\" : false }","title":"Configuration"},{"location":"analysis/cloudwatch/#debug","text":"You can confirm the results value in AnalysisRun . $ kubectl get analysisrun/rollouts-name-xxxxxxxxxx-xx -o yaml ( snip ) status: metricResults: - count: 2 failed: 1 measurements: - finishedAt: \"2021-09-08T17:29:14Z\" phase: Failed startedAt: \"2021-09-08T17:29:13Z\" value: '[[0.0029476787030213707 0.006100422336931018 0.01020408163265306 0.007932573128408527 0.00589622641509434 0.006339144215530904]]' - finishedAt: \"2021-09-08T17:30:14Z\" phase: Successful startedAt: \"2021-09-08T17:30:14Z\" value: '[[0.004484304932735426 0.0058374494836102376 0.006736068585425597 0.008444444444444444 0.006859756097560976 0.0045385779122541605]]' name: success-rate phase: Running successful: 1 phase: Running startedAt: \"2021-09-08T17:29:14Z\"","title":"debug"},{"location":"analysis/datadog/","text":"Datadog Metrics \u00b6 Important Available since v0.10.0 A Datadog query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : loq-error-rate spec : args : - name : service-name metrics : - name : error-rate interval : 5m successCondition : result <= 0.01 failureLimit : 3 provider : datadog : interval : 5m query : | sum:requests.error.count{service:{{args.service-name}}} / sum:requests.request.count{service:{{args.service-name}}} Datadog api and app tokens can be configured in a kubernetes secret in argo-rollouts namespace. apiVersion : v1 kind : Secret metadata : name : datadog type : Opaque data : address : https://api.datadoghq.com api-key : <datadog-api-key> app-key : <datadog-app-key>","title":"DataDog"},{"location":"analysis/datadog/#datadog-metrics","text":"Important Available since v0.10.0 A Datadog query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : loq-error-rate spec : args : - name : service-name metrics : - name : error-rate interval : 5m successCondition : result <= 0.01 failureLimit : 3 provider : datadog : interval : 5m query : | sum:requests.error.count{service:{{args.service-name}}} / sum:requests.request.count{service:{{args.service-name}}} Datadog api and app tokens can be configured in a kubernetes secret in argo-rollouts namespace. apiVersion : v1 kind : Secret metadata : name : datadog type : Opaque data : address : https://api.datadoghq.com api-key : <datadog-api-key> app-key : <datadog-app-key>","title":"Datadog Metrics"},{"location":"analysis/graphite/","text":"Graphite Metrics \u00b6 A Graphite query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m # Note that the Argo Rollouts Graphite metrics provider returns results as an array of float64s with 6 decimal places. successCondition : results[0] >= 90.000000 failureLimit : 3 provider : graphite : address : http://graphite.example.com:9090 query : | target=summarize( asPercent( sumSeries( stats.timers.httpServerRequests.app.{{args.service-name}}.exception.*.method.*.outcome.{CLIENT_ERROR,INFORMATIONAL,REDIRECTION,SUCCESS}.status.*.uri.*.count ), sumSeries( stats.timers.httpServerRequests.app.{{args.service-name}}.exception.*.method.*.outcome.*.status.*.uri.*.count ) ), '5min', 'avg' )","title":"Graphite Metrics"},{"location":"analysis/graphite/#graphite-metrics","text":"A Graphite query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m # Note that the Argo Rollouts Graphite metrics provider returns results as an array of float64s with 6 decimal places. successCondition : results[0] >= 90.000000 failureLimit : 3 provider : graphite : address : http://graphite.example.com:9090 query : | target=summarize( asPercent( sumSeries( stats.timers.httpServerRequests.app.{{args.service-name}}.exception.*.method.*.outcome.{CLIENT_ERROR,INFORMATIONAL,REDIRECTION,SUCCESS}.status.*.uri.*.count ), sumSeries( stats.timers.httpServerRequests.app.{{args.service-name}}.exception.*.method.*.outcome.*.status.*.uri.*.count ) ), '5min', 'avg' )","title":"Graphite Metrics"},{"location":"analysis/job/","text":"Job Metrics \u00b6 A Kubernetes Job can be used to run analysis. When a Job is used, the metric is considered successful if the Job completes and had an exit code of zero, otherwise it is failed. metrics : - name : test provider : job : spec : backoffLimit : 1 template : spec : containers : - name : test image : my-image:latest command : [ my-test-script , my-service.default.svc.cluster.local ] restartPolicy : Never","title":"Job"},{"location":"analysis/job/#job-metrics","text":"A Kubernetes Job can be used to run analysis. When a Job is used, the metric is considered successful if the Job completes and had an exit code of zero, otherwise it is failed. metrics : - name : test provider : job : spec : backoffLimit : 1 template : spec : containers : - name : test image : my-image:latest command : [ my-test-script , my-service.default.svc.cluster.local ] restartPolicy : Never","title":"Job Metrics"},{"location":"analysis/kayenta/","text":"Kayenta (e.g. Mann-Whitney Analysis) \u00b6 Analysis can also be done as part of an Experiment . This example starts both a canary and baseline ReplicaSet. The ReplicaSets run for 1 hour, then scale down to zero. Call out to Kayenta to perform Mann-Whintney analysis against the two pods. Demonstrates ability to start a short-lived experiment and an asynchronous analysis. This example demonstrates: The ability to start an Experiment as part of rollout steps, which launches multiple ReplicaSets (e.g. baseline & canary) The ability to reference and supply pod-template-hash to an AnalysisRun Kayenta metrics Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : strategy : canary : steps : - experiment : duration : 1h templates : - name : baseline specRef : stable - name : canary specRef : canary analyses : - templateName : mann-whitney args : - name : stable-hash valueFrom : podTemplateHashValue : Stable - name : canary-hash valueFrom : podTemplateHashValue : Latest AnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : start-time - name : end-time - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.example.com application : guestbook canaryConfigName : my-test thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 start : \"{{args.start-time}}\" end : \"{{args.end-time}}\" experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60 start : \"{{args.start-time}}\" end : \"{{args.end-time}}\" Experiment # This is the resulting experiment that is produced by the step apiVersion : argoproj.io/v1alpha1 kind : Experiment name : name : guestbook-6c54544bf9-0 spec : duration : 1h templates : - name : baseline replicas : 1 spec : containers : - name : guestbook image : guestbook:v1 - name : canary replicas : 1 spec : containers : - name : guestbook image : guestbook:v2 analysis : templateName : mann-whitney args : - name : start-time value : \"{{experiment.availableAt}}\" - name : end-time value : \"{{experiment.finishedAt}}\" In order to perform multiple kayenta runs over some time duration, the interval and count fields can be supplied. When the start and end fields are omitted from the kayenta scopes, the values will be implicitly decided as: start = if lookback: true start of analysis, otherwise current time - interval end = current time apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.intuit.com application : guestbook canaryConfigName : my-test interval : 3600 count : 3 # loopback will cause start time value to be equal to start of analysis # lookback: true thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60","title":"Kayenta"},{"location":"analysis/kayenta/#kayenta-eg-mann-whitney-analysis","text":"Analysis can also be done as part of an Experiment . This example starts both a canary and baseline ReplicaSet. The ReplicaSets run for 1 hour, then scale down to zero. Call out to Kayenta to perform Mann-Whintney analysis against the two pods. Demonstrates ability to start a short-lived experiment and an asynchronous analysis. This example demonstrates: The ability to start an Experiment as part of rollout steps, which launches multiple ReplicaSets (e.g. baseline & canary) The ability to reference and supply pod-template-hash to an AnalysisRun Kayenta metrics Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : strategy : canary : steps : - experiment : duration : 1h templates : - name : baseline specRef : stable - name : canary specRef : canary analyses : - templateName : mann-whitney args : - name : stable-hash valueFrom : podTemplateHashValue : Stable - name : canary-hash valueFrom : podTemplateHashValue : Latest AnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : start-time - name : end-time - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.example.com application : guestbook canaryConfigName : my-test thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 start : \"{{args.start-time}}\" end : \"{{args.end-time}}\" experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60 start : \"{{args.start-time}}\" end : \"{{args.end-time}}\" Experiment # This is the resulting experiment that is produced by the step apiVersion : argoproj.io/v1alpha1 kind : Experiment name : name : guestbook-6c54544bf9-0 spec : duration : 1h templates : - name : baseline replicas : 1 spec : containers : - name : guestbook image : guestbook:v1 - name : canary replicas : 1 spec : containers : - name : guestbook image : guestbook:v2 analysis : templateName : mann-whitney args : - name : start-time value : \"{{experiment.availableAt}}\" - name : end-time value : \"{{experiment.finishedAt}}\" In order to perform multiple kayenta runs over some time duration, the interval and count fields can be supplied. When the start and end fields are omitted from the kayenta scopes, the values will be implicitly decided as: start = if lookback: true start of analysis, otherwise current time - interval end = current time apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : mann-whitney spec : args : - name : stable-hash - name : canary-hash metrics : - name : mann-whitney provider : kayenta : address : https://kayenta.intuit.com application : guestbook canaryConfigName : my-test interval : 3600 count : 3 # loopback will cause start time value to be equal to start of analysis # lookback: true thresholds : pass : 90 marginal : 75 scopes : - name : default controlScope : scope : app=guestbook and rollouts-pod-template-hash={{args.stable-hash}} step : 60 experimentScope : scope : app=guestbook and rollouts-pod-template-hash={{args.canary-hash}} step : 60","title":"Kayenta (e.g. Mann-Whitney Analysis)"},{"location":"analysis/newrelic/","text":"NewRelic Metrics \u00b6 Important Available since v0.10.0 A New Relic query using NRQL can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : application-name metrics : - name : success-rate successCondition : result.successRate >= 0.95 provider : newRelic : profile : my-newrelic-secret # optional, defaults to 'newrelic' query : | FROM Transaction SELECT percentage(count(*), WHERE httpResponseCode != 500) as successRate where appName = '{{ args.application-name }}' The result evaluated for the condition will always be map or list of maps. The name will follow the pattern of either function or function.field , e.g. SELECT average(duration) from Transaction will yield average.duration . In this case the field result cannot be accessed with dot notation and instead should be accessed like result['average.duration'] . Query results can be renamed using the NRQL clause AS as seen above. A New Relic access profile can be configured using a Kubernetes secret in the argo-rollouts namespace. Alternate accounts can be used by creating more secrets of the same format and specifying which secret to use in the metric provider configuration using the profile field. apiVersion : v1 kind : Secret metadata : name : newrelic type : Opaque data : personal-api-key : <newrelic-personal-api-key> account-id : <newrelic-account-id> region : \"us\" # optional, defaults to \"us\" if not set. Only set to \"eu\" if you use EU New Relic To use the New Relic metric provider from behind a proxy, provide a base-url-rest key pointing to the base URL of the New Relic REST API for your proxy, and a base-url-nerdgraph key pointing to the base URL for NerdGraph for your proxy: apiVersion : v1 kind : Secret metadata : name : newrelic type : Opaque data : personal-api-key : <newrelic-personal-api-key> account-id : <newrelic-account-id> region : \"us\" # optional, defaults to \"us\" if not set. Only set to \"eu\" if you use EU New Relic base-url-rest : <your-base-url> base-url-nerdgraph : <your-base-url>","title":"NewRelic"},{"location":"analysis/newrelic/#newrelic-metrics","text":"Important Available since v0.10.0 A New Relic query using NRQL can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : application-name metrics : - name : success-rate successCondition : result.successRate >= 0.95 provider : newRelic : profile : my-newrelic-secret # optional, defaults to 'newrelic' query : | FROM Transaction SELECT percentage(count(*), WHERE httpResponseCode != 500) as successRate where appName = '{{ args.application-name }}' The result evaluated for the condition will always be map or list of maps. The name will follow the pattern of either function or function.field , e.g. SELECT average(duration) from Transaction will yield average.duration . In this case the field result cannot be accessed with dot notation and instead should be accessed like result['average.duration'] . Query results can be renamed using the NRQL clause AS as seen above. A New Relic access profile can be configured using a Kubernetes secret in the argo-rollouts namespace. Alternate accounts can be used by creating more secrets of the same format and specifying which secret to use in the metric provider configuration using the profile field. apiVersion : v1 kind : Secret metadata : name : newrelic type : Opaque data : personal-api-key : <newrelic-personal-api-key> account-id : <newrelic-account-id> region : \"us\" # optional, defaults to \"us\" if not set. Only set to \"eu\" if you use EU New Relic To use the New Relic metric provider from behind a proxy, provide a base-url-rest key pointing to the base URL of the New Relic REST API for your proxy, and a base-url-nerdgraph key pointing to the base URL for NerdGraph for your proxy: apiVersion : v1 kind : Secret metadata : name : newrelic type : Opaque data : personal-api-key : <newrelic-personal-api-key> account-id : <newrelic-account-id> region : \"us\" # optional, defaults to \"us\" if not set. Only set to \"eu\" if you use EU New Relic base-url-rest : <your-base-url> base-url-nerdgraph : <your-base-url>","title":"NewRelic Metrics"},{"location":"analysis/prometheus/","text":"Prometheus Metrics \u00b6 A Prometheus query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m # NOTE: prometheus queries return results in the form of a vector. # So it is common to access the index 0 of the returned array to obtain the value successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) The example shows Istio metrics, but you can use any kind of metric available to your prometheus instance. We suggest you validate your PromQL expression using the Prometheus GUI first . See the Analysis Overview page for more details on the available options.","title":"Prometheus"},{"location":"analysis/prometheus/#prometheus-metrics","text":"A Prometheus query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m # NOTE: prometheus queries return results in the form of a vector. # So it is common to access the index 0 of the returned array to obtain the value successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) The example shows Istio metrics, but you can use any kind of metric available to your prometheus instance. We suggest you validate your PromQL expression using the Prometheus GUI first . See the Analysis Overview page for more details on the available options.","title":"Prometheus Metrics"},{"location":"analysis/wavefront/","text":"Wavefront Metrics \u00b6 A Wavefront query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result >= 0.95 failureLimit : 3 provider : wavefront : address : example.wavefront.com query : | sum(rate( 5m, ts(\"istio.requestcount.count\", response_code!=500 and destination_service=\"{{args.service-name}}\" ))) / sum(rate( 5m, ts(\"istio.requestcount.count\", reporter=client and destination_service=\"{{args.service-name}}\" ))) Wavefront api tokens can be configured in a kubernetes secret in argo-rollouts namespace. apiVersion : v1 kind : Secret metadata : name : wavefront-api-tokens type : Opaque data : example1.wavefront.com : <token1> example2.wavefront.com : <token2>","title":"Wavefront"},{"location":"analysis/wavefront/#wavefront-metrics","text":"A Wavefront query can be used to obtain measurements for analysis. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result >= 0.95 failureLimit : 3 provider : wavefront : address : example.wavefront.com query : | sum(rate( 5m, ts(\"istio.requestcount.count\", response_code!=500 and destination_service=\"{{args.service-name}}\" ))) / sum(rate( 5m, ts(\"istio.requestcount.count\", reporter=client and destination_service=\"{{args.service-name}}\" ))) Wavefront api tokens can be configured in a kubernetes secret in argo-rollouts namespace. apiVersion : v1 kind : Secret metadata : name : wavefront-api-tokens type : Opaque data : example1.wavefront.com : <token1> example2.wavefront.com : <token2>","title":"Wavefront Metrics"},{"location":"analysis/web/","text":"Web Metrics \u00b6 An HTTP request can be performed against some external service to obtain the measurement. This example makes a HTTP GET request to some URL. The webhook response must return JSON content. The result of the optional jsonPath expression will be assigned to the result variable that can be referenced in the successCondition and failureCondition expressions. If omitted, will use the entire body of the as the result variable. metrics : - name : webmetric successCondition : result == true provider : web : url : \"http://my-server.com/api/v1/measurement?service={{ args.service-name }}\" timeoutSeconds : 20 # defaults to 10 seconds headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" jsonPath : \"{$.data.ok}\" In the following example, given the payload, the measurement will be Successful if the data.ok field was true , and the data.successPercent was greater than 0.90 { \"data\" : { \"ok\" : true , \"successPercent\" : 0.95 } } metrics : - name : webmetric successCondition : \"result.ok && result.successPercent >= 0.90\" provider : web : url : \"http://my-server.com/api/v1/measurement?service={{ args.service-name }}\" headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" jsonPath : \"{$.data}\" NOTE: if the result is a string, two convenience functions asInt and asFloat are provided to convert a result value to a numeric type so that mathematical comparison operators can be used (e.g. >, <, >=, <=). Optional web methods \u00b6 It is possible to use a POST or PUT requests, by specifying the method and body fields metrics : - name : webmetric successCondition : result == true provider : web : method : POST # valid values are GET|POST|PUT, defaults to GET url : \"http://my-server.com/api/v1/measurement?service={{ args.service-name }}\" timeoutSeconds : 20 # defaults to 10 seconds headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" - key : Content-Type # if body is a json, it is recommended to set the Content-Type value : \"application/json\" body : \"{\\\"key\\\": \\\"string value\\\"}\" jsonPath : \"{$.data.ok}\" !!! tip In order to send in JSON, you have to encode it yourself, and send the correct Content-Type as well. Setting a body field for a GET request will result in an error.","title":"Web"},{"location":"analysis/web/#web-metrics","text":"An HTTP request can be performed against some external service to obtain the measurement. This example makes a HTTP GET request to some URL. The webhook response must return JSON content. The result of the optional jsonPath expression will be assigned to the result variable that can be referenced in the successCondition and failureCondition expressions. If omitted, will use the entire body of the as the result variable. metrics : - name : webmetric successCondition : result == true provider : web : url : \"http://my-server.com/api/v1/measurement?service={{ args.service-name }}\" timeoutSeconds : 20 # defaults to 10 seconds headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" jsonPath : \"{$.data.ok}\" In the following example, given the payload, the measurement will be Successful if the data.ok field was true , and the data.successPercent was greater than 0.90 { \"data\" : { \"ok\" : true , \"successPercent\" : 0.95 } } metrics : - name : webmetric successCondition : \"result.ok && result.successPercent >= 0.90\" provider : web : url : \"http://my-server.com/api/v1/measurement?service={{ args.service-name }}\" headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" jsonPath : \"{$.data}\" NOTE: if the result is a string, two convenience functions asInt and asFloat are provided to convert a result value to a numeric type so that mathematical comparison operators can be used (e.g. >, <, >=, <=).","title":"Web Metrics"},{"location":"analysis/web/#optional-web-methods","text":"It is possible to use a POST or PUT requests, by specifying the method and body fields metrics : - name : webmetric successCondition : result == true provider : web : method : POST # valid values are GET|POST|PUT, defaults to GET url : \"http://my-server.com/api/v1/measurement?service={{ args.service-name }}\" timeoutSeconds : 20 # defaults to 10 seconds headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" - key : Content-Type # if body is a json, it is recommended to set the Content-Type value : \"application/json\" body : \"{\\\"key\\\": \\\"string value\\\"}\" jsonPath : \"{$.data.ok}\" !!! tip In order to send in JSON, you have to encode it yourself, and send the correct Content-Type as well. Setting a body field for a GET request will result in an error.","title":"Optional web methods"},{"location":"features/analysis/","text":"Analysis & Progressive Delivery \u00b6 Argo Rollouts provides several ways to perform analysis to drive progressive delivery. This document describes how to achieve various forms of progressive delivery, varying the point in time analysis is performed, it's frequency, and occurrence. Custom Resource Definitions \u00b6 CRD Description Rollout A Rollout acts as a drop-in replacement for a Deployment resource. It provides additional blueGreen and canary update strategies. These strategies can create AnalysisRuns and Experiments during the update, which will progress the update, or abort it. AnalysisTemplate An AnalysisTemplate is a template spec which defines how to perform a canary analysis, such as the metrics which it should perform, its frequency, and the values which are considered successful or failed. AnalysisTemplates may be parameterized with inputs values. ClusterAnalysisTemplate A ClusterAnalysisTemplate is like an AnalysisTemplate , but it is not limited to its namespace. It can be used by any Rollout throughout the cluster. AnalysisRun An AnalysisRun is an instantiation of an AnalysisTemplate . AnalysisRuns are like Jobs in that they eventually complete. Completed runs are considered Successful, Failed, or Inconclusive, and the result of the run affect if the Rollout's update will continue, abort, or pause, respectively. Experiment An Experiment is limited run of one or more ReplicaSets for the purposes of analysis. Experiments typically run for a pre-determined duration, but can also run indefinitely until stopped. Experiments may reference an AnalysisTemplate to run during or after the experiment. The canonical use case for an Experiment is to start a baseline and canary deployment in parallel, and compare the metrics produced by the baseline and canary pods for an equal comparison. Background Analysis \u00b6 Analysis can be run in the background -- while the canary is progressing through its rollout steps. The following example gradually increments the canary weight by 20% every 10 minutes until it reaches 100%. In the background, an AnalysisRun is started based on the AnalysisTemplate named success-rate . The success-rate template queries a prometheus server, measuring the HTTP success rates at 5 minute intervals/samples. It has no end time, and continues until stopped or failed. If the metric is measured to be less than 95%, and there are three such measurements, the analysis is considered Failed. The failed analysis causes the Rollout to abort, setting the canary weight back to zero, and the Rollout would be considered in a Degraded . Otherwise, if the rollout completes all of its canary steps, the rollout is considered successful and the analysis run is stopped by the controller. This example highlights: Background analysis style of progressive delivery Using a Prometheus query to perform a measurement The ability to parameterize the analysis Delay starting the analysis run until step 3 (Set Weight 40%) Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templates : - templateName : success-rate startingStep : 2 # delay starting analysis run until setWeight: 40% args : - name : service-name value : guestbook-svc.default.svc.cluster.local steps : - setWeight : 20 - pause : { duration : 10m } - setWeight : 40 - pause : { duration : 10m } - setWeight : 60 - pause : { duration : 10m } - setWeight : 80 - pause : { duration : 10m } AnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m # NOTE: prometheus queries return results in the form of a vector. # So it is common to access the index 0 of the returned array to obtain the value successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Inline Analysis \u00b6 Analysis can also be performed as a rollout step as an inline \"analysis\" step. When analysis is performed \"inlined,\" an AnalysisRun is started when the step is reached, and blocks the rollout until the run is completed. The success or failure of the analysis run decides if the rollout will proceed to the next step, or abort the rollout completely. This example sets the canary weight to 20%, pauses for 5 minutes, then runs an analysis. If the analysis was successful, continues with rollout, otherwise aborts. This example demonstrates: The ability to invoke an analysis in-line as part of steps apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : steps : - setWeight : 20 - pause : { duration : 5m } - analysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local In this example, the AnalysisTemplate is identical to the background analysis example, but since no interval is specified, the analysis will perform a single measurement and complete. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name - name : prometheus-port value : 9090 metrics : - name : success-rate successCondition : result[0] >= 0.95 provider : prometheus : address : \"http://prometheus.example.com:{{args.prometheus-port}}\" query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Multiple measurements can be performed over a longer duration period, by specifying the count and interval fields: metrics : - name : success-rate successCondition : result[0] >= 0.95 interval : 60s count : 5 provider : prometheus : address : http://prometheus.example.com:9090 query : ... ClusterAnalysisTemplates \u00b6 Important Available since v0.9.0 A Rollout can reference a Cluster scoped AnalysisTemplate called a ClusterAnalysisTemplate . This can be useful when you want to share an AnalysisTemplate across multiple Rollouts; in different namespaces, and avoid duplicating the same template in every namespace. Use the field clusterScope: true to reference a ClusterAnalysisTemplate instead of an AnalysisTemplate. Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : steps : - setWeight : 20 - pause : { duration : 5m } - analysis : templates : - templateName : success-rate clusterScope : true args : - name : service-name value : guestbook-svc.default.svc.cluster.local ClusterAnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : ClusterAnalysisTemplate metadata : name : success-rate spec : args : - name : service-name - name : prometheus-port value : 9090 metrics : - name : success-rate successCondition : result[0] >= 0.95 provider : prometheus : address : \"http://prometheus.example.com:{{args.prometheus-port}}\" query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Note The resulting AnalysisRun will still run in the namespace of the Rollout Analysis with Multiple Templates \u00b6 A Rollout can reference multiple AnalysisTemplates when constructing an AnalysisRun. This allows users to compose analysis from multiple AnalysisTemplates. If multiple templates are referenced, then the controller will merge the templates together. The controller combines the metrics and args fields of all the templates. Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templates : - templateName : success-rate - templateName : error-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local AnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) --- apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : error-rate spec : args : - name : service-name metrics : - name : error-rate interval : 5m successCondition : result[0] <= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code=~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) AnalysisRun # NOTE: Generated AnalysisRun from the multiple templates apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun metadata : name : guestbook-CurrentPodHash-multiple-templates spec : args : - name : service-name value : guestbook-svc.default.svc.cluster.local metrics : - name : success-rate interval : 5m successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) - name : error-rate interval : 5m successCondition : result[0] <= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code=~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Note The controller will error when merging the templates if: Multiple metrics in the templates have the same name Two arguments with the same name have different default values no matter the argument value in Rollout Analysis Template Arguments \u00b6 AnalysisTemplates may declare a set of arguments that can be passed by Rollouts. The args can then be used as in metrics configuration and are resolved at the time the AnalysisRun is created. Argument placeholders are defined as {{ args.<name> }} . apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : args-example spec : args : # required in Rollout due to no default value - name : service-name - name : stable-hash - name : latest-hash # optional in Rollout given the default value - name : api-url value : http://example/measure # from secret - name : api-token valueFrom : secretKeyRef : name : token-secret key : apiToken metrics : - name : webmetric successCondition : result == 'true' provider : web : # placeholders are resolved when an AnalysisRun is created url : \"{{ args.api-url }}?service={{ args.service-name }}\" headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" jsonPath : \"{$.results.ok}\" Analysis arguments defined in a Rollout are merged with the args from the AnalysisTemplate when the AnalysisRun is created. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templates : - templateName : args-example args : # required value - name : service-name value : guestbook-svc.default.svc.cluster.local # override default value - name : api-url value : http://other-api # pod template hash from the stable ReplicaSet - name : stable-hash valueFrom : podTemplateHashValue : Stable # pod template hash from the latest ReplicaSet - name : latest-hash valueFrom : podTemplateHashValue : Latest Analysis arguments also support valueFrom for reading any Rollout fields and passing them as arguments to AnalysisTemplate. An example would be to reference metadata labels like env and region and passing them along to AnalysisTemplate, or any field from the Rollout status apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : appType : demo-app buildType : nginx-app ... env : dev region : us-west-2 spec : ... strategy : canary : analysis : templates : - templateName : args-example args : ... - name : env valueFrom : fieldRef : fieldPath : metadata.labels['env'] # region where this app is deployed - name : region valueFrom : fieldRef : fieldPath : metadata.labels['region'] - name : canary-hash valueFrom : fieldRef : fieldPath : status.canary.weights.canary.podTemplateHash BlueGreen Pre Promotion Analysis \u00b6 A Rollout using the BlueGreen strategy can launch an AnalysisRun before it switches traffic to the new version using pre-promotion. This can be used to block the Service selector switch until the AnalysisRun finishes successfully. The success or failure of the AnalysisRun decides if the Rollout switches traffic, or abort the Rollout completely. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : blueGreen : activeService : active-svc previewService : preview-svc prePromotionAnalysis : templates : - templateName : smoke-tests args : - name : service-name value : preview-svc.default.svc.cluster.local In this example, the Rollout creates a pre-promotion AnalysisRun once the new ReplicaSet is fully available. The Rollout will not switch traffic to the new version until the analysis run finishes successfully. Note: if the autoPromotionSeconds field is specified and the Rollout has waited auto promotion seconds amount of time, the Rollout marks the AnalysisRun successful and switches the traffic to a new version automatically. If the AnalysisRun completes before then, the Rollout will not create another AnalysisRun and wait out the rest of the autoPromotionSeconds . BlueGreen Post Promotion Analysis \u00b6 A Rollout using a BlueGreen strategy can launch an analysis run after the traffic switch to the new version using post-promotion analysis. If post-promotion Analysis fails or errors, the Rollout enters an aborted state and switches traffic back to the previous stable Replicaset. When post-analysis is Successful, the Rollout is considered fully promoted and the new ReplicaSet will be marked as stable. The old ReplicaSet will then be scaled down according to scaleDownDelaySeconds (default 30 seconds). apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : blueGreen : activeService : active-svc previewService : preview-svc scaleDownDelaySeconds : 600 # 10 minutes postPromotionAnalysis : templates : - templateName : smoke-tests args : - name : service-name value : preview-svc.default.svc.cluster.local Failure Conditions and Failure Limit \u00b6 failureCondition can be used to cause an analysis run to fail. failureLimit is the maximum number of failed run an analysis is allowed. The following example continually polls the defined Prometheus server to get the total number of errors(i.e., HTTP response code >= 500) every 5 minutes, causing the measurement to fail if ten or more errors are encountered. The entire analysis run is considered as Failed after three failed measurements. metrics : - name : total-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code=~\"5.*\"}[5m] )) Dry-Run Mode \u00b6 Important Available since v1.2 dryRun can be used on a metric to control whether or not to evaluate that metric in a dry-run mode. A metric running in the dry-run mode won't impact the final state of the rollout or experiment even if it fails or the evaluation comes out as inconclusive. The following example queries prometheus every 5 minutes to get the total number of 4XX and 5XX errors, and even if the evaluation of the metric to monitor the 5XX error-rate fail, the analysis run will pass. dryRun : - metricName : total-5xx-errors metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] )) RegEx matches are also supported. .* can be used to make all the metrics run in the dry-run mode. In the following example, even if one or both metrics fail, the analysis run will pass. dryRun : - metricName : .* metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] )) Dry-Run Summary \u00b6 If one or more metrics are running in the dry-run mode, the summary of the dry-run results gets appended to the analysis run message. Assuming that the total-4xx-errors metric fails in the above example but, the total-5xx-errors succeeds, the final dry-run summary will look like this. Message : Run Terminated Run Summary : ... Dry Run Summary : Count : 2 Successful : 1 Failed : 1 Metric Results : ... Dry-Run Rollouts \u00b6 If a rollout wants to dry run its analysis, it simply needs to specify the dryRun field to its analysis stanza. In the following example, all the metrics from random-fail and always-pass get merged and executed in the dry-run mode. kind : Rollout spec : ... steps : - analysis : templates : - templateName : random-fail - templateName : always-pass dryRun : - metricName : .* Dry-Run Experiments \u00b6 If an experiment wants to dry run its analysis, it simply needs to specify the dryRun field under its specs. In the following example, all the metrics from analyze-job matching the RegEx rule test.* will be executed in the dry-run mode. kind : Experiment spec : templates : - name : baseline selector : matchLabels : app : rollouts-demo template : metadata : labels : app : rollouts-demo spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue analyses : - name : analyze-job templateName : analyze-job dryRun : - metricName : test.* Measurements Retention \u00b6 Important Available since v1.2 measurementRetention can be used to retain other than the latest ten results for the metrics running in any mode (dry/non-dry). Setting this option to 0 would disable it and, the controller will revert to the existing behavior of retaining the latest ten measurements. The following example queries Prometheus every 5 minutes to get the total number of 4XX and 5XX errors and retains the latest twenty measurements for the 5XX metric run results instead of the default ten. measurementRetention : - metricName : total-5xx-errors limit : 20 metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] )) RegEx matches are also supported. .* can be used to apply the same retention rule to all the metrics. In the following example, the controller will retain the latest twenty run results for all the metrics instead of the default ten results. measurementRetention : - metricName : .* limit : 20 metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] )) Inconclusive Runs \u00b6 Analysis runs can also be considered Inconclusive , which indicates the run was neither successful, nor failed. Inconclusive runs causes a rollout to become paused at its current step. Manual intervention is then needed to either resume the rollout, or abort. One example of how analysis runs could become Inconclusive , is when a metric defines no success or failure conditions. metrics : - name : my-query provider : prometheus : address : http://prometheus.example.com:9090 query : ... Inconclusive analysis runs might also happen when both success and failure conditions are specified, but the measurement value did not meet either condition. metrics : - name : success-rate successCondition : result[0] >= 0.90 failureCondition : result[0] < 0.50 provider : prometheus : address : http://prometheus.example.com:9090 query : ... A use case for having Inconclusive analysis runs are to enable Argo Rollouts to automate the execution of analysis runs, and collect the measurement, but still allow human judgement to decide whether or not measurement value is acceptable and decide to proceed or abort. Delay Analysis Runs \u00b6 If the analysis run does not need to start immediately (i.e give the metric provider time to collect metrics on the canary version), Analysis Runs can delay the specific metric analysis. Each metric can be configured to have a different delay. In additional to the metric specific delays, the rollouts with background analysis can delay creating an analysis run until a certain step is reached Delaying a specific analysis metric: metrics : - name : success-rate # Do not start this analysis until 5 minutes after the analysis run starts initialDelay : 5m successCondition : result[0] >= 0.90 provider : prometheus : address : http://prometheus.example.com:9090 query : ... Delaying starting background analysis run until step 3 (Set Weight 40%): apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : strategy : canary : analysis : templates : - templateName : success-rate startingStep : 2 steps : - setWeight : 20 - pause : { duration : 10m } - setWeight : 40 - pause : { duration : 10m } Referencing Secrets \u00b6 AnalysisTemplates and AnalysisRuns can reference secret objects in .spec.args . This allows users to securely pass authentication information to Metric Providers, like login credentials or API tokens. An AnalysisRun can only reference secrets from the same namespace as it's running in. This is only relevant for AnalysisRuns, since AnalysisTemplates do not resolve the secret. In the following example, an AnalysisTemplate references an API token and passes it to a Web metric provider. This example demonstrates: The ability to reference a secret in the AnalysisTemplate .spec.args The ability to pass secret arguments to Metric Providers apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate spec : args : - name : api-token valueFrom : secretKeyRef : name : token-secret key : apiToken metrics : - name : webmetric provider : web : headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" Handling Metric Results \u00b6 NaN and Infinity \u00b6 Metric providers can sometimes return values of NaN (not a number) and infinity. Users can edit the successCondition and failureCondition fields to handle these cases accordingly. Here are three examples where a metric result of NaN is considered successful, inconclusive and failed respectively. apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : isNaN(result) || result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Successful startedAt : \"2021-02-10T00:15:26Z\" value : NaN name : success-rate phase : Successful successful : 1 phase : Successful startedAt : \"2021-02-10T00:15:26Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : result >= 0.95 failureCondition : result < 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Inconclusive startedAt : \"2021-02-10T00:15:26Z\" value : NaN name : success-rate phase : Inconclusive successful : 1 phase : Inconclusive startedAt : \"2021-02-10T00:15:26Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Failed startedAt : \"2021-02-10T00:15:26Z\" value : NaN name : success-rate phase : Failed successful : 1 phase : Failed startedAt : \"2021-02-10T00:15:26Z\" Here are two examples where a metric result of infinity is considered successful and failed respectively. apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Successful startedAt : \"2021-02-10T00:15:26Z\" value : +Inf name : success-rate phase : Successful successful : 1 phase : Successful startedAt : \"2021-02-10T00:15:26Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... failureCondition : isInf(result) status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Failed startedAt : \"2021-02-10T00:15:26Z\" value : +Inf name : success-rate phase : Failed successful : 1 phase : Failed startedAt : \"2021-02-10T00:15:26Z\" Empty array \u00b6 Prometheus \u00b6 Metric providers can sometimes return empty array, e.g., no data returned from prometheus query. Here are two examples where a metric result of empty array is considered successful and failed respectively. apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : len(result) == 0 || result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-09-08T19:15:49Z\" phase : Successful startedAt : \"2021-09-08T19:15:49Z\" value : '[]' name : success-rate phase : Successful successful : 1 phase : Successful startedAt : \"2021-09-08T19:15:49Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : len(result) > 0 && result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-09-08T19:19:44Z\" phase : Failed startedAt : \"2021-09-08T19:19:44Z\" value : '[]' name : success-rate phase : Failed successful : 1 phase : Failed startedAt : \"2021-09-08T19:19:44Z\" Datadog \u00b6 Datadog queries can return empty results if the query takes place during a time interval with no metrics. The Datadog provider will return a nil value yielding an error during the evaluation phase like: invalid operation: < (mismatched types <nil> and float64) However, empty query results yielding a nil value can be handled using the default() function. Here is a succeeding example using the default() function: successCondition : default(result, 0) < 0.05","title":"Overview"},{"location":"features/analysis/#analysis-progressive-delivery","text":"Argo Rollouts provides several ways to perform analysis to drive progressive delivery. This document describes how to achieve various forms of progressive delivery, varying the point in time analysis is performed, it's frequency, and occurrence.","title":"Analysis &amp; Progressive Delivery"},{"location":"features/analysis/#custom-resource-definitions","text":"CRD Description Rollout A Rollout acts as a drop-in replacement for a Deployment resource. It provides additional blueGreen and canary update strategies. These strategies can create AnalysisRuns and Experiments during the update, which will progress the update, or abort it. AnalysisTemplate An AnalysisTemplate is a template spec which defines how to perform a canary analysis, such as the metrics which it should perform, its frequency, and the values which are considered successful or failed. AnalysisTemplates may be parameterized with inputs values. ClusterAnalysisTemplate A ClusterAnalysisTemplate is like an AnalysisTemplate , but it is not limited to its namespace. It can be used by any Rollout throughout the cluster. AnalysisRun An AnalysisRun is an instantiation of an AnalysisTemplate . AnalysisRuns are like Jobs in that they eventually complete. Completed runs are considered Successful, Failed, or Inconclusive, and the result of the run affect if the Rollout's update will continue, abort, or pause, respectively. Experiment An Experiment is limited run of one or more ReplicaSets for the purposes of analysis. Experiments typically run for a pre-determined duration, but can also run indefinitely until stopped. Experiments may reference an AnalysisTemplate to run during or after the experiment. The canonical use case for an Experiment is to start a baseline and canary deployment in parallel, and compare the metrics produced by the baseline and canary pods for an equal comparison.","title":"Custom Resource Definitions"},{"location":"features/analysis/#background-analysis","text":"Analysis can be run in the background -- while the canary is progressing through its rollout steps. The following example gradually increments the canary weight by 20% every 10 minutes until it reaches 100%. In the background, an AnalysisRun is started based on the AnalysisTemplate named success-rate . The success-rate template queries a prometheus server, measuring the HTTP success rates at 5 minute intervals/samples. It has no end time, and continues until stopped or failed. If the metric is measured to be less than 95%, and there are three such measurements, the analysis is considered Failed. The failed analysis causes the Rollout to abort, setting the canary weight back to zero, and the Rollout would be considered in a Degraded . Otherwise, if the rollout completes all of its canary steps, the rollout is considered successful and the analysis run is stopped by the controller. This example highlights: Background analysis style of progressive delivery Using a Prometheus query to perform a measurement The ability to parameterize the analysis Delay starting the analysis run until step 3 (Set Weight 40%) Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templates : - templateName : success-rate startingStep : 2 # delay starting analysis run until setWeight: 40% args : - name : service-name value : guestbook-svc.default.svc.cluster.local steps : - setWeight : 20 - pause : { duration : 10m } - setWeight : 40 - pause : { duration : 10m } - setWeight : 60 - pause : { duration : 10m } - setWeight : 80 - pause : { duration : 10m } AnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m # NOTE: prometheus queries return results in the form of a vector. # So it is common to access the index 0 of the returned array to obtain the value successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] ))","title":"Background Analysis"},{"location":"features/analysis/#inline-analysis","text":"Analysis can also be performed as a rollout step as an inline \"analysis\" step. When analysis is performed \"inlined,\" an AnalysisRun is started when the step is reached, and blocks the rollout until the run is completed. The success or failure of the analysis run decides if the rollout will proceed to the next step, or abort the rollout completely. This example sets the canary weight to 20%, pauses for 5 minutes, then runs an analysis. If the analysis was successful, continues with rollout, otherwise aborts. This example demonstrates: The ability to invoke an analysis in-line as part of steps apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : steps : - setWeight : 20 - pause : { duration : 5m } - analysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local In this example, the AnalysisTemplate is identical to the background analysis example, but since no interval is specified, the analysis will perform a single measurement and complete. apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name - name : prometheus-port value : 9090 metrics : - name : success-rate successCondition : result[0] >= 0.95 provider : prometheus : address : \"http://prometheus.example.com:{{args.prometheus-port}}\" query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Multiple measurements can be performed over a longer duration period, by specifying the count and interval fields: metrics : - name : success-rate successCondition : result[0] >= 0.95 interval : 60s count : 5 provider : prometheus : address : http://prometheus.example.com:9090 query : ...","title":"Inline Analysis"},{"location":"features/analysis/#clusteranalysistemplates","text":"Important Available since v0.9.0 A Rollout can reference a Cluster scoped AnalysisTemplate called a ClusterAnalysisTemplate . This can be useful when you want to share an AnalysisTemplate across multiple Rollouts; in different namespaces, and avoid duplicating the same template in every namespace. Use the field clusterScope: true to reference a ClusterAnalysisTemplate instead of an AnalysisTemplate. Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : steps : - setWeight : 20 - pause : { duration : 5m } - analysis : templates : - templateName : success-rate clusterScope : true args : - name : service-name value : guestbook-svc.default.svc.cluster.local ClusterAnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : ClusterAnalysisTemplate metadata : name : success-rate spec : args : - name : service-name - name : prometheus-port value : 9090 metrics : - name : success-rate successCondition : result[0] >= 0.95 provider : prometheus : address : \"http://prometheus.example.com:{{args.prometheus-port}}\" query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Note The resulting AnalysisRun will still run in the namespace of the Rollout","title":"ClusterAnalysisTemplates"},{"location":"features/analysis/#analysis-with-multiple-templates","text":"A Rollout can reference multiple AnalysisTemplates when constructing an AnalysisRun. This allows users to compose analysis from multiple AnalysisTemplates. If multiple templates are referenced, then the controller will merge the templates together. The controller combines the metrics and args fields of all the templates. Rollout apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templates : - templateName : success-rate - templateName : error-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local AnalysisTemplate apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : success-rate spec : args : - name : service-name metrics : - name : success-rate interval : 5m successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) --- apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : error-rate spec : args : - name : service-name metrics : - name : error-rate interval : 5m successCondition : result[0] <= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code=~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) AnalysisRun # NOTE: Generated AnalysisRun from the multiple templates apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun metadata : name : guestbook-CurrentPodHash-multiple-templates spec : args : - name : service-name value : guestbook-svc.default.svc.cluster.local metrics : - name : success-rate interval : 5m successCondition : result[0] >= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code!~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) - name : error-rate interval : 5m successCondition : result[0] <= 0.95 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code=~\"5.*\"}[5m] )) / sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\"}[5m] )) Note The controller will error when merging the templates if: Multiple metrics in the templates have the same name Two arguments with the same name have different default values no matter the argument value in Rollout","title":"Analysis with Multiple Templates"},{"location":"features/analysis/#analysis-template-arguments","text":"AnalysisTemplates may declare a set of arguments that can be passed by Rollouts. The args can then be used as in metrics configuration and are resolved at the time the AnalysisRun is created. Argument placeholders are defined as {{ args.<name> }} . apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate metadata : name : args-example spec : args : # required in Rollout due to no default value - name : service-name - name : stable-hash - name : latest-hash # optional in Rollout given the default value - name : api-url value : http://example/measure # from secret - name : api-token valueFrom : secretKeyRef : name : token-secret key : apiToken metrics : - name : webmetric successCondition : result == 'true' provider : web : # placeholders are resolved when an AnalysisRun is created url : \"{{ args.api-url }}?service={{ args.service-name }}\" headers : - key : Authorization value : \"Bearer {{ args.api-token }}\" jsonPath : \"{$.results.ok}\" Analysis arguments defined in a Rollout are merged with the args from the AnalysisTemplate when the AnalysisRun is created. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : canary : analysis : templates : - templateName : args-example args : # required value - name : service-name value : guestbook-svc.default.svc.cluster.local # override default value - name : api-url value : http://other-api # pod template hash from the stable ReplicaSet - name : stable-hash valueFrom : podTemplateHashValue : Stable # pod template hash from the latest ReplicaSet - name : latest-hash valueFrom : podTemplateHashValue : Latest Analysis arguments also support valueFrom for reading any Rollout fields and passing them as arguments to AnalysisTemplate. An example would be to reference metadata labels like env and region and passing them along to AnalysisTemplate, or any field from the Rollout status apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : appType : demo-app buildType : nginx-app ... env : dev region : us-west-2 spec : ... strategy : canary : analysis : templates : - templateName : args-example args : ... - name : env valueFrom : fieldRef : fieldPath : metadata.labels['env'] # region where this app is deployed - name : region valueFrom : fieldRef : fieldPath : metadata.labels['region'] - name : canary-hash valueFrom : fieldRef : fieldPath : status.canary.weights.canary.podTemplateHash","title":"Analysis Template Arguments"},{"location":"features/analysis/#bluegreen-pre-promotion-analysis","text":"A Rollout using the BlueGreen strategy can launch an AnalysisRun before it switches traffic to the new version using pre-promotion. This can be used to block the Service selector switch until the AnalysisRun finishes successfully. The success or failure of the AnalysisRun decides if the Rollout switches traffic, or abort the Rollout completely. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : blueGreen : activeService : active-svc previewService : preview-svc prePromotionAnalysis : templates : - templateName : smoke-tests args : - name : service-name value : preview-svc.default.svc.cluster.local In this example, the Rollout creates a pre-promotion AnalysisRun once the new ReplicaSet is fully available. The Rollout will not switch traffic to the new version until the analysis run finishes successfully. Note: if the autoPromotionSeconds field is specified and the Rollout has waited auto promotion seconds amount of time, the Rollout marks the AnalysisRun successful and switches the traffic to a new version automatically. If the AnalysisRun completes before then, the Rollout will not create another AnalysisRun and wait out the rest of the autoPromotionSeconds .","title":"BlueGreen Pre Promotion Analysis"},{"location":"features/analysis/#bluegreen-post-promotion-analysis","text":"A Rollout using a BlueGreen strategy can launch an analysis run after the traffic switch to the new version using post-promotion analysis. If post-promotion Analysis fails or errors, the Rollout enters an aborted state and switches traffic back to the previous stable Replicaset. When post-analysis is Successful, the Rollout is considered fully promoted and the new ReplicaSet will be marked as stable. The old ReplicaSet will then be scaled down according to scaleDownDelaySeconds (default 30 seconds). apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : ... strategy : blueGreen : activeService : active-svc previewService : preview-svc scaleDownDelaySeconds : 600 # 10 minutes postPromotionAnalysis : templates : - templateName : smoke-tests args : - name : service-name value : preview-svc.default.svc.cluster.local","title":"BlueGreen Post Promotion Analysis"},{"location":"features/analysis/#failure-conditions-and-failure-limit","text":"failureCondition can be used to cause an analysis run to fail. failureLimit is the maximum number of failed run an analysis is allowed. The following example continually polls the defined Prometheus server to get the total number of errors(i.e., HTTP response code >= 500) every 5 minutes, causing the measurement to fail if ten or more errors are encountered. The entire analysis run is considered as Failed after three failed measurements. metrics : - name : total-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code=~\"5.*\"}[5m] ))","title":"Failure Conditions and Failure Limit"},{"location":"features/analysis/#dry-run-mode","text":"Important Available since v1.2 dryRun can be used on a metric to control whether or not to evaluate that metric in a dry-run mode. A metric running in the dry-run mode won't impact the final state of the rollout or experiment even if it fails or the evaluation comes out as inconclusive. The following example queries prometheus every 5 minutes to get the total number of 4XX and 5XX errors, and even if the evaluation of the metric to monitor the 5XX error-rate fail, the analysis run will pass. dryRun : - metricName : total-5xx-errors metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] )) RegEx matches are also supported. .* can be used to make all the metrics run in the dry-run mode. In the following example, even if one or both metrics fail, the analysis run will pass. dryRun : - metricName : .* metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] ))","title":"Dry-Run Mode"},{"location":"features/analysis/#dry-run-summary","text":"If one or more metrics are running in the dry-run mode, the summary of the dry-run results gets appended to the analysis run message. Assuming that the total-4xx-errors metric fails in the above example but, the total-5xx-errors succeeds, the final dry-run summary will look like this. Message : Run Terminated Run Summary : ... Dry Run Summary : Count : 2 Successful : 1 Failed : 1 Metric Results : ...","title":"Dry-Run Summary"},{"location":"features/analysis/#dry-run-rollouts","text":"If a rollout wants to dry run its analysis, it simply needs to specify the dryRun field to its analysis stanza. In the following example, all the metrics from random-fail and always-pass get merged and executed in the dry-run mode. kind : Rollout spec : ... steps : - analysis : templates : - templateName : random-fail - templateName : always-pass dryRun : - metricName : .*","title":"Dry-Run Rollouts"},{"location":"features/analysis/#dry-run-experiments","text":"If an experiment wants to dry run its analysis, it simply needs to specify the dryRun field under its specs. In the following example, all the metrics from analyze-job matching the RegEx rule test.* will be executed in the dry-run mode. kind : Experiment spec : templates : - name : baseline selector : matchLabels : app : rollouts-demo template : metadata : labels : app : rollouts-demo spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue analyses : - name : analyze-job templateName : analyze-job dryRun : - metricName : test.*","title":"Dry-Run Experiments"},{"location":"features/analysis/#measurements-retention","text":"Important Available since v1.2 measurementRetention can be used to retain other than the latest ten results for the metrics running in any mode (dry/non-dry). Setting this option to 0 would disable it and, the controller will revert to the existing behavior of retaining the latest ten measurements. The following example queries Prometheus every 5 minutes to get the total number of 4XX and 5XX errors and retains the latest twenty measurements for the 5XX metric run results instead of the default ten. measurementRetention : - metricName : total-5xx-errors limit : 20 metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] )) RegEx matches are also supported. .* can be used to apply the same retention rule to all the metrics. In the following example, the controller will retain the latest twenty run results for all the metrics instead of the default ten results. measurementRetention : - metricName : .* limit : 20 metrics : - name : total-5xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"5.*\"}[5m] )) - name : total-4xx-errors interval : 5m failureCondition : result[0] >= 10 failureLimit : 3 provider : prometheus : address : http://prometheus.example.com:9090 query : | sum(irate( istio_requests_total{reporter=\"source\",destination_service=~\"{{args.service-name}}\",response_code~\"4.*\"}[5m] ))","title":"Measurements Retention"},{"location":"features/analysis/#inconclusive-runs","text":"Analysis runs can also be considered Inconclusive , which indicates the run was neither successful, nor failed. Inconclusive runs causes a rollout to become paused at its current step. Manual intervention is then needed to either resume the rollout, or abort. One example of how analysis runs could become Inconclusive , is when a metric defines no success or failure conditions. metrics : - name : my-query provider : prometheus : address : http://prometheus.example.com:9090 query : ... Inconclusive analysis runs might also happen when both success and failure conditions are specified, but the measurement value did not meet either condition. metrics : - name : success-rate successCondition : result[0] >= 0.90 failureCondition : result[0] < 0.50 provider : prometheus : address : http://prometheus.example.com:9090 query : ... A use case for having Inconclusive analysis runs are to enable Argo Rollouts to automate the execution of analysis runs, and collect the measurement, but still allow human judgement to decide whether or not measurement value is acceptable and decide to proceed or abort.","title":"Inconclusive Runs"},{"location":"features/analysis/#delay-analysis-runs","text":"If the analysis run does not need to start immediately (i.e give the metric provider time to collect metrics on the canary version), Analysis Runs can delay the specific metric analysis. Each metric can be configured to have a different delay. In additional to the metric specific delays, the rollouts with background analysis can delay creating an analysis run until a certain step is reached Delaying a specific analysis metric: metrics : - name : success-rate # Do not start this analysis until 5 minutes after the analysis run starts initialDelay : 5m successCondition : result[0] >= 0.90 provider : prometheus : address : http://prometheus.example.com:9090 query : ... Delaying starting background analysis run until step 3 (Set Weight 40%): apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook spec : strategy : canary : analysis : templates : - templateName : success-rate startingStep : 2 steps : - setWeight : 20 - pause : { duration : 10m } - setWeight : 40 - pause : { duration : 10m }","title":"Delay Analysis Runs"},{"location":"features/analysis/#referencing-secrets","text":"AnalysisTemplates and AnalysisRuns can reference secret objects in .spec.args . This allows users to securely pass authentication information to Metric Providers, like login credentials or API tokens. An AnalysisRun can only reference secrets from the same namespace as it's running in. This is only relevant for AnalysisRuns, since AnalysisTemplates do not resolve the secret. In the following example, an AnalysisTemplate references an API token and passes it to a Web metric provider. This example demonstrates: The ability to reference a secret in the AnalysisTemplate .spec.args The ability to pass secret arguments to Metric Providers apiVersion : argoproj.io/v1alpha1 kind : AnalysisTemplate spec : args : - name : api-token valueFrom : secretKeyRef : name : token-secret key : apiToken metrics : - name : webmetric provider : web : headers : - key : Authorization value : \"Bearer {{ args.api-token }}\"","title":"Referencing Secrets"},{"location":"features/analysis/#handling-metric-results","text":"","title":"Handling Metric Results"},{"location":"features/analysis/#nan-and-infinity","text":"Metric providers can sometimes return values of NaN (not a number) and infinity. Users can edit the successCondition and failureCondition fields to handle these cases accordingly. Here are three examples where a metric result of NaN is considered successful, inconclusive and failed respectively. apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : isNaN(result) || result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Successful startedAt : \"2021-02-10T00:15:26Z\" value : NaN name : success-rate phase : Successful successful : 1 phase : Successful startedAt : \"2021-02-10T00:15:26Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : result >= 0.95 failureCondition : result < 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Inconclusive startedAt : \"2021-02-10T00:15:26Z\" value : NaN name : success-rate phase : Inconclusive successful : 1 phase : Inconclusive startedAt : \"2021-02-10T00:15:26Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Failed startedAt : \"2021-02-10T00:15:26Z\" value : NaN name : success-rate phase : Failed successful : 1 phase : Failed startedAt : \"2021-02-10T00:15:26Z\" Here are two examples where a metric result of infinity is considered successful and failed respectively. apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Successful startedAt : \"2021-02-10T00:15:26Z\" value : +Inf name : success-rate phase : Successful successful : 1 phase : Successful startedAt : \"2021-02-10T00:15:26Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... failureCondition : isInf(result) status : metricResults : - count : 1 measurements : - finishedAt : \"2021-02-10T00:15:26Z\" phase : Failed startedAt : \"2021-02-10T00:15:26Z\" value : +Inf name : success-rate phase : Failed successful : 1 phase : Failed startedAt : \"2021-02-10T00:15:26Z\"","title":"NaN and Infinity"},{"location":"features/analysis/#empty-array","text":"","title":"Empty array"},{"location":"features/analysis/#prometheus","text":"Metric providers can sometimes return empty array, e.g., no data returned from prometheus query. Here are two examples where a metric result of empty array is considered successful and failed respectively. apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : len(result) == 0 || result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-09-08T19:15:49Z\" phase : Successful startedAt : \"2021-09-08T19:15:49Z\" value : '[]' name : success-rate phase : Successful successful : 1 phase : Successful startedAt : \"2021-09-08T19:15:49Z\" apiVersion : argoproj.io/v1alpha1 kind : AnalysisRun ... successCondition : len(result) > 0 && result >= 0.95 status : metricResults : - count : 1 measurements : - finishedAt : \"2021-09-08T19:19:44Z\" phase : Failed startedAt : \"2021-09-08T19:19:44Z\" value : '[]' name : success-rate phase : Failed successful : 1 phase : Failed startedAt : \"2021-09-08T19:19:44Z\"","title":"Prometheus"},{"location":"features/analysis/#datadog","text":"Datadog queries can return empty results if the query takes place during a time interval with no metrics. The Datadog provider will return a nil value yielding an error during the evaluation phase like: invalid operation: < (mismatched types <nil> and float64) However, empty query results yielding a nil value can be handled using the default() function. Here is a succeeding example using the default() function: successCondition : default(result, 0) < 0.05","title":"Datadog"},{"location":"features/bluegreen/","text":"BlueGreen Deployment Strategy \u00b6 A Blue Green Deployment allows users to reduce the amount of time multiple versions running at the same time. Overview \u00b6 In addition to managing ReplicaSets, the rollout controller will modify a Service resource during the BlueGreenUpdate strategy. The Rollout spec has users specify a reference to active service and optionally a preview service in the same namespace. The active Service is used to send regular application traffic to the old version, while the preview Service is used as funnel traffic to the new version. The rollout controller ensures proper traffic routing by injecting a unique hash of the ReplicaSet to these services' selectors. This allows the rollout to define an active and preview stack and a process to migrate replica sets from the preview to the active. When there is a change to the .spec.template field of a rollout, the controller will create the new ReplicaSet. If the active service is not sending traffic to a ReplicaSet, the controller will immediately start sending traffic to the ReplicaSet. Otherwise, the active service will point at the old ReplicaSet while the ReplicaSet becomes available. Once the new ReplicaSet becomes available, the controller will modify the active service to point at the new ReplicaSet. After waiting some time configured by the .spec.strategy.blueGreen.scaleDownDelaySeconds , the controller will scale down the old ReplicaSet. Important When the rollout changes the selector on a service, there is a propagation delay before all the nodes update their IP tables to send traffic to the new pods instead of the old. During this delay, traffic will be directed to the old pods if the nodes have not been updated yet. In order to prevent the packets from being sent to a node that killed the old pod, the rollout uses the scaleDownDelaySeconds field to give nodes enough time to broadcast the IP table changes. Example \u00b6 apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-bluegreen spec : replicas : 2 revisionHistoryLimit : 2 selector : matchLabels : app : rollout-bluegreen template : metadata : labels : app : rollout-bluegreen spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue imagePullPolicy : Always ports : - containerPort : 8080 strategy : blueGreen : # activeService specifies the service to update with the new template hash at time of promotion. # This field is mandatory for the blueGreen update strategy. activeService : rollout-bluegreen-active # previewService specifies the service to update with the new template hash before promotion. # This allows the preview stack to be reachable without serving production traffic. # This field is optional. previewService : rollout-bluegreen-preview # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout # immediately before the promotion. If omitted, the default behavior is to promote the new # stack as soon as the ReplicaSet are completely ready/available. # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT` autoPromotionEnabled : false Configurable Features \u00b6 Here are the optional fields that will change the behavior of BlueGreen deployment: spec : strategy : blueGreen : autoPromotionEnabled : boolean autoPromotionSeconds : *int32 antiAffinity : object previewService : string prePromotionAnalysis : object postPromotionAnalysis : object previewReplicaCount : *int32 scaleDownDelaySeconds : *int32 scaleDownDelayRevisionLimit : *int32 Sequence of Events \u00b6 The following describes the sequence of events that happen during a blue-green update. Beginning at a fully promoted, steady-state, a revision 1 ReplicaSet is pointed to by both the activeService and previewService . A user initiates an update by modifying the pod template ( spec.template.spec ). The revision 2 ReplicaSet is created with size 0. The preview service is modified to point to the revision 2 ReplicaSet. The activeService remains pointing to revision 1. The revision 2 ReplicaSet is scaled to either spec.replicas or previewReplicaCount if set. Once revision 2 ReplicaSet Pods are fully available, prePromotionAnalysis begins. Upon success of prePromotionAnalysis , the blue/green pauses if autoPromotionEnabled is false, or autoPromotionSeconds is non-zero. The rollout is resumed either manually by a user, or automatically by surpassing autoPromotionSeconds . The revision 2 ReplicaSet is scaled to the spec.replicas , if the previewReplicaCount feature was used. The rollout \"promotes\" the revision 2 ReplicaSet by updating the activeService to point to it. At this point, there are no services pointing to revision 1 postPromotionAnalysis analysis begins Once postPromotionAnalysis completes successfully, the update is successful and the revision 2 ReplicaSet is marked as stable. The rollout is considered fully-promoted. After waiting scaleDownDelaySeconds (default 30 seconds), the revision 1 ReplicaSet is scaled down autoPromotionEnabled \u00b6 The AutoPromotionEnabled will make the rollout automatically promote the new ReplicaSet to the active service once the new ReplicaSet is healthy. This field is defaulted to true if it is not specified. Defaults to true autoPromotionSeconds \u00b6 The AutoPromotionSeconds will make the rollout automatically promote the new ReplicaSet to active Service after the AutoPromotionSeconds time has passed since the rollout has entered a paused state. If the AutoPromotionEnabled field is set to true, this field will be ignored Defaults to nil antiAffinity \u00b6 Check out the Anti Affinity document document for more information. Defaults to nil maxUnavailable \u00b6 The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxSurge is 0. Defaults to 0 prePromotionAnalysis \u00b6 Configures the Analysis before it switches traffic to the new version. The AnalysisRun can be used to block the Service selector switch until the AnalysisRun finishes successful. The success or failure of the analysis run decides if the Rollout will switch traffic, or abort the Rollout completely. Defaults to nil postPromotionAnalysis \u00b6 Configures the Analysis after the traffic switch to new version. If the analysis run fails or errors out, the Rollout enters an aborted state and switch traffic back to the previous stable Replicaset. If scaleDownDelaySeconds is specified, the controller will cancel any AnalysisRuns at time of scaleDownDelay to scale down the ReplicaSet. If it is omitted, and post analysis is specified, it will scale down the ReplicaSet only after the AnalysisRun completes (with a minimum of 30 seconds). Defaults to nil previewService \u00b6 The PreviewService field references a Service that will be modified to send traffic to the new ReplicaSet before the new one is promoted to receiving traffic from the active service. Once the new ReplicaSet starts receiving traffic from the active service, the preview service will also be modified to send traffic to the new ReplicaSet as well. The Rollout always makes sure that the preview service is sending traffic to the newest ReplicaSet. As a result, if a new version is introduced before the old version is promoted to the active service, the controller will immediately switch over to that brand new version. This feature is used to provide an endpoint that can be used to test a new version of an application. Defaults to an empty string previewReplicaCount \u00b6 The PreviewReplicaCount field will indicate the number of replicas that the new version of an application should run. Once the application is ready to promote to the active service, the controller will scale the new ReplicaSet to the value of the spec.replicas . The rollout will not switch over the active service to the new ReplicaSet until it matches the spec.replicas count. This feature is mainly used to save resources during the testing phase. If the application does not need a fully scaled up application for the tests, this feature can help save some resources. If omitted, the preview ReplicaSet stack will be scaled to 100% of the replicas. scaleDownDelaySeconds \u00b6 The ScaleDownDelaySeconds is used to delay scaling down the old ReplicaSet after the active Service is switched to the new ReplicaSet. Defaults to 30 scaleDownDelayRevisionLimit \u00b6 The ScaleDownDelayRevisionLimit limits the number of old active ReplicaSets to keep scaled up while they wait for the scaleDownDelay to pass after being removed from the active service. If omitted, all ReplicaSets will be retained for the specified scaleDownDelay","title":"BlueGreen"},{"location":"features/bluegreen/#bluegreen-deployment-strategy","text":"A Blue Green Deployment allows users to reduce the amount of time multiple versions running at the same time.","title":"BlueGreen Deployment Strategy"},{"location":"features/bluegreen/#overview","text":"In addition to managing ReplicaSets, the rollout controller will modify a Service resource during the BlueGreenUpdate strategy. The Rollout spec has users specify a reference to active service and optionally a preview service in the same namespace. The active Service is used to send regular application traffic to the old version, while the preview Service is used as funnel traffic to the new version. The rollout controller ensures proper traffic routing by injecting a unique hash of the ReplicaSet to these services' selectors. This allows the rollout to define an active and preview stack and a process to migrate replica sets from the preview to the active. When there is a change to the .spec.template field of a rollout, the controller will create the new ReplicaSet. If the active service is not sending traffic to a ReplicaSet, the controller will immediately start sending traffic to the ReplicaSet. Otherwise, the active service will point at the old ReplicaSet while the ReplicaSet becomes available. Once the new ReplicaSet becomes available, the controller will modify the active service to point at the new ReplicaSet. After waiting some time configured by the .spec.strategy.blueGreen.scaleDownDelaySeconds , the controller will scale down the old ReplicaSet. Important When the rollout changes the selector on a service, there is a propagation delay before all the nodes update their IP tables to send traffic to the new pods instead of the old. During this delay, traffic will be directed to the old pods if the nodes have not been updated yet. In order to prevent the packets from being sent to a node that killed the old pod, the rollout uses the scaleDownDelaySeconds field to give nodes enough time to broadcast the IP table changes.","title":"Overview"},{"location":"features/bluegreen/#example","text":"apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-bluegreen spec : replicas : 2 revisionHistoryLimit : 2 selector : matchLabels : app : rollout-bluegreen template : metadata : labels : app : rollout-bluegreen spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue imagePullPolicy : Always ports : - containerPort : 8080 strategy : blueGreen : # activeService specifies the service to update with the new template hash at time of promotion. # This field is mandatory for the blueGreen update strategy. activeService : rollout-bluegreen-active # previewService specifies the service to update with the new template hash before promotion. # This allows the preview stack to be reachable without serving production traffic. # This field is optional. previewService : rollout-bluegreen-preview # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout # immediately before the promotion. If omitted, the default behavior is to promote the new # stack as soon as the ReplicaSet are completely ready/available. # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT` autoPromotionEnabled : false","title":"Example"},{"location":"features/bluegreen/#configurable-features","text":"Here are the optional fields that will change the behavior of BlueGreen deployment: spec : strategy : blueGreen : autoPromotionEnabled : boolean autoPromotionSeconds : *int32 antiAffinity : object previewService : string prePromotionAnalysis : object postPromotionAnalysis : object previewReplicaCount : *int32 scaleDownDelaySeconds : *int32 scaleDownDelayRevisionLimit : *int32","title":"Configurable Features"},{"location":"features/bluegreen/#sequence-of-events","text":"The following describes the sequence of events that happen during a blue-green update. Beginning at a fully promoted, steady-state, a revision 1 ReplicaSet is pointed to by both the activeService and previewService . A user initiates an update by modifying the pod template ( spec.template.spec ). The revision 2 ReplicaSet is created with size 0. The preview service is modified to point to the revision 2 ReplicaSet. The activeService remains pointing to revision 1. The revision 2 ReplicaSet is scaled to either spec.replicas or previewReplicaCount if set. Once revision 2 ReplicaSet Pods are fully available, prePromotionAnalysis begins. Upon success of prePromotionAnalysis , the blue/green pauses if autoPromotionEnabled is false, or autoPromotionSeconds is non-zero. The rollout is resumed either manually by a user, or automatically by surpassing autoPromotionSeconds . The revision 2 ReplicaSet is scaled to the spec.replicas , if the previewReplicaCount feature was used. The rollout \"promotes\" the revision 2 ReplicaSet by updating the activeService to point to it. At this point, there are no services pointing to revision 1 postPromotionAnalysis analysis begins Once postPromotionAnalysis completes successfully, the update is successful and the revision 2 ReplicaSet is marked as stable. The rollout is considered fully-promoted. After waiting scaleDownDelaySeconds (default 30 seconds), the revision 1 ReplicaSet is scaled down","title":"Sequence of Events"},{"location":"features/bluegreen/#autopromotionenabled","text":"The AutoPromotionEnabled will make the rollout automatically promote the new ReplicaSet to the active service once the new ReplicaSet is healthy. This field is defaulted to true if it is not specified. Defaults to true","title":"autoPromotionEnabled"},{"location":"features/bluegreen/#autopromotionseconds","text":"The AutoPromotionSeconds will make the rollout automatically promote the new ReplicaSet to active Service after the AutoPromotionSeconds time has passed since the rollout has entered a paused state. If the AutoPromotionEnabled field is set to true, this field will be ignored Defaults to nil","title":"autoPromotionSeconds"},{"location":"features/bluegreen/#antiaffinity","text":"Check out the Anti Affinity document document for more information. Defaults to nil","title":"antiAffinity"},{"location":"features/bluegreen/#maxunavailable","text":"The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxSurge is 0. Defaults to 0","title":"maxUnavailable"},{"location":"features/bluegreen/#prepromotionanalysis","text":"Configures the Analysis before it switches traffic to the new version. The AnalysisRun can be used to block the Service selector switch until the AnalysisRun finishes successful. The success or failure of the analysis run decides if the Rollout will switch traffic, or abort the Rollout completely. Defaults to nil","title":"prePromotionAnalysis"},{"location":"features/bluegreen/#postpromotionanalysis","text":"Configures the Analysis after the traffic switch to new version. If the analysis run fails or errors out, the Rollout enters an aborted state and switch traffic back to the previous stable Replicaset. If scaleDownDelaySeconds is specified, the controller will cancel any AnalysisRuns at time of scaleDownDelay to scale down the ReplicaSet. If it is omitted, and post analysis is specified, it will scale down the ReplicaSet only after the AnalysisRun completes (with a minimum of 30 seconds). Defaults to nil","title":"postPromotionAnalysis"},{"location":"features/bluegreen/#previewservice","text":"The PreviewService field references a Service that will be modified to send traffic to the new ReplicaSet before the new one is promoted to receiving traffic from the active service. Once the new ReplicaSet starts receiving traffic from the active service, the preview service will also be modified to send traffic to the new ReplicaSet as well. The Rollout always makes sure that the preview service is sending traffic to the newest ReplicaSet. As a result, if a new version is introduced before the old version is promoted to the active service, the controller will immediately switch over to that brand new version. This feature is used to provide an endpoint that can be used to test a new version of an application. Defaults to an empty string","title":"previewService"},{"location":"features/bluegreen/#previewreplicacount","text":"The PreviewReplicaCount field will indicate the number of replicas that the new version of an application should run. Once the application is ready to promote to the active service, the controller will scale the new ReplicaSet to the value of the spec.replicas . The rollout will not switch over the active service to the new ReplicaSet until it matches the spec.replicas count. This feature is mainly used to save resources during the testing phase. If the application does not need a fully scaled up application for the tests, this feature can help save some resources. If omitted, the preview ReplicaSet stack will be scaled to 100% of the replicas.","title":"previewReplicaCount"},{"location":"features/bluegreen/#scaledowndelayseconds","text":"The ScaleDownDelaySeconds is used to delay scaling down the old ReplicaSet after the active Service is switched to the new ReplicaSet. Defaults to 30","title":"scaleDownDelaySeconds"},{"location":"features/bluegreen/#scaledowndelayrevisionlimit","text":"The ScaleDownDelayRevisionLimit limits the number of old active ReplicaSets to keep scaled up while they wait for the scaleDownDelay to pass after being removed from the active service. If omitted, all ReplicaSets will be retained for the specified scaleDownDelay","title":"scaleDownDelayRevisionLimit"},{"location":"features/canary/","text":"Canary Deployment Strategy \u00b6 A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage of the production traffic. Overview \u00b6 Since there is no agreed upon standard for a canary deployment, the rollouts controller allows users to outline how they want to run their canary deployment. Users can define a list of steps the controller uses to manipulate the ReplicaSets when there is a change to the .spec.template . Each step will be evaluated before the new ReplicaSet is promoted to the stable version, and the old version is completely scaled down. Each step can have one of two fields. The setWeight field dictates the percentage of traffic that should be sent to the canary, and the pause struct instructs the rollout to pause. When the controller reaches a pause step for a rollout, it will set adds a PauseCondition struct to the .status.PauseConditions field. If the duration field within the pause struct is set, the rollout will not progress to the next step until it has waited for the value of the duration field. Otherwise, the rollout will wait indefinitely until that Pause condition is removed. By using the setWeight and the pause fields, a user can declarative describe how they want to progress to the new version. Below is an example of a canary strategy. Important If the canary Rollout does not use traffic management , the Rollout makes a best effort attempt to achieve the percentage listed in the last setWeight step between the new and old version. For example, if a Rollout has 10 Replicas and 10% for the first setWeight step, the controller will scale the new desired ReplicaSet to 1 replicas and the old stable ReplicaSet to 9. In the case where the setWeight is 15%, the Rollout attempts to get there by rounding up the calculation (i.e. the new ReplicaSet has 2 pods since 15% of 10, rounds up to 2 and the old ReplicaSet has 9 pods since 85% of 10, rounds up to 9). If a user wants to have more fine-grained control of the percentages without a large number of Replicas, that user should use the traffic management functionality. Example \u00b6 apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout spec : replicas : 10 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 minReadySeconds : 30 revisionHistoryLimit : 3 strategy : canary : #Indicates that the rollout should use the Canary strategy maxSurge : \"25%\" maxUnavailable : 0 steps : - setWeight : 10 - pause : duration : 1h # 1 hour - setWeight : 20 - pause : {} # pause indefinitely Pause Duration \u00b6 Pause duration can be specified with an optional time unit suffix. Valid time units are \"s\", \"m\", \"h\". Defaults to \"s\" if not specified. spec : strategy : canary : steps : - pause : { duration : 10 } # 10 seconds - pause : { duration : 10s } # 10 seconds - pause : { duration : 10m } # 10 minutes - pause : { duration : 10h } # 10 hours - pause : {} # pause indefinitely If no duration is specified for a pause step, the rollout will be paused indefinitely. To unpause, use the argo kubectl plugin promote command. # promote to the next step kubectl argo rollouts promote <rollout> Dynamic Canary Scale (with Traffic Routing) \u00b6 By default, the rollout controller will scale the canary to match the current trafficWeight of the current step. For example, if the current weight is 25%, and there are four replicas, then the canary will be scaled to 1, to match the traffic weight. It is possible to control the canary replica's scale during the steps such that it does not necessary match the traffic weight. Some use cases for this: The new version should not yet be exposed to the public (setWeight: 0), but you would like to scale the canary up for testing purposes. You wish to scale the canary stack up minimally, and use some header based traffic shaping to the canary, while setWeight is still set to 0. You wish to scale the canary up to 100%, in order to facilitate traffic shadowing. Important Setting canary scale is only available when using the canary strategy with a traffic router, since the basic canary needs to control canary scale in order to approximate canary weight. To control canary scales and weights during steps, use the setCanaryScale step and indicate which scale the the canary should use: explicit replica count without changing traffic weight ( replicas ) explicit weight percentage of total spec.replicas without changing traffic weight( weight ) to or not to match current canary's setWeight step ( matchTrafficWeight: true or false ) spec : strategy : canary : steps : # explicit count - setCanaryScale : replicas : 3 # a percentage of spec.replicas - setCanaryScale : weight : 25 # matchTrafficWeight returns to the default behavior of matching the canary traffic weight - setCanaryScale : matchTrafficWeight : true When using setCanaryScale with explicit values for either replicas or weight, one must be careful if used in conjunction with the setWeight step. If done incorrectly, an imbalanced amount of traffic may be directed to the canary (in proportion to the Rollout's scale). For example, the following set of steps would cause 90% of traffic to only be served by 10% of pods: spec : replicas : 10 strategy : canary : steps : # 1 canary pod (10% of spec.replicas) - setCanaryScale : weight : 10 # 90% of traffic to the 1 canary pod - setWeight : 90 - pause : {} The above situation is caused by the changed behvaior of setWeight after setCanaryScale . To reset, set matchTrafficWeight: true and the setWeight behavior will be restored, i.e., subsequent setWeight will create canary replicas matching the traffic weight. Dynamic Stable Scale (with Traffic Routing) \u00b6 Important Available since v1.1 When using traffic routing, by default the stable ReplicaSet is left scaled to 100% during the update. This has the advantage that if an abort occurs, traffic can be immediately shifted back to the stable ReplicaSet without delay. However, it has the disadvantage that during the update, there will eventually exist double the number of replica pods running (similar to in a blue-green deployment), since the stable ReplicaSet is left scaled up for the full duration of the update. It is possible to dynamically reduce the scale of the stable ReplicaSet during an update such that it scales down as the traffic weight increases to canary. This would be desirable in scenarios where the Rollout has a high replica count and resource cost is a concern, or in bare-metal situations where it is not possible to create additional node capacity to accommodate double the replicas. The ability to dynamically scale the stable ReplicaSet can be enabled by setting the canary.dynamicStableScale flag to true: spec : strategy : canary : dynamicStableScale : true NOTE: that if dynamicStableScale is set, and the rollout is aborted, the canary ReplicaSet will dynamically scale down as traffic shifts back to stable. If you wish to leave the canary ReplicaSet scaled up while aborting, an explicit value for abortScaleDownDelaySeconds can be set: spec : strategy : canary : dynamicStableScale : true abortScaleDownDelaySeconds : 600 Mimicking Rolling Update \u00b6 If the steps field is omitted, the canary strategy will mimic the rolling update behavior. Similar to the deployment, the canary strategy has the maxSurge and maxUnavailable fields to configure how the Rollout should progress to the new version. Other Configurable Features \u00b6 Here are the optional fields that will modify the behavior of canary strategy: spec : strategy : canary : analysis : object antiAffinity : object canaryService : string stableService : string maxSurge : stringOrInt maxUnavailable : stringOrInt trafficRouting : object analysis \u00b6 Configure the background Analysis to execute during the rollout. If the analysis is unsuccessful the rollout will be aborted. Defaults to nil antiAffinity \u00b6 Check out the Anti Affinity document document for more information. Defaults to nil canaryService \u00b6 canaryService references a Service that will be modified to send traffic to only the canary ReplicaSet. This allows users to only hit the canary ReplicaSet. Defaults to an empty string stableService \u00b6 stableService the name of a Service which selects pods with stable version and doesn't select any pods with canary version. This allows users to only hit the stable ReplicaSet. Defaults to an empty string maxSurge \u00b6 maxSurge defines the maximum number of replicas the rollout can create to move to the correct ratio set by the last setWeight. Max Surge can either be an integer or percentage as a string (i.e. \"20%\") Defaults to \"25%\". maxUnavailable \u00b6 The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxSurge is 0. Defaults to 25% trafficRouting \u00b6 The traffic management rules to apply to control the flow of traffic between the active and canary versions. If not set, the default weighted pod replica based routing will be used. Defaults to nil","title":"Canary"},{"location":"features/canary/#canary-deployment-strategy","text":"A canary rollout is a deployment strategy where the operator releases a new version of their application to a small percentage of the production traffic.","title":"Canary Deployment Strategy"},{"location":"features/canary/#overview","text":"Since there is no agreed upon standard for a canary deployment, the rollouts controller allows users to outline how they want to run their canary deployment. Users can define a list of steps the controller uses to manipulate the ReplicaSets when there is a change to the .spec.template . Each step will be evaluated before the new ReplicaSet is promoted to the stable version, and the old version is completely scaled down. Each step can have one of two fields. The setWeight field dictates the percentage of traffic that should be sent to the canary, and the pause struct instructs the rollout to pause. When the controller reaches a pause step for a rollout, it will set adds a PauseCondition struct to the .status.PauseConditions field. If the duration field within the pause struct is set, the rollout will not progress to the next step until it has waited for the value of the duration field. Otherwise, the rollout will wait indefinitely until that Pause condition is removed. By using the setWeight and the pause fields, a user can declarative describe how they want to progress to the new version. Below is an example of a canary strategy. Important If the canary Rollout does not use traffic management , the Rollout makes a best effort attempt to achieve the percentage listed in the last setWeight step between the new and old version. For example, if a Rollout has 10 Replicas and 10% for the first setWeight step, the controller will scale the new desired ReplicaSet to 1 replicas and the old stable ReplicaSet to 9. In the case where the setWeight is 15%, the Rollout attempts to get there by rounding up the calculation (i.e. the new ReplicaSet has 2 pods since 15% of 10, rounds up to 2 and the old ReplicaSet has 9 pods since 85% of 10, rounds up to 9). If a user wants to have more fine-grained control of the percentages without a large number of Replicas, that user should use the traffic management functionality.","title":"Overview"},{"location":"features/canary/#example","text":"apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout spec : replicas : 10 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 minReadySeconds : 30 revisionHistoryLimit : 3 strategy : canary : #Indicates that the rollout should use the Canary strategy maxSurge : \"25%\" maxUnavailable : 0 steps : - setWeight : 10 - pause : duration : 1h # 1 hour - setWeight : 20 - pause : {} # pause indefinitely","title":"Example"},{"location":"features/canary/#pause-duration","text":"Pause duration can be specified with an optional time unit suffix. Valid time units are \"s\", \"m\", \"h\". Defaults to \"s\" if not specified. spec : strategy : canary : steps : - pause : { duration : 10 } # 10 seconds - pause : { duration : 10s } # 10 seconds - pause : { duration : 10m } # 10 minutes - pause : { duration : 10h } # 10 hours - pause : {} # pause indefinitely If no duration is specified for a pause step, the rollout will be paused indefinitely. To unpause, use the argo kubectl plugin promote command. # promote to the next step kubectl argo rollouts promote <rollout>","title":"Pause Duration"},{"location":"features/canary/#dynamic-canary-scale-with-traffic-routing","text":"By default, the rollout controller will scale the canary to match the current trafficWeight of the current step. For example, if the current weight is 25%, and there are four replicas, then the canary will be scaled to 1, to match the traffic weight. It is possible to control the canary replica's scale during the steps such that it does not necessary match the traffic weight. Some use cases for this: The new version should not yet be exposed to the public (setWeight: 0), but you would like to scale the canary up for testing purposes. You wish to scale the canary stack up minimally, and use some header based traffic shaping to the canary, while setWeight is still set to 0. You wish to scale the canary up to 100%, in order to facilitate traffic shadowing. Important Setting canary scale is only available when using the canary strategy with a traffic router, since the basic canary needs to control canary scale in order to approximate canary weight. To control canary scales and weights during steps, use the setCanaryScale step and indicate which scale the the canary should use: explicit replica count without changing traffic weight ( replicas ) explicit weight percentage of total spec.replicas without changing traffic weight( weight ) to or not to match current canary's setWeight step ( matchTrafficWeight: true or false ) spec : strategy : canary : steps : # explicit count - setCanaryScale : replicas : 3 # a percentage of spec.replicas - setCanaryScale : weight : 25 # matchTrafficWeight returns to the default behavior of matching the canary traffic weight - setCanaryScale : matchTrafficWeight : true When using setCanaryScale with explicit values for either replicas or weight, one must be careful if used in conjunction with the setWeight step. If done incorrectly, an imbalanced amount of traffic may be directed to the canary (in proportion to the Rollout's scale). For example, the following set of steps would cause 90% of traffic to only be served by 10% of pods: spec : replicas : 10 strategy : canary : steps : # 1 canary pod (10% of spec.replicas) - setCanaryScale : weight : 10 # 90% of traffic to the 1 canary pod - setWeight : 90 - pause : {} The above situation is caused by the changed behvaior of setWeight after setCanaryScale . To reset, set matchTrafficWeight: true and the setWeight behavior will be restored, i.e., subsequent setWeight will create canary replicas matching the traffic weight.","title":"Dynamic Canary Scale (with Traffic Routing)"},{"location":"features/canary/#dynamic-stable-scale-with-traffic-routing","text":"Important Available since v1.1 When using traffic routing, by default the stable ReplicaSet is left scaled to 100% during the update. This has the advantage that if an abort occurs, traffic can be immediately shifted back to the stable ReplicaSet without delay. However, it has the disadvantage that during the update, there will eventually exist double the number of replica pods running (similar to in a blue-green deployment), since the stable ReplicaSet is left scaled up for the full duration of the update. It is possible to dynamically reduce the scale of the stable ReplicaSet during an update such that it scales down as the traffic weight increases to canary. This would be desirable in scenarios where the Rollout has a high replica count and resource cost is a concern, or in bare-metal situations where it is not possible to create additional node capacity to accommodate double the replicas. The ability to dynamically scale the stable ReplicaSet can be enabled by setting the canary.dynamicStableScale flag to true: spec : strategy : canary : dynamicStableScale : true NOTE: that if dynamicStableScale is set, and the rollout is aborted, the canary ReplicaSet will dynamically scale down as traffic shifts back to stable. If you wish to leave the canary ReplicaSet scaled up while aborting, an explicit value for abortScaleDownDelaySeconds can be set: spec : strategy : canary : dynamicStableScale : true abortScaleDownDelaySeconds : 600","title":"Dynamic Stable Scale (with Traffic Routing)"},{"location":"features/canary/#mimicking-rolling-update","text":"If the steps field is omitted, the canary strategy will mimic the rolling update behavior. Similar to the deployment, the canary strategy has the maxSurge and maxUnavailable fields to configure how the Rollout should progress to the new version.","title":"Mimicking Rolling Update"},{"location":"features/canary/#other-configurable-features","text":"Here are the optional fields that will modify the behavior of canary strategy: spec : strategy : canary : analysis : object antiAffinity : object canaryService : string stableService : string maxSurge : stringOrInt maxUnavailable : stringOrInt trafficRouting : object","title":"Other Configurable Features"},{"location":"features/canary/#analysis","text":"Configure the background Analysis to execute during the rollout. If the analysis is unsuccessful the rollout will be aborted. Defaults to nil","title":"analysis"},{"location":"features/canary/#antiaffinity","text":"Check out the Anti Affinity document document for more information. Defaults to nil","title":"antiAffinity"},{"location":"features/canary/#canaryservice","text":"canaryService references a Service that will be modified to send traffic to only the canary ReplicaSet. This allows users to only hit the canary ReplicaSet. Defaults to an empty string","title":"canaryService"},{"location":"features/canary/#stableservice","text":"stableService the name of a Service which selects pods with stable version and doesn't select any pods with canary version. This allows users to only hit the stable ReplicaSet. Defaults to an empty string","title":"stableService"},{"location":"features/canary/#maxsurge","text":"maxSurge defines the maximum number of replicas the rollout can create to move to the correct ratio set by the last setWeight. Max Surge can either be an integer or percentage as a string (i.e. \"20%\") Defaults to \"25%\".","title":"maxSurge"},{"location":"features/canary/#maxunavailable","text":"The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). This can not be 0 if MaxSurge is 0. Defaults to 25%","title":"maxUnavailable"},{"location":"features/canary/#trafficrouting","text":"The traffic management rules to apply to control the flow of traffic between the active and canary versions. If not set, the default weighted pod replica based routing will be used. Defaults to nil","title":"trafficRouting"},{"location":"features/controller-metrics/","text":"Controller Metrics \u00b6 The Argo Rollouts controller is already instrumented with Prometheus metrics available at /metrics in port 8090. You can use these metrics to look at the health of the controller either via dashboards or via other Prometheus integrations. Installing and configuring Prometheus \u00b6 To take advantage of the metrics you need to have Prometheus installed in your Kubernetes cluster. If you don't have an existing installation of Prometheus you can use any of the common methods to install it in your cluster. Popular options include the Prometheus Helm chart or the Prometheus Operator . Once Prometheus is running in your cluster you need to make sure that it scrapes the Argo Rollouts endpoint. Prometheus already contains a service discovery mechanism for Kubernetes, but you need to configure it first . Depending on your installation method you might need to take additional actions to scrape the Argo Rollouts endpoint. For example, if you used the Helm chart of Prometheus you need to annotate your Argo Rollouts Controller with the following: metadata : annotations : prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"8090\" You can always see if the controller is reached successfully in the Prometheus \"Targets\" screen: Once the controller metrics are read by your Prometheus instance, you can use them like any other Prometheus data source. Creating Grafana Dashboards \u00b6 You can easily visualize the metrics from the controller using Grafana dashboards. Install Grafana in your cluster and connect it your Prometheus instance . Then you can create any dashboard by using the available metrics (described in detail in the next sections). As a starting point you can find an existing dashboard at https://github.com/argoproj/argo-rollouts/blob/master/examples/dashboard.json You can import this Dashboard in your Grafana installation as a JSON file . Available metrics for Rollout Objects \u00b6 The Argo Rollouts controller publishes the following prometheus metrics about Argo Rollout objects. Name Description rollout_info Information about rollout. rollout_info_replicas_available The number of available replicas per rollout. rollout_info_replicas_unavailable The number of unavailable replicas per rollout. rollout_phase Information on the state of the rollout. rollout_reconcile Rollout reconciliation performance. rollout_reconcile_error Error occurring during the rollout. experiment_info Information about Experiment. experiment_phase Information on the state of the experiment. experiment_reconcile Experiments reconciliation performance. experiment_reconcile_error Error occurring during the experiment. analysis_run_info Information about analysis run. analysis_run_metric_phase Information on the duration of a specific metric in the Analysis Run. analysis_run_metric_type Information on the type of a specific metric in the Analysis Runs. analysis_run_phase Information on the state of the Analysis Run. analysis_run_reconcile Analysis Run reconciliation performance. analysis_run_reconcile_error Error occurring during the analysis run. Available metrics for the controller itself \u00b6 The controller also publishes the following Prometheus metrics to describe the controller health. Name Description controller_clientset_k8s_request_total Number of kubernetes requests executed during application reconciliation. workqueue_adds_total Total number of adds handled by workqueue workqueue_depth Current depth of workqueue workqueue_queue_duration_seconds How long in seconds an item stays in workqueue before being requested. workqueue_work_duration_seconds How long in seconds processing an item from workqueue takes. workqueue_unfinished_work_seconds How many seconds of work has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases. workqueue_longest_running_processor_seconds How many seconds has the longest running processor for workqueue been running workqueue_retries_total Total number of retries handled by workqueue In addition, the Argo-rollouts offers metrics on CPU, memory and file descriptor usage as well as the process start time and memory stats of current Go processes.","title":"Controller Metrics"},{"location":"features/controller-metrics/#controller-metrics","text":"The Argo Rollouts controller is already instrumented with Prometheus metrics available at /metrics in port 8090. You can use these metrics to look at the health of the controller either via dashboards or via other Prometheus integrations.","title":"Controller Metrics"},{"location":"features/controller-metrics/#installing-and-configuring-prometheus","text":"To take advantage of the metrics you need to have Prometheus installed in your Kubernetes cluster. If you don't have an existing installation of Prometheus you can use any of the common methods to install it in your cluster. Popular options include the Prometheus Helm chart or the Prometheus Operator . Once Prometheus is running in your cluster you need to make sure that it scrapes the Argo Rollouts endpoint. Prometheus already contains a service discovery mechanism for Kubernetes, but you need to configure it first . Depending on your installation method you might need to take additional actions to scrape the Argo Rollouts endpoint. For example, if you used the Helm chart of Prometheus you need to annotate your Argo Rollouts Controller with the following: metadata : annotations : prometheus.io/scrape : \"true\" prometheus.io/path : /metrics prometheus.io/port : \"8090\" You can always see if the controller is reached successfully in the Prometheus \"Targets\" screen: Once the controller metrics are read by your Prometheus instance, you can use them like any other Prometheus data source.","title":"Installing and configuring Prometheus"},{"location":"features/controller-metrics/#creating-grafana-dashboards","text":"You can easily visualize the metrics from the controller using Grafana dashboards. Install Grafana in your cluster and connect it your Prometheus instance . Then you can create any dashboard by using the available metrics (described in detail in the next sections). As a starting point you can find an existing dashboard at https://github.com/argoproj/argo-rollouts/blob/master/examples/dashboard.json You can import this Dashboard in your Grafana installation as a JSON file .","title":"Creating Grafana Dashboards"},{"location":"features/controller-metrics/#available-metrics-for-rollout-objects","text":"The Argo Rollouts controller publishes the following prometheus metrics about Argo Rollout objects. Name Description rollout_info Information about rollout. rollout_info_replicas_available The number of available replicas per rollout. rollout_info_replicas_unavailable The number of unavailable replicas per rollout. rollout_phase Information on the state of the rollout. rollout_reconcile Rollout reconciliation performance. rollout_reconcile_error Error occurring during the rollout. experiment_info Information about Experiment. experiment_phase Information on the state of the experiment. experiment_reconcile Experiments reconciliation performance. experiment_reconcile_error Error occurring during the experiment. analysis_run_info Information about analysis run. analysis_run_metric_phase Information on the duration of a specific metric in the Analysis Run. analysis_run_metric_type Information on the type of a specific metric in the Analysis Runs. analysis_run_phase Information on the state of the Analysis Run. analysis_run_reconcile Analysis Run reconciliation performance. analysis_run_reconcile_error Error occurring during the analysis run.","title":"Available metrics for Rollout Objects"},{"location":"features/controller-metrics/#available-metrics-for-the-controller-itself","text":"The controller also publishes the following Prometheus metrics to describe the controller health. Name Description controller_clientset_k8s_request_total Number of kubernetes requests executed during application reconciliation. workqueue_adds_total Total number of adds handled by workqueue workqueue_depth Current depth of workqueue workqueue_queue_duration_seconds How long in seconds an item stays in workqueue before being requested. workqueue_work_duration_seconds How long in seconds processing an item from workqueue takes. workqueue_unfinished_work_seconds How many seconds of work has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases. workqueue_longest_running_processor_seconds How many seconds has the longest running processor for workqueue been running workqueue_retries_total Total number of retries handled by workqueue In addition, the Argo-rollouts offers metrics on CPU, memory and file descriptor usage as well as the process start time and memory stats of current Go processes.","title":"Available metrics for the controller itself"},{"location":"features/ephemeral-metadata/","text":"Ephemeral Metadata \u00b6 Important Available for canary rollouts since v0.10.0 Important Available for blue-green rollouts since v1.0 One use case is for a Rollout to label or annotate the desired/stable pods with user-defined labels/annotations, for only the duration which they are the desired or stable set, and for the labels to be updated/removed as soon as the ReplicaSet switches roles (e.g. from desired to stable). The use case which this enables, is to allow prometheus, wavefront, datadog queries and dashboards to be built, which can rely on a consistent labels, rather than the rollouts-pod-template-hash which is unpredictable and changing from revision to revision. A Rollout using the canary strategy has the ability to attach ephemeral metadata to the stable or canary Pods using the stableMetadata and canaryMetadata fields respectively. spec : strategy : canary : stableMetadata : labels : role : stable canaryMetadata : labels : role : canary A Rollout using the blue-green strategy has the ability to attach ephemeral metadata to the active or preview Pods using the activeMetadata and previewMetadata fields respectively. spec : strategy : blueGreen : activeMetadata : labels : role : active previewMetadata : labels : role : preview During an update, the Rollout will create the desired ReplicaSet while also merging the metadata defined in canaryMetadata / previewMetadata to the desired ReplicaSet's spec.template.metadata . This results in all Pods of the ReplicaSet being created with the desired metadata. When the rollout becomes fully promoted, the desired ReplicaSet becomes the stable, and is updated to use the labels and annotations under stableMetadata / activeMetadata . The Pods of the ReplicaSet will then be updated in place to use the stable metadata (without recreating the pods). Important In order for tooling to take advantage of this feature, they would need to recognize the change in labels and/or annotations that happen after the Pod has already started. Not all tools may detect this.","title":"Ephemeral Metadata"},{"location":"features/ephemeral-metadata/#ephemeral-metadata","text":"Important Available for canary rollouts since v0.10.0 Important Available for blue-green rollouts since v1.0 One use case is for a Rollout to label or annotate the desired/stable pods with user-defined labels/annotations, for only the duration which they are the desired or stable set, and for the labels to be updated/removed as soon as the ReplicaSet switches roles (e.g. from desired to stable). The use case which this enables, is to allow prometheus, wavefront, datadog queries and dashboards to be built, which can rely on a consistent labels, rather than the rollouts-pod-template-hash which is unpredictable and changing from revision to revision. A Rollout using the canary strategy has the ability to attach ephemeral metadata to the stable or canary Pods using the stableMetadata and canaryMetadata fields respectively. spec : strategy : canary : stableMetadata : labels : role : stable canaryMetadata : labels : role : canary A Rollout using the blue-green strategy has the ability to attach ephemeral metadata to the active or preview Pods using the activeMetadata and previewMetadata fields respectively. spec : strategy : blueGreen : activeMetadata : labels : role : active previewMetadata : labels : role : preview During an update, the Rollout will create the desired ReplicaSet while also merging the metadata defined in canaryMetadata / previewMetadata to the desired ReplicaSet's spec.template.metadata . This results in all Pods of the ReplicaSet being created with the desired metadata. When the rollout becomes fully promoted, the desired ReplicaSet becomes the stable, and is updated to use the labels and annotations under stableMetadata / activeMetadata . The Pods of the ReplicaSet will then be updated in place to use the stable metadata (without recreating the pods). Important In order for tooling to take advantage of this feature, they would need to recognize the change in labels and/or annotations that happen after the Pod has already started. Not all tools may detect this.","title":"Ephemeral Metadata"},{"location":"features/experiment/","text":"Experiment CRD \u00b6 What is the Experiment CRD? \u00b6 The Experiment CRD allows users to have ephemeral runs of one or more ReplicaSets. In addition to running ephemeral ReplicaSets, the Experiment CRD can launch AnalysisRuns alongside the ReplicaSets. Generally, those AnalysisRun is used to confirm that new ReplicaSets are running as expected. Use cases of Experiments \u00b6 A user wants to run two versions of an application for a specific duration to enable Kayenta-style analysis of their application. The Experiment CRD creates 2 ReplicaSets (a baseline and a canary) based on the spec.templates field of the Experiment and waits until both are healthy. After the duration passes, the Experiment scales down the ReplicaSets, and the user can start the Kayenta analysis run. A user can use experiments to enable A/B/C testing by launching multiple experiments with a different version of their application for a long duration. Each Experiment has one PodSpec template that defines a specific version a user would want to run. The Experiment allows users to launch multiple experiments at once and keep each Experiment self-contained. Launching a new version of an existing application with different labels to avoid receiving traffic from a Kubernetes service. The user can run tests against the new version before continuing the Rollout. Experiment Spec \u00b6 Below is an example of an experiment that creates two ReplicaSets with 1 replica each and runs them for 20 minutes once they both become available. Additionally, several AnalysisRuns are run to perform analysis against the pods of the Experiment apiVersion : argoproj.io/v1alpha1 kind : Experiment metadata : name : example-experiment spec : # Duration of the experiment, beginning from when all ReplicaSets became healthy (optional) # If omitted, will run indefinitely until terminated, or until all analyses which were marked # `requiredForCompletion` have completed. duration : 20m # Deadline in seconds in which a ReplicaSet should make progress towards becoming available. # If exceeded, the Experiment will fail. progressDeadlineSeconds : 30 # List of pod template specs to run in the experiment as ReplicaSets templates : - name : purple # Number of replicas to run (optional). If omitted, will run a single replica replicas : 1 selector : matchLabels : app : canary-demo color : purple template : metadata : labels : app : canary-demo color : purple spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:purple imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP - name : orange replicas : 1 minReadySeconds : 10 selector : matchLabels : app : canary-demo color : orange template : metadata : labels : app : canary-demo color : orange spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:orange imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP # List of AnalysisTemplate references to perform during the experiment analyses : - name : purple templateName : http-benchmark args : - name : host value : purple - name : orange templateName : http-benchmark args : - name : host value : orange - name : compare-results templateName : compare # If requiredForCompletion is true for an analysis reference, the Experiment will not complete # until this analysis has completed. requiredForCompletion : true args : - name : host value : purple Experiment Lifecycle \u00b6 An Experiment is intended to temporarily run one or more templates. The lifecycle of an Experiment is as follows: Create and scale a ReplicaSet for each pod template specified under spec.templates Wait for all ReplicaSets reach full availability. If a ReplicaSet does not become available within spec.progressDeadlineSeconds , the Experiment will fail. Once available, the Experiment will transition from the Pending state to a Running state. Once an Experiment is considered Running , it will begin an AnalysisRun for every AnalysisTemplate referenced under spec.analyses . If a duration is specified under spec.duration , the Experiment will wait until the duration has elapsed before completing the Experiment. If an AnalysisRun fails or errors, the Experiment will end prematurely, with a status equal to the unsuccessful AnalysisRun (i.e. Failed or Error ) If one or more of the referenced AnalysisTemplates is marked with requiredForCompletion: true , the Experiment will not complete until those AnalysisRuns have completed, even if it exceeds the Experiment duration. If neither a spec.duration or requiredForCompletion: true is specified, the Experiment will run indefinitely, until explicitly terminated (by setting spec.terminate: true ). Once an Experiment is complete, the ReplicaSets will be scaled to zero, and any incomplete AnalysisRuns will be terminated. Note ReplicaSet names are generated by combining the Experiment name with the template name. Integration With Rollouts \u00b6 A rollout using the Canary strategy can create an experiment using an experiment step. The experiment step serves as a blocking step for the Rollout, and a Rollout will not continue until the Experiment succeeds. The Rollout creates an Experiment using the configuration in the experiment step of the Rollout. If the Experiment fails or errors, the Rollout will abort. Note Experiment names are generated by combining the Rollout's name, the PodHash of the new ReplicaSet, the current revision of the Rollout, and the current step-index. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : ... strategy : canary : steps : - experiment : duration : 1h templates : - name : baseline specRef : stable - name : canary specRef : canary analyses : - name : mann-whitney templateName : mann-whitney args : - name : baseline-hash value : \"{{templates.baseline.podTemplateHash}}\" - name : canary-hash value : \"{{templates.canary.podTemplateHash}}\" In the example above, during an update of a Rollout, the Rollout will launch an Experiment. The Experiment will create two ReplicaSets: baseline and canary , with one replica each, and will run for one hour. The baseline template uses the PodSpec from the stable ReplicaSet, and the canary template uses the PodSpec from the canary ReplicaSet. Additionally, the Experiment will perform analysis using the AnalysisTemplate named mann-whitney . The AnalysisRun is supplied with the pod-hash details of the baseline and canary to perform the necessary metrics queries, using the {{templates.baseline.podTemplateHash}} and {{templates.canary.podTemplateHash}} variables respectively. Note The pod-hashes of the baseline / canary ReplicaSets created by the Experiment, will have different values than the pod-hashes of the stable / canary ReplicaSets created by the Rollout. This is despite the fact that the PodSpec are the same. This is intentional behavior, in order to allow the metrics of the Experiment's pods to be delineated and queried separately from the metrics of the Rollout pods. Weighted Experiment Step with Traffic Routing \u00b6 Important Available since v1.1 A Rollout using the Canary strategy along with Traffic Routing can split traffic to an experiment stack in a fine-grained manner. When Traffic Routing is enabled, the Rollout Experiment step allows traffic to be shifted to experiment pods. Note This feature is currently available only for the SMI, ALB, and Istio Traffic Routers. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : ... strategy : canary : trafficRouting : alb : ingress : ingress ... steps : - experiment : duration : 1h templates : - name : experiment-baseline specRef : stable weight : 5 - name : experiment-canary specRef : canary weight : 5 In the above example, during an update, the first step would start a baseline vs. canary experiment. When pods are ready (Experiment enters Running phase), the rollout would direct 5% of traffic to experiment-canary and 5% to experiment-baseline , leaving the remaining 90% of traffic to the old stack. Note When a weighted experiment step with traffic routing is used, a service is auto-created for each experiment template. The traffic routers use this service to send traffic to the experiment pods.","title":"Experiments"},{"location":"features/experiment/#experiment-crd","text":"","title":"Experiment CRD"},{"location":"features/experiment/#what-is-the-experiment-crd","text":"The Experiment CRD allows users to have ephemeral runs of one or more ReplicaSets. In addition to running ephemeral ReplicaSets, the Experiment CRD can launch AnalysisRuns alongside the ReplicaSets. Generally, those AnalysisRun is used to confirm that new ReplicaSets are running as expected.","title":"What is the Experiment CRD?"},{"location":"features/experiment/#use-cases-of-experiments","text":"A user wants to run two versions of an application for a specific duration to enable Kayenta-style analysis of their application. The Experiment CRD creates 2 ReplicaSets (a baseline and a canary) based on the spec.templates field of the Experiment and waits until both are healthy. After the duration passes, the Experiment scales down the ReplicaSets, and the user can start the Kayenta analysis run. A user can use experiments to enable A/B/C testing by launching multiple experiments with a different version of their application for a long duration. Each Experiment has one PodSpec template that defines a specific version a user would want to run. The Experiment allows users to launch multiple experiments at once and keep each Experiment self-contained. Launching a new version of an existing application with different labels to avoid receiving traffic from a Kubernetes service. The user can run tests against the new version before continuing the Rollout.","title":"Use cases of Experiments"},{"location":"features/experiment/#experiment-spec","text":"Below is an example of an experiment that creates two ReplicaSets with 1 replica each and runs them for 20 minutes once they both become available. Additionally, several AnalysisRuns are run to perform analysis against the pods of the Experiment apiVersion : argoproj.io/v1alpha1 kind : Experiment metadata : name : example-experiment spec : # Duration of the experiment, beginning from when all ReplicaSets became healthy (optional) # If omitted, will run indefinitely until terminated, or until all analyses which were marked # `requiredForCompletion` have completed. duration : 20m # Deadline in seconds in which a ReplicaSet should make progress towards becoming available. # If exceeded, the Experiment will fail. progressDeadlineSeconds : 30 # List of pod template specs to run in the experiment as ReplicaSets templates : - name : purple # Number of replicas to run (optional). If omitted, will run a single replica replicas : 1 selector : matchLabels : app : canary-demo color : purple template : metadata : labels : app : canary-demo color : purple spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:purple imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP - name : orange replicas : 1 minReadySeconds : 10 selector : matchLabels : app : canary-demo color : orange template : metadata : labels : app : canary-demo color : orange spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:orange imagePullPolicy : Always ports : - name : http containerPort : 8080 protocol : TCP # List of AnalysisTemplate references to perform during the experiment analyses : - name : purple templateName : http-benchmark args : - name : host value : purple - name : orange templateName : http-benchmark args : - name : host value : orange - name : compare-results templateName : compare # If requiredForCompletion is true for an analysis reference, the Experiment will not complete # until this analysis has completed. requiredForCompletion : true args : - name : host value : purple","title":"Experiment Spec"},{"location":"features/experiment/#experiment-lifecycle","text":"An Experiment is intended to temporarily run one or more templates. The lifecycle of an Experiment is as follows: Create and scale a ReplicaSet for each pod template specified under spec.templates Wait for all ReplicaSets reach full availability. If a ReplicaSet does not become available within spec.progressDeadlineSeconds , the Experiment will fail. Once available, the Experiment will transition from the Pending state to a Running state. Once an Experiment is considered Running , it will begin an AnalysisRun for every AnalysisTemplate referenced under spec.analyses . If a duration is specified under spec.duration , the Experiment will wait until the duration has elapsed before completing the Experiment. If an AnalysisRun fails or errors, the Experiment will end prematurely, with a status equal to the unsuccessful AnalysisRun (i.e. Failed or Error ) If one or more of the referenced AnalysisTemplates is marked with requiredForCompletion: true , the Experiment will not complete until those AnalysisRuns have completed, even if it exceeds the Experiment duration. If neither a spec.duration or requiredForCompletion: true is specified, the Experiment will run indefinitely, until explicitly terminated (by setting spec.terminate: true ). Once an Experiment is complete, the ReplicaSets will be scaled to zero, and any incomplete AnalysisRuns will be terminated. Note ReplicaSet names are generated by combining the Experiment name with the template name.","title":"Experiment Lifecycle"},{"location":"features/experiment/#integration-with-rollouts","text":"A rollout using the Canary strategy can create an experiment using an experiment step. The experiment step serves as a blocking step for the Rollout, and a Rollout will not continue until the Experiment succeeds. The Rollout creates an Experiment using the configuration in the experiment step of the Rollout. If the Experiment fails or errors, the Rollout will abort. Note Experiment names are generated by combining the Rollout's name, the PodHash of the new ReplicaSet, the current revision of the Rollout, and the current step-index. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : ... strategy : canary : steps : - experiment : duration : 1h templates : - name : baseline specRef : stable - name : canary specRef : canary analyses : - name : mann-whitney templateName : mann-whitney args : - name : baseline-hash value : \"{{templates.baseline.podTemplateHash}}\" - name : canary-hash value : \"{{templates.canary.podTemplateHash}}\" In the example above, during an update of a Rollout, the Rollout will launch an Experiment. The Experiment will create two ReplicaSets: baseline and canary , with one replica each, and will run for one hour. The baseline template uses the PodSpec from the stable ReplicaSet, and the canary template uses the PodSpec from the canary ReplicaSet. Additionally, the Experiment will perform analysis using the AnalysisTemplate named mann-whitney . The AnalysisRun is supplied with the pod-hash details of the baseline and canary to perform the necessary metrics queries, using the {{templates.baseline.podTemplateHash}} and {{templates.canary.podTemplateHash}} variables respectively. Note The pod-hashes of the baseline / canary ReplicaSets created by the Experiment, will have different values than the pod-hashes of the stable / canary ReplicaSets created by the Rollout. This is despite the fact that the PodSpec are the same. This is intentional behavior, in order to allow the metrics of the Experiment's pods to be delineated and queried separately from the metrics of the Rollout pods.","title":"Integration With Rollouts"},{"location":"features/experiment/#weighted-experiment-step-with-traffic-routing","text":"Important Available since v1.1 A Rollout using the Canary strategy along with Traffic Routing can split traffic to an experiment stack in a fine-grained manner. When Traffic Routing is enabled, the Rollout Experiment step allows traffic to be shifted to experiment pods. Note This feature is currently available only for the SMI, ALB, and Istio Traffic Routers. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : guestbook labels : app : guestbook spec : ... strategy : canary : trafficRouting : alb : ingress : ingress ... steps : - experiment : duration : 1h templates : - name : experiment-baseline specRef : stable weight : 5 - name : experiment-canary specRef : canary weight : 5 In the above example, during an update, the first step would start a baseline vs. canary experiment. When pods are ready (Experiment enters Running phase), the rollout would direct 5% of traffic to experiment-canary and 5% to experiment-baseline , leaving the remaining 90% of traffic to the old stack. Note When a weighted experiment step with traffic routing is used, a service is auto-created for each experiment template. The traffic routers use this service to send traffic to the experiment pods.","title":"Weighted Experiment Step with Traffic Routing"},{"location":"features/helm/","text":"Using Argo Rollouts with Helm \u00b6 Argo Rollouts will always respond to changes in Rollouts resources regardless of how the change was made. This means that Argo Rollouts is compatible with all templating solutions that you might use to manage your deployments. Argo Rollouts manifests can be managed with the Helm package manager . If your Helm chart contains Rollout Resources, then as soon as you install/upgrade your chart, Argo Rollouts will take over and initiate the progressive delivery process. Here is an example Rollout that is managed with Helm: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : {{ template \"helm-guestbook.fullname\" . }} labels : app : {{ template \"helm-guestbook.name\" . }} chart : {{ template \"helm-guestbook.chart\" . }} release : {{ .Release.Name }} heritage : {{ .Release.Service }} spec : replicas : {{ .Values.replicaCount }} revisionHistoryLimit : 3 selector : matchLabels : app : {{ template \"helm-guestbook.name\" . }} release : {{ .Release.Name }} strategy : blueGreen : activeService : {{ template \"helm-guestbook.fullname\" . }} previewService : {{ template \"helm-guestbook.fullname\" . }} -preview template : metadata : labels : app : {{ template \"helm-guestbook.name\" . }} release : {{ .Release.Name }} spec : containers : - name : {{ .Chart.Name }} image : \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy : {{ .Values.image.pullPolicy }} ports : - name : http containerPort : 80 protocol : TCP livenessProbe : httpGet : path : / port : http readinessProbe : httpGet : path : / port : http resources : {{ toYaml .Values.resources | indent 12 }} {{ - with .Values.nodeSelector }} nodeSelector : {{ toYaml . | indent 8 }} {{ - end }} {{ - with .Values.affinity }} affinity : {{ toYaml . | indent 8 }} {{ - end }} {{ - with .Values.tolerations }} tolerations : {{ toYaml . | indent 8 }} {{ - end }} You can find the full example at https://github.com/argoproj/argo-rollouts/tree/master/examples/helm-blue-green .","title":"Helm"},{"location":"features/helm/#using-argo-rollouts-with-helm","text":"Argo Rollouts will always respond to changes in Rollouts resources regardless of how the change was made. This means that Argo Rollouts is compatible with all templating solutions that you might use to manage your deployments. Argo Rollouts manifests can be managed with the Helm package manager . If your Helm chart contains Rollout Resources, then as soon as you install/upgrade your chart, Argo Rollouts will take over and initiate the progressive delivery process. Here is an example Rollout that is managed with Helm: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : {{ template \"helm-guestbook.fullname\" . }} labels : app : {{ template \"helm-guestbook.name\" . }} chart : {{ template \"helm-guestbook.chart\" . }} release : {{ .Release.Name }} heritage : {{ .Release.Service }} spec : replicas : {{ .Values.replicaCount }} revisionHistoryLimit : 3 selector : matchLabels : app : {{ template \"helm-guestbook.name\" . }} release : {{ .Release.Name }} strategy : blueGreen : activeService : {{ template \"helm-guestbook.fullname\" . }} previewService : {{ template \"helm-guestbook.fullname\" . }} -preview template : metadata : labels : app : {{ template \"helm-guestbook.name\" . }} release : {{ .Release.Name }} spec : containers : - name : {{ .Chart.Name }} image : \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" imagePullPolicy : {{ .Values.image.pullPolicy }} ports : - name : http containerPort : 80 protocol : TCP livenessProbe : httpGet : path : / port : http readinessProbe : httpGet : path : / port : http resources : {{ toYaml .Values.resources | indent 12 }} {{ - with .Values.nodeSelector }} nodeSelector : {{ toYaml . | indent 8 }} {{ - end }} {{ - with .Values.affinity }} affinity : {{ toYaml . | indent 8 }} {{ - end }} {{ - with .Values.tolerations }} tolerations : {{ toYaml . | indent 8 }} {{ - end }} You can find the full example at https://github.com/argoproj/argo-rollouts/tree/master/examples/helm-blue-green .","title":"Using Argo Rollouts with Helm"},{"location":"features/hpa-support/","text":"Horizontal Pod Autoscaling \u00b6 Horizontal Pod Autoscaling (HPA) automatically scales the number of pods in owned by a Kubernetes resource based on observed CPU utilization or user-configured metrics. In order to accomplish this behavior, HPA only supports resources with the scale endpoint enabled with a couple of required fields. The scale endpoint allows the HPA to understand the current state of a resource and modify the resource to scale it appropriately. Argo Rollouts added support for the scale endpoint in the 0.3.0 release. After being modified by the HPA, the Argo Rollouts controller is responsible for reconciling that change in replicas. Since the strategies within a Rollout are very different, the Argo Rollouts controller handles the scale endpoint differently for various strategies. Below is the behavior for the different strategies: Blue Green \u00b6 The HPA will scale rollouts using the BlueGreen strategy using the metrics from the ReplicaSet receiving traffic from the active service. When the HPA changes the replicas count, the Argo Rollouts controller will first scale up the ReplicaSet receiving traffic from the active service before ReplicaSet receiving traffic from the preview service. The controller will scale up the ReplicaSet receiving traffic from the preview service to prepare it for when the rollout switches the preview to active. If there are no ReplicaSets receiving from the active service, the controller will use all the pods that match the base selector to determine scaling events. In that case, the controller will scale up the latest ReplicaSet to the new count and scale down the older ReplicaSets. Canary (ReplicaSet based) \u00b6 The HPA will scale rollouts using the Canary Strategy using the metrics of all the ReplicasSets within the rollout. Since the Argo Rollouts controller does not control the service that sends traffic to those ReplicaSets, it assumes that all the ReplicaSets in the rollout are receiving traffic. Example \u00b6 Below is an example of a Horizontal Pod Autoscaler that scales a rollout based on CPU metrics: apiVersion : autoscaling/v1 kind : HorizontalPodAutoscaler metadata : name : hpa-rollout-example spec : maxReplicas : 6 minReplicas : 2 scaleTargetRef : apiVersion : argoproj.io/v1alpha1 kind : Rollout name : example-rollout targetCPUUtilizationPercentage : 80 Requirements \u00b6 In order for the HPA to manipulate the rollout, the Kubernetes cluster hosting the rollout CRD needs the subresources support for CRDs. This feature was introduced as alpha in Kubernetes version 1.10 and transitioned to beta in Kubernetes version 1.11. If a user wants to use HPA on v1.10, the Kubernetes Cluster operator will need to add a custom feature flag to the API server. After 1.10, the flag is turned on by default. Check out the following link for more information on setting the custom feature flag.","title":"HPA"},{"location":"features/hpa-support/#horizontal-pod-autoscaling","text":"Horizontal Pod Autoscaling (HPA) automatically scales the number of pods in owned by a Kubernetes resource based on observed CPU utilization or user-configured metrics. In order to accomplish this behavior, HPA only supports resources with the scale endpoint enabled with a couple of required fields. The scale endpoint allows the HPA to understand the current state of a resource and modify the resource to scale it appropriately. Argo Rollouts added support for the scale endpoint in the 0.3.0 release. After being modified by the HPA, the Argo Rollouts controller is responsible for reconciling that change in replicas. Since the strategies within a Rollout are very different, the Argo Rollouts controller handles the scale endpoint differently for various strategies. Below is the behavior for the different strategies:","title":"Horizontal Pod Autoscaling"},{"location":"features/hpa-support/#blue-green","text":"The HPA will scale rollouts using the BlueGreen strategy using the metrics from the ReplicaSet receiving traffic from the active service. When the HPA changes the replicas count, the Argo Rollouts controller will first scale up the ReplicaSet receiving traffic from the active service before ReplicaSet receiving traffic from the preview service. The controller will scale up the ReplicaSet receiving traffic from the preview service to prepare it for when the rollout switches the preview to active. If there are no ReplicaSets receiving from the active service, the controller will use all the pods that match the base selector to determine scaling events. In that case, the controller will scale up the latest ReplicaSet to the new count and scale down the older ReplicaSets.","title":"Blue Green"},{"location":"features/hpa-support/#canary-replicaset-based","text":"The HPA will scale rollouts using the Canary Strategy using the metrics of all the ReplicasSets within the rollout. Since the Argo Rollouts controller does not control the service that sends traffic to those ReplicaSets, it assumes that all the ReplicaSets in the rollout are receiving traffic.","title":"Canary (ReplicaSet based)"},{"location":"features/hpa-support/#example","text":"Below is an example of a Horizontal Pod Autoscaler that scales a rollout based on CPU metrics: apiVersion : autoscaling/v1 kind : HorizontalPodAutoscaler metadata : name : hpa-rollout-example spec : maxReplicas : 6 minReplicas : 2 scaleTargetRef : apiVersion : argoproj.io/v1alpha1 kind : Rollout name : example-rollout targetCPUUtilizationPercentage : 80","title":"Example"},{"location":"features/hpa-support/#requirements","text":"In order for the HPA to manipulate the rollout, the Kubernetes cluster hosting the rollout CRD needs the subresources support for CRDs. This feature was introduced as alpha in Kubernetes version 1.10 and transitioned to beta in Kubernetes version 1.11. If a user wants to use HPA on v1.10, the Kubernetes Cluster operator will need to add a custom feature flag to the API server. After 1.10, the flag is turned on by default. Check out the following link for more information on setting the custom feature flag.","title":"Requirements"},{"location":"features/kubectl-plugin/","text":"Kubectl Plugin \u00b6 Kubectl plugins are a way to extend the kubectl command to provide additional behavior. Generally, they are used to add new functionality to kubectl and automate scriptable workflows against a cluster. The official documentation on them is here . Argo Rollouts offers a Kubectl plugin to enrich the experience with Rollouts, Experiments, and Analysis from the command line. It offers the ability to visualize the Argo Rollouts resources and run routine operations like promote or retry on those resources from the command. Installation \u00b6 See the installation guide for instructions on installing the plugin. Usage \u00b6 The best way to get information on the available Argo Rollouts kubectl plugin commands is by run kubectl argo rollouts . The plugin lists all the available commands that the tool can execute along with a description of each commend. All the plugin's commands interact with the Kubernetes API server and use KubeConfig credentials for authentication. Since the plugin leverages the KubeConfig of the user running the command, the plugin has the permissions of those configs. Similar to kubectl, the plugin uses many of the same flags as the kubectl. For example, the kubectl argo rollouts get rollout canary-demo -w command starts a watch on the canary-demo rollout object similar to how the kubectl get deployment canary-demo -w command starts a watch on a deployment. Visualizing Rollouts and Experiments \u00b6 In addition to encapsulating many routine commands, the Argo Rollouts kubectl plugin supports visualizing rollouts and experiments with the get command. The get command provides a clean representation of either the rollouts or the experiments running in a cluster. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. As an example, here is a rollout retrieved with a get command: Here is a table to explain some of the icons on the tree view: Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job If the get command includes the watch flag ( -w or --watch ), the terminal updates as the rollouts or experiment progress highlighting the progress.","title":"Overview"},{"location":"features/kubectl-plugin/#kubectl-plugin","text":"Kubectl plugins are a way to extend the kubectl command to provide additional behavior. Generally, they are used to add new functionality to kubectl and automate scriptable workflows against a cluster. The official documentation on them is here . Argo Rollouts offers a Kubectl plugin to enrich the experience with Rollouts, Experiments, and Analysis from the command line. It offers the ability to visualize the Argo Rollouts resources and run routine operations like promote or retry on those resources from the command.","title":"Kubectl Plugin"},{"location":"features/kubectl-plugin/#installation","text":"See the installation guide for instructions on installing the plugin.","title":"Installation"},{"location":"features/kubectl-plugin/#usage","text":"The best way to get information on the available Argo Rollouts kubectl plugin commands is by run kubectl argo rollouts . The plugin lists all the available commands that the tool can execute along with a description of each commend. All the plugin's commands interact with the Kubernetes API server and use KubeConfig credentials for authentication. Since the plugin leverages the KubeConfig of the user running the command, the plugin has the permissions of those configs. Similar to kubectl, the plugin uses many of the same flags as the kubectl. For example, the kubectl argo rollouts get rollout canary-demo -w command starts a watch on the canary-demo rollout object similar to how the kubectl get deployment canary-demo -w command starts a watch on a deployment.","title":"Usage"},{"location":"features/kubectl-plugin/#visualizing-rollouts-and-experiments","text":"In addition to encapsulating many routine commands, the Argo Rollouts kubectl plugin supports visualizing rollouts and experiments with the get command. The get command provides a clean representation of either the rollouts or the experiments running in a cluster. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. As an example, here is a rollout retrieved with a get command: Here is a table to explain some of the icons on the tree view: Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job If the get command includes the watch flag ( -w or --watch ), the terminal updates as the rollouts or experiment progress highlighting the progress.","title":"Visualizing Rollouts and Experiments"},{"location":"features/kustomize/","text":"Kustomize Integration \u00b6 Kustomize can be extended to understand CRD objects through the use of transformer configs . Using transformer configs, kustomize can be \"taught\" about the structure of a Rollout object and leverage kustomize features such as ConfigMap/Secret generators, variable references, and common labels & annotations. To use Rollouts with kustomize: Download rollout-transform.yaml into your kustomize directory. Include rollout-transform.yaml in your kustomize configurations section: kind : Kustomization apiVersion : kustomize.config.k8s.io/v1beta1 configurations : - rollout-transform.yaml An example kustomize app demonstrating the ability to use transformers with Rollouts can be seen here . With Kustomize 3.6.1 it is possible to reference the configuration directly from a remote resource: configurations : - https://argoproj.github.io/argo-rollouts/features/kustomize/rollout-transform.yaml With Kustomize 4.1.0 kustomize can use kubernetes OpenAPI data to get merge key and patch strategy information about resource types . For example, given the following rollout: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-canary spec : strategy : canary : steps : # detail of the canary steps is omitted template : metadata : labels : app : rollout-canary spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue imagePullPolicy : Always ports : - containerPort : 8080 user can update the Rollout via a patch in a kustomization file, to change the image to nginx apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - rollout-canary.yaml openapi : path : <path-to-directory>/rollout_cr_schema.json patchesStrategicMerge : - |- apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-canary spec: template: spec: containers: - name: rollouts-demo image: nginx The OpenAPI data is auto-generated and defined in this file . An example kustomize app demonstrating the ability to use OpenAPI data with Rollouts can be seen here .","title":"Kustomize"},{"location":"features/kustomize/#kustomize-integration","text":"Kustomize can be extended to understand CRD objects through the use of transformer configs . Using transformer configs, kustomize can be \"taught\" about the structure of a Rollout object and leverage kustomize features such as ConfigMap/Secret generators, variable references, and common labels & annotations. To use Rollouts with kustomize: Download rollout-transform.yaml into your kustomize directory. Include rollout-transform.yaml in your kustomize configurations section: kind : Kustomization apiVersion : kustomize.config.k8s.io/v1beta1 configurations : - rollout-transform.yaml An example kustomize app demonstrating the ability to use transformers with Rollouts can be seen here . With Kustomize 3.6.1 it is possible to reference the configuration directly from a remote resource: configurations : - https://argoproj.github.io/argo-rollouts/features/kustomize/rollout-transform.yaml With Kustomize 4.1.0 kustomize can use kubernetes OpenAPI data to get merge key and patch strategy information about resource types . For example, given the following rollout: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-canary spec : strategy : canary : steps : # detail of the canary steps is omitted template : metadata : labels : app : rollout-canary spec : containers : - name : rollouts-demo image : argoproj/rollouts-demo:blue imagePullPolicy : Always ports : - containerPort : 8080 user can update the Rollout via a patch in a kustomization file, to change the image to nginx apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - rollout-canary.yaml openapi : path : <path-to-directory>/rollout_cr_schema.json patchesStrategicMerge : - |- apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-canary spec: template: spec: containers: - name: rollouts-demo image: nginx The OpenAPI data is auto-generated and defined in this file . An example kustomize app demonstrating the ability to use OpenAPI data with Rollouts can be seen here .","title":"Kustomize Integration"},{"location":"features/notifications/","text":"Notifications \u00b6 Important Available since v1.1 Argo Rollouts provides notifications powered by the Notifications Engine . Controller administrators can leverage flexible systems of triggers and templates to configure notifications requested by the end users. The end-users can subscribe to the configured triggers by adding an annotation to the Rollout objects. Configuration \u00b6 The trigger defines the condition when the notification should be sent as well as the notification content template. Default Argo Rollouts comes with a list of built-in triggers that cover the most important events of Argo Rollout live-cycle. Both triggers and templates are configured in the argo-rollouts-notification-configmap ConfigMap. In order to get started quickly, you can use pre-configured notification templates defined in notifications-install.yaml . If you are leveraging Kustomize it is recommended to include notifications-install.yaml as a remote resource into your kustomization.yaml file: apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml - https://github.com/argoproj/argo-rollouts/releases/latest/download/notifications-install.yaml After including the argo-rollouts-notification-configmap ConfigMap the administrator needs to configure integration with the required notifications service such as Slack or MS Teams. An example below demonstrates Slack integration: apiVersion : v1 kind : ConfigMap metadata : name : argo-rollouts-notification-configmap data : # detail of the templates is omitted # detail of the triggers is omitted service.slack : | token: $slack-token --- apiVersion : v1 kind : Secret metadata : name : argo-rollouts-notification-secret stringData : slack-token : <my-slack-token> Learn more about supported services and configuration settings in services documentation . Default Trigger templates \u00b6 Currently the following triggers have built-in templates . on-rollout-completed when a rollout is finished and all its steps are completed on-rollout-step-completed when an individual step inside a rollout definition is completed on-rollout-updated when a rollout definition is changed on-scaling-replica-set when the number of replicas in a rollout is changed Subscriptions \u00b6 The end-users can start leveraging notifications using notifications.argoproj.io/subscribe.<trigger>.<service>: <recipient> annotation. For example, the following annotation subscribes two Slack channels to notifications about canary rollout step completion: --- apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-canary annotations : notifications.argoproj.io/subscribe.on-rollout-step-completed.slack : my-channel1;my-channel2 Annotation key consists of following parts: on-rollout-step-completed - trigger name slack - notification service name my-channel1;my-channel2 - a semicolon separated list of recipients Customization \u00b6 The Rollout administrator can customize the notifications by configuring notification templates and custom triggers in argo-rollouts-notification-configmap ConfigMap. Templates \u00b6 The notification template is a stateless function that generates the notification content. The template is leveraging html/template golang package. It is meant to be reusable and can be referenced by multiple triggers. An example below demonstrates a sample template: apiVersion : v1 kind : ConfigMap metadata : name : argo-rollouts-notification-configmap data : template.my-purple-template : | message: | Rollout {{.rollout.metadata.name}} has purple image slack: attachments: | [{ \"title\": \"{{ .rollout.metadata.name}}\", \"color\": \"#800080\" }] Each template has access to the following fields: rollout holds the rollout object. recipient holds the recipient name. The message field of the template definition allows creating a basic notification for any notification service. You can leverage notification service-specific fields to create complex notifications. For example using service-specific you can add blocks and attachments for Slack, subject for Email or URL path, and body for Webhook. See corresponding service documentation for more information. Custom Triggers \u00b6 In addition to custom notification template administrator and configure custom triggers. Custom trigger defines the condition when the notification should be sent. The definition includes name, condition and notification templates reference. The condition is a predicate expression that returns true if the notification should be sent. The trigger condition evaluation is powered by antonmedv/expr . The condition language syntax is described at Language-Definition.md . The trigger is configured in argo-rollouts-notification-configmap ConfigMap. For example the following trigger sends a notification when rollout pod spec uses argoproj/rollouts-demo:purple image: apiVersion : v1 kind : ConfigMap metadata : name : argo-rollouts-notification-configmap data : trigger.on-purple : | - send: [my-purple-template] when: rollout.spec.template.spec.containers[0].image == 'argoproj/rollouts-demo:purple' Each condition might use several templates. Typically each template is responsible for generating a service-specific notification part.","title":"Overview"},{"location":"features/notifications/#notifications","text":"Important Available since v1.1 Argo Rollouts provides notifications powered by the Notifications Engine . Controller administrators can leverage flexible systems of triggers and templates to configure notifications requested by the end users. The end-users can subscribe to the configured triggers by adding an annotation to the Rollout objects.","title":"Notifications"},{"location":"features/notifications/#configuration","text":"The trigger defines the condition when the notification should be sent as well as the notification content template. Default Argo Rollouts comes with a list of built-in triggers that cover the most important events of Argo Rollout live-cycle. Both triggers and templates are configured in the argo-rollouts-notification-configmap ConfigMap. In order to get started quickly, you can use pre-configured notification templates defined in notifications-install.yaml . If you are leveraging Kustomize it is recommended to include notifications-install.yaml as a remote resource into your kustomization.yaml file: apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml - https://github.com/argoproj/argo-rollouts/releases/latest/download/notifications-install.yaml After including the argo-rollouts-notification-configmap ConfigMap the administrator needs to configure integration with the required notifications service such as Slack or MS Teams. An example below demonstrates Slack integration: apiVersion : v1 kind : ConfigMap metadata : name : argo-rollouts-notification-configmap data : # detail of the templates is omitted # detail of the triggers is omitted service.slack : | token: $slack-token --- apiVersion : v1 kind : Secret metadata : name : argo-rollouts-notification-secret stringData : slack-token : <my-slack-token> Learn more about supported services and configuration settings in services documentation .","title":"Configuration"},{"location":"features/notifications/#default-trigger-templates","text":"Currently the following triggers have built-in templates . on-rollout-completed when a rollout is finished and all its steps are completed on-rollout-step-completed when an individual step inside a rollout definition is completed on-rollout-updated when a rollout definition is changed on-scaling-replica-set when the number of replicas in a rollout is changed","title":"Default Trigger templates"},{"location":"features/notifications/#subscriptions","text":"The end-users can start leveraging notifications using notifications.argoproj.io/subscribe.<trigger>.<service>: <recipient> annotation. For example, the following annotation subscribes two Slack channels to notifications about canary rollout step completion: --- apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-canary annotations : notifications.argoproj.io/subscribe.on-rollout-step-completed.slack : my-channel1;my-channel2 Annotation key consists of following parts: on-rollout-step-completed - trigger name slack - notification service name my-channel1;my-channel2 - a semicolon separated list of recipients","title":"Subscriptions"},{"location":"features/notifications/#customization","text":"The Rollout administrator can customize the notifications by configuring notification templates and custom triggers in argo-rollouts-notification-configmap ConfigMap.","title":"Customization"},{"location":"features/notifications/#templates","text":"The notification template is a stateless function that generates the notification content. The template is leveraging html/template golang package. It is meant to be reusable and can be referenced by multiple triggers. An example below demonstrates a sample template: apiVersion : v1 kind : ConfigMap metadata : name : argo-rollouts-notification-configmap data : template.my-purple-template : | message: | Rollout {{.rollout.metadata.name}} has purple image slack: attachments: | [{ \"title\": \"{{ .rollout.metadata.name}}\", \"color\": \"#800080\" }] Each template has access to the following fields: rollout holds the rollout object. recipient holds the recipient name. The message field of the template definition allows creating a basic notification for any notification service. You can leverage notification service-specific fields to create complex notifications. For example using service-specific you can add blocks and attachments for Slack, subject for Email or URL path, and body for Webhook. See corresponding service documentation for more information.","title":"Templates"},{"location":"features/notifications/#custom-triggers","text":"In addition to custom notification template administrator and configure custom triggers. Custom trigger defines the condition when the notification should be sent. The definition includes name, condition and notification templates reference. The condition is a predicate expression that returns true if the notification should be sent. The trigger condition evaluation is powered by antonmedv/expr . The condition language syntax is described at Language-Definition.md . The trigger is configured in argo-rollouts-notification-configmap ConfigMap. For example the following trigger sends a notification when rollout pod spec uses argoproj/rollouts-demo:purple image: apiVersion : v1 kind : ConfigMap metadata : name : argo-rollouts-notification-configmap data : trigger.on-purple : | - send: [my-purple-template] when: rollout.spec.template.spec.containers[0].image == 'argoproj/rollouts-demo:purple' Each condition might use several templates. Typically each template is responsible for generating a service-specific notification part.","title":"Custom Triggers"},{"location":"features/restart/","text":"Restarting Rollout Pods \u00b6 For various reasons, applications often need to be restarted, e.g. for hygiene purposes or to force startup logic to occur again such as reloading of a modified Secret. In these scenarios, it is undesirable to go through an entire blue-green or canary update process. Argo Rollouts supports the ability to restart all of its Pods by performing a rolling recreate of all the Pods in a Rollout while skipping the regular BlueGreen or Canary update strategy. How it works \u00b6 A rollout can be restarted via the kubectl plugin, using the restart command : kubectl-argo-rollouts restart ROLLOUT Alternatively, if Rollouts is used with Argo CD, the there is a bundled \"restart\" action which can be performed via the Argo CD UI or CLI: argocd app actions run my-app restart --kind Rollout --resource-name my-rollout Both of these mechanisms updates the Rollout's .spec.restartAt to the current time in the form of a RFC 3339 formatted UTC string (e.g. 2020-03-30T21:19:35Z), which indicates to the Rollout controller that all of a Rollout's Pods should have been created after this timestamp. During a restart, the controller iterates through each ReplicaSet to see if all the Pods have a creation timestamp which is newer than the restartAt time. For every pod older than the restartAt timestamp, the Pod will be evicted, allowing the ReplicaSet to replace the pod with a recreated one. To prevent too many Pods from restarting at once, the controller limits itself to deleting up to maxUnavailable Pods at a time. Secondly, since pods are evicted and not deleted, the restart process will honor any PodDisruptionBudgets which are in place. The controller restarts ReplicaSets in the following order: 1. stable ReplicaSet 2. current ReplicaSet 3. all other ReplicaSets beginning with the oldest If a Rollout's pod template spec ( spec.template ) is modified in the middle of a restart, the restart is canceled, and the normal blue-green or canary update will occur. Note: Unlike deployments, where a \"restart\" is nothing but a normal rolling upgrade that happened to be triggered by a timestamp in the pod spec annotation, Argo Rollouts facilitates restarts by terminating pods and allowing the existing ReplicaSet to replace the terminated pods. This design choice was made in order to allow a restart to occur even when a Rollout was in the middle of a long-running blue-green/canary update (e.g. a paused canary). However, some consequences of this are: Restarting a Rollout which has a single replica will cause downtime since Argo Rollouts needs to terminate the pod in order to replace it. Restarting a rollout will be slower than a deployment's rolling update, since maxSurge is not used to bring up newer pods faster. maxUnavailable will be used to restart multiple pods at a time (starting in v0.10). But if maxUnavailable pods is 0, the controller will still restart pods one at a time. Scheduled Restarts \u00b6 Users can schedule a restart on their Rollout by setting the .spec.restartAt field to a time in the future. The controller only starts the restart after the current time is after the restartAt time.","title":"Restarting Rollouts"},{"location":"features/restart/#restarting-rollout-pods","text":"For various reasons, applications often need to be restarted, e.g. for hygiene purposes or to force startup logic to occur again such as reloading of a modified Secret. In these scenarios, it is undesirable to go through an entire blue-green or canary update process. Argo Rollouts supports the ability to restart all of its Pods by performing a rolling recreate of all the Pods in a Rollout while skipping the regular BlueGreen or Canary update strategy.","title":"Restarting Rollout Pods"},{"location":"features/restart/#how-it-works","text":"A rollout can be restarted via the kubectl plugin, using the restart command : kubectl-argo-rollouts restart ROLLOUT Alternatively, if Rollouts is used with Argo CD, the there is a bundled \"restart\" action which can be performed via the Argo CD UI or CLI: argocd app actions run my-app restart --kind Rollout --resource-name my-rollout Both of these mechanisms updates the Rollout's .spec.restartAt to the current time in the form of a RFC 3339 formatted UTC string (e.g. 2020-03-30T21:19:35Z), which indicates to the Rollout controller that all of a Rollout's Pods should have been created after this timestamp. During a restart, the controller iterates through each ReplicaSet to see if all the Pods have a creation timestamp which is newer than the restartAt time. For every pod older than the restartAt timestamp, the Pod will be evicted, allowing the ReplicaSet to replace the pod with a recreated one. To prevent too many Pods from restarting at once, the controller limits itself to deleting up to maxUnavailable Pods at a time. Secondly, since pods are evicted and not deleted, the restart process will honor any PodDisruptionBudgets which are in place. The controller restarts ReplicaSets in the following order: 1. stable ReplicaSet 2. current ReplicaSet 3. all other ReplicaSets beginning with the oldest If a Rollout's pod template spec ( spec.template ) is modified in the middle of a restart, the restart is canceled, and the normal blue-green or canary update will occur. Note: Unlike deployments, where a \"restart\" is nothing but a normal rolling upgrade that happened to be triggered by a timestamp in the pod spec annotation, Argo Rollouts facilitates restarts by terminating pods and allowing the existing ReplicaSet to replace the terminated pods. This design choice was made in order to allow a restart to occur even when a Rollout was in the middle of a long-running blue-green/canary update (e.g. a paused canary). However, some consequences of this are: Restarting a Rollout which has a single replica will cause downtime since Argo Rollouts needs to terminate the pod in order to replace it. Restarting a rollout will be slower than a deployment's rolling update, since maxSurge is not used to bring up newer pods faster. maxUnavailable will be used to restart multiple pods at a time (starting in v0.10). But if maxUnavailable pods is 0, the controller will still restart pods one at a time.","title":"How it works"},{"location":"features/restart/#scheduled-restarts","text":"Users can schedule a restart on their Rollout by setting the .spec.restartAt field to a time in the future. The controller only starts the restart after the current time is after the restartAt time.","title":"Scheduled Restarts"},{"location":"features/scaledown-aborted-rs/","text":"Scaledown New Replicaset on Aborted Rollout \u00b6 Upon an aborted update, we may scale down the new replicaset for all strategies. Users can then choose to leave the new replicaset scaled up indefinitely by setting abortScaleDownDelaySeconds to 0, or adjust the value to something larger (or smaller). The following table summarizes the behavior under combinations of rollout strategy and abortScaleDownDelaySeconds . Note that abortScaleDownDelaySeconds is not applicable to argo-rollouts v1.0. abortScaleDownDelaySeconds = nil is the default, which means in v1.1 across all rollout strategies, the new replicaset is scaled down in 30 seconds on abort by default. strategy v1.0 behavior abortScaleDownDelaySeconds v1.1 behavior blue-green does not scale down nil scales down after 30 seconds blue-green does not scale down 0 does not scale down blue-green does not scale down N scales down after N seconds basic canary rolling update back to stable N/A rolling update back to stable canary w/ traffic routing scales down immediately nil scales down after 30 seconds canary w/ traffic routing scales down immediately 0 does not scale down canary w/ traffic routing scales down immediately N scales down after N seconds canary w/ traffic routing + setCanaryScale does not scale down (bug) * should behave like canary w/ traffic routing","title":"Scaledown Aborted Rollouts"},{"location":"features/scaledown-aborted-rs/#scaledown-new-replicaset-on-aborted-rollout","text":"Upon an aborted update, we may scale down the new replicaset for all strategies. Users can then choose to leave the new replicaset scaled up indefinitely by setting abortScaleDownDelaySeconds to 0, or adjust the value to something larger (or smaller). The following table summarizes the behavior under combinations of rollout strategy and abortScaleDownDelaySeconds . Note that abortScaleDownDelaySeconds is not applicable to argo-rollouts v1.0. abortScaleDownDelaySeconds = nil is the default, which means in v1.1 across all rollout strategies, the new replicaset is scaled down in 30 seconds on abort by default. strategy v1.0 behavior abortScaleDownDelaySeconds v1.1 behavior blue-green does not scale down nil scales down after 30 seconds blue-green does not scale down 0 does not scale down blue-green does not scale down N scales down after N seconds basic canary rolling update back to stable N/A rolling update back to stable canary w/ traffic routing scales down immediately nil scales down after 30 seconds canary w/ traffic routing scales down immediately 0 does not scale down canary w/ traffic routing scales down immediately N scales down after N seconds canary w/ traffic routing + setCanaryScale does not scale down (bug) * should behave like canary w/ traffic routing","title":"Scaledown New Replicaset on Aborted Rollout"},{"location":"features/specification/","text":"Rollout Specification \u00b6 The following describes all the available fields of a rollout: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout-canary spec : # Number of desired pods. # Defaults to 1. replicas : 5 analysis : # limits the number of successful analysis runs and experiments to be stored in a history # Defaults to 5. successfulRunHistoryLimit : 10 # limits the number of unsuccessful analysis runs and experiments to be stored in a history. # Stages for unsuccessful: \"Error\", \"Failed\", \"Inconclusive\" # Defaults to 5. unsuccessfulRunHistoryLimit : 10 # Label selector for pods. Existing ReplicaSets whose pods are selected by # this will be the ones affected by this rollout. It must match the pod # template's labels. selector : matchLabels : app : guestbook # Template describes the pods that will be created. Same as deployment template : spec : containers : - name : guestbook image : argoproj/rollouts-demo:blue # Minimum number of seconds for which a newly created pod should be ready # without any of its container crashing, for it to be considered available. # Defaults to 0 (pod will be considered available as soon as it is ready) minReadySeconds : 30 # The number of old ReplicaSets to retain. # Defaults to 10 revisionHistoryLimit : 3 # Pause allows a user to manually pause a rollout at any time. A rollout # will not advance through its steps while it is manually paused, but HPA # auto-scaling will still occur. Typically not explicitly set the manifest, # but controlled via tools (e.g. kubectl argo rollouts pause). If true at # initial creation of Rollout, replicas are not scaled up automatically # from zero unless manually promoted. paused : true # The maximum time in seconds in which a rollout must make progress during # an update, before it is considered to be failed. Argo Rollouts will # continue to process failed rollouts and a condition with a # ProgressDeadlineExceeded reason will be surfaced in the rollout status. # Note that progress will not be estimated during the time a rollout is # paused. # Defaults to 600s progressDeadlineSeconds : 600 # Whether to abort the update when ProgressDeadlineSeconds # is exceeded if analysis or experiment is not used. # Optional and default is false. progressDeadlineAbort : false # UTC timestamp in which a Rollout should sequentially restart all of # its pods. Used by the `kubectl argo rollouts restart ROLLOUT` command. # The controller will ensure all pods have a creationTimestamp greater # than or equal to this value. restartAt : \"2020-03-30T21:19:35Z\" strategy : # Blue-green update strategy blueGreen : # Reference to service that the rollout modifies as the active service. # Required. activeService : active-service # Pre-promotion analysis run which performs analysis before the service # cutover. +optional prePromotionAnalysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local # Post-promotion analysis run which performs analysis after the service # cutover. +optional postPromotionAnalysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local # Name of the service that the rollout modifies as the preview service. # +optional previewService : preview-service # The number of replicas to run under the preview service before the # switchover. Once the rollout is resumed the new ReplicaSet will be fully # scaled up before the switch occurs +optional previewReplicaCount : 1 # Indicates if the rollout should automatically promote the new ReplicaSet # to the active service or enter a paused state. If not specified, the # default value is true. +optional autoPromotionEnabled : false # Automatically promotes the current ReplicaSet to active after the # specified pause delay in seconds after the ReplicaSet becomes ready. # If omitted, the Rollout enters and remains in a paused state until # manually resumed by resetting spec.Paused to false. +optional autoPromotionSeconds : 30 # Adds a delay before scaling down the previous ReplicaSet. If omitted, # the Rollout waits 30 seconds before scaling down the previous ReplicaSet. # A minimum of 30 seconds is recommended to ensure IP table propagation # across the nodes in a cluster. scaleDownDelaySeconds : 30 # Limits the number of old RS that can run at once before getting scaled # down. Defaults to nil scaleDownDelayRevisionLimit : 2 # Add a delay in second before scaling down the preview replicaset # if update is aborted. 0 means not to scale down. Default is 30 second abortScaleDownDelaySeconds : 30 # Anti Affinity configuration between desired and previous ReplicaSet. # Only one must be specified antiAffinity : requiredDuringSchedulingIgnoredDuringExecution : {} preferredDuringSchedulingIgnoredDuringExecution : weight : 1 # Between 1 - 100 # Canary update strategy canary : # Reference to a service which the controller will update to select # canary pods. Required for traffic routing. canaryService : canary-service # Reference to a service which the controller will update to select # stable pods. Required for traffic routing. stableService : stable-service # Metadata which will be attached to the canary pods. This metadata will # only exist during an update, since there are no canary pods in a fully # promoted rollout. canaryMetadata : annotations : role : canary labels : role : canary # metadata which will be attached to the stable pods stableMetadata : annotations : role : stable labels : role : stable # The maximum number of pods that can be unavailable during the update. # Value can be an absolute number (ex: 5) or a percentage of total pods # at the start of update (ex: 10%). Absolute number is calculated from # percentage by rounding down. This can not be 0 if MaxSurge is 0. By # default, a fixed value of 1 is used. Example: when this is set to 30%, # the old RC can be scaled down by 30% immediately when the rolling # update starts. Once new pods are ready, old RC can be scaled down # further, followed by scaling up the new RC, ensuring that at least 70% # of original number of pods are available at all times during the # update. +optional maxUnavailable : 1 # The maximum number of pods that can be scheduled above the original # number of pods. Value can be an absolute number (ex: 5) or a # percentage of total pods at the start of the update (ex: 10%). This # can not be 0 if MaxUnavailable is 0. Absolute number is calculated # from percentage by rounding up. By default, a value of 1 is used. # Example: when this is set to 30%, the new RC can be scaled up by 30% # immediately when the rolling update starts. Once old pods have been # killed, new RC can be scaled up further, ensuring that total number # of pods running at any time during the update is at most 130% of # original pods. +optional maxSurge : \"20%\" # Adds a delay before scaling down the previous ReplicaSet when the # canary strategy is used with traffic routing (default 30 seconds). # A delay in scaling down the previous ReplicaSet is needed after # switching the stable service selector to point to the new ReplicaSet, # in order to give time for traffic providers to re-target the new pods. # This value is ignored with basic, replica-weighted canary without # traffic routing. ScaleDownDelaySeconds : 30 # Limits the number of old RS that can run at one time before getting # scaled down. Defaults to nil ScaleDownDelayRevisionLimit : 2 # Background analysis to run during a rollout update. Skipped upon # initial deploy of a rollout. +optional analysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local # valueFrom.podTemplateHashValue is a convenience to supply the # rollouts-pod-template-hash value of either the Stable ReplicaSet # or the Latest ReplicaSet - name : stable-hash valueFrom : podTemplateHashValue : Stable - name : latest-hash valueFrom : podTemplateHashValue : Latest # valueFrom.fieldRef allows metadata about the rollout to be # supplied as arguments to analysis. - name : region valueFrom : fieldRef : fieldPath : metadata.labels['region'] # Steps define sequence of steps to take during an update of the # canary. Skipped upon initial deploy of a rollout. +optional steps : # Sets the ratio of canary ReplicaSet to 20% - setWeight : 20 # Pauses the rollout for an hour. Supported units: s, m, h - pause : duration : 1h # Pauses indefinitely until manually resumed - pause : {} # set canary scale to a explicit count without changing traffic weight # (supported only with trafficRouting) - setCanaryScale : replicas : 3 # set canary scale to a percentage of spec.replicas without changing traffic weight # (supported only with trafficRouting) - setCanaryScale : weight : 25 # set canary scale to match the canary traffic weight (default behavior) - setCanaryScale : matchTrafficWeight : true # an inline analysis step - analysis : templates : - templateName : success-rate # an inline experiment step - experiment : duration : 1h templates : - name : baseline specRef : stable - name : canary specRef : canary analyses : - name : mann-whitney templateName : mann-whitney # Anti-affinity configuration between desired and previous ReplicaSet. # Only one must be specified. antiAffinity : requiredDuringSchedulingIgnoredDuringExecution : {} preferredDuringSchedulingIgnoredDuringExecution : weight : 1 # Between 1 - 100 # Traffic routing specifies the ingress controller or service mesh # configuration to achieve advanced traffic splitting. If omitted, # will achieve traffic split via a weighted replica counts between # the canary and stable ReplicaSet. trafficRouting : # Istio traffic routing configuration istio : # Either virtualService or virtualServices can be configured. virtualService : name : rollout-vsvc # required routes : - primary # optional if there is a single route in VirtualService, required otherwise virtualServices : # One or more virtualServices can be configured - name : rollouts-vsvc1 # required routes : - primary # optional if there is a single route in VirtualService, required otherwise - name : rollouts-vsvc2 # required routes : - secondary # optional if there is a single route in VirtualService, required otherwise # NGINX Ingress Controller routing configuration nginx : stableIngress : primary-ingress # required annotationPrefix : customingress.nginx.ingress.kubernetes.io # optional additionalIngressAnnotations : # optional canary-by-header : X-Canary canary-by-header-value : iwantsit # ALB Ingress Controller routing configuration alb : ingress : ingress # required servicePort : 443 # required annotationPrefix : custom.alb.ingress.kubernetes.io # optional # Service Mesh Interface routing configuration smi : rootService : root-svc # optional trafficSplitName : rollout-example-traffic-split # optional # Add a delay in second before scaling down the canary pods when update # is aborted for canary strategy with traffic routing (not applicable for basic canary). # 0 means canary pods are not scaled down. Default is 30 seconds. abortScaleDownDelaySeconds : 30 status : pauseConditions : - reason : StepPause startTime : 2019-10-00T1234 - reason : BlueGreenPause startTime : 2019-10-00T1234 - reason : AnalysisRunInconclusive startTime : 2019-10-00T1234 Examples \u00b6 You can find examples of Rollouts at: The example directory The Argo Rollouts Demo application","title":"Rollout Spec"},{"location":"features/specification/#rollout-specification","text":"The following describes all the available fields of a rollout: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : example-rollout-canary spec : # Number of desired pods. # Defaults to 1. replicas : 5 analysis : # limits the number of successful analysis runs and experiments to be stored in a history # Defaults to 5. successfulRunHistoryLimit : 10 # limits the number of unsuccessful analysis runs and experiments to be stored in a history. # Stages for unsuccessful: \"Error\", \"Failed\", \"Inconclusive\" # Defaults to 5. unsuccessfulRunHistoryLimit : 10 # Label selector for pods. Existing ReplicaSets whose pods are selected by # this will be the ones affected by this rollout. It must match the pod # template's labels. selector : matchLabels : app : guestbook # Template describes the pods that will be created. Same as deployment template : spec : containers : - name : guestbook image : argoproj/rollouts-demo:blue # Minimum number of seconds for which a newly created pod should be ready # without any of its container crashing, for it to be considered available. # Defaults to 0 (pod will be considered available as soon as it is ready) minReadySeconds : 30 # The number of old ReplicaSets to retain. # Defaults to 10 revisionHistoryLimit : 3 # Pause allows a user to manually pause a rollout at any time. A rollout # will not advance through its steps while it is manually paused, but HPA # auto-scaling will still occur. Typically not explicitly set the manifest, # but controlled via tools (e.g. kubectl argo rollouts pause). If true at # initial creation of Rollout, replicas are not scaled up automatically # from zero unless manually promoted. paused : true # The maximum time in seconds in which a rollout must make progress during # an update, before it is considered to be failed. Argo Rollouts will # continue to process failed rollouts and a condition with a # ProgressDeadlineExceeded reason will be surfaced in the rollout status. # Note that progress will not be estimated during the time a rollout is # paused. # Defaults to 600s progressDeadlineSeconds : 600 # Whether to abort the update when ProgressDeadlineSeconds # is exceeded if analysis or experiment is not used. # Optional and default is false. progressDeadlineAbort : false # UTC timestamp in which a Rollout should sequentially restart all of # its pods. Used by the `kubectl argo rollouts restart ROLLOUT` command. # The controller will ensure all pods have a creationTimestamp greater # than or equal to this value. restartAt : \"2020-03-30T21:19:35Z\" strategy : # Blue-green update strategy blueGreen : # Reference to service that the rollout modifies as the active service. # Required. activeService : active-service # Pre-promotion analysis run which performs analysis before the service # cutover. +optional prePromotionAnalysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local # Post-promotion analysis run which performs analysis after the service # cutover. +optional postPromotionAnalysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local # Name of the service that the rollout modifies as the preview service. # +optional previewService : preview-service # The number of replicas to run under the preview service before the # switchover. Once the rollout is resumed the new ReplicaSet will be fully # scaled up before the switch occurs +optional previewReplicaCount : 1 # Indicates if the rollout should automatically promote the new ReplicaSet # to the active service or enter a paused state. If not specified, the # default value is true. +optional autoPromotionEnabled : false # Automatically promotes the current ReplicaSet to active after the # specified pause delay in seconds after the ReplicaSet becomes ready. # If omitted, the Rollout enters and remains in a paused state until # manually resumed by resetting spec.Paused to false. +optional autoPromotionSeconds : 30 # Adds a delay before scaling down the previous ReplicaSet. If omitted, # the Rollout waits 30 seconds before scaling down the previous ReplicaSet. # A minimum of 30 seconds is recommended to ensure IP table propagation # across the nodes in a cluster. scaleDownDelaySeconds : 30 # Limits the number of old RS that can run at once before getting scaled # down. Defaults to nil scaleDownDelayRevisionLimit : 2 # Add a delay in second before scaling down the preview replicaset # if update is aborted. 0 means not to scale down. Default is 30 second abortScaleDownDelaySeconds : 30 # Anti Affinity configuration between desired and previous ReplicaSet. # Only one must be specified antiAffinity : requiredDuringSchedulingIgnoredDuringExecution : {} preferredDuringSchedulingIgnoredDuringExecution : weight : 1 # Between 1 - 100 # Canary update strategy canary : # Reference to a service which the controller will update to select # canary pods. Required for traffic routing. canaryService : canary-service # Reference to a service which the controller will update to select # stable pods. Required for traffic routing. stableService : stable-service # Metadata which will be attached to the canary pods. This metadata will # only exist during an update, since there are no canary pods in a fully # promoted rollout. canaryMetadata : annotations : role : canary labels : role : canary # metadata which will be attached to the stable pods stableMetadata : annotations : role : stable labels : role : stable # The maximum number of pods that can be unavailable during the update. # Value can be an absolute number (ex: 5) or a percentage of total pods # at the start of update (ex: 10%). Absolute number is calculated from # percentage by rounding down. This can not be 0 if MaxSurge is 0. By # default, a fixed value of 1 is used. Example: when this is set to 30%, # the old RC can be scaled down by 30% immediately when the rolling # update starts. Once new pods are ready, old RC can be scaled down # further, followed by scaling up the new RC, ensuring that at least 70% # of original number of pods are available at all times during the # update. +optional maxUnavailable : 1 # The maximum number of pods that can be scheduled above the original # number of pods. Value can be an absolute number (ex: 5) or a # percentage of total pods at the start of the update (ex: 10%). This # can not be 0 if MaxUnavailable is 0. Absolute number is calculated # from percentage by rounding up. By default, a value of 1 is used. # Example: when this is set to 30%, the new RC can be scaled up by 30% # immediately when the rolling update starts. Once old pods have been # killed, new RC can be scaled up further, ensuring that total number # of pods running at any time during the update is at most 130% of # original pods. +optional maxSurge : \"20%\" # Adds a delay before scaling down the previous ReplicaSet when the # canary strategy is used with traffic routing (default 30 seconds). # A delay in scaling down the previous ReplicaSet is needed after # switching the stable service selector to point to the new ReplicaSet, # in order to give time for traffic providers to re-target the new pods. # This value is ignored with basic, replica-weighted canary without # traffic routing. ScaleDownDelaySeconds : 30 # Limits the number of old RS that can run at one time before getting # scaled down. Defaults to nil ScaleDownDelayRevisionLimit : 2 # Background analysis to run during a rollout update. Skipped upon # initial deploy of a rollout. +optional analysis : templates : - templateName : success-rate args : - name : service-name value : guestbook-svc.default.svc.cluster.local # valueFrom.podTemplateHashValue is a convenience to supply the # rollouts-pod-template-hash value of either the Stable ReplicaSet # or the Latest ReplicaSet - name : stable-hash valueFrom : podTemplateHashValue : Stable - name : latest-hash valueFrom : podTemplateHashValue : Latest # valueFrom.fieldRef allows metadata about the rollout to be # supplied as arguments to analysis. - name : region valueFrom : fieldRef : fieldPath : metadata.labels['region'] # Steps define sequence of steps to take during an update of the # canary. Skipped upon initial deploy of a rollout. +optional steps : # Sets the ratio of canary ReplicaSet to 20% - setWeight : 20 # Pauses the rollout for an hour. Supported units: s, m, h - pause : duration : 1h # Pauses indefinitely until manually resumed - pause : {} # set canary scale to a explicit count without changing traffic weight # (supported only with trafficRouting) - setCanaryScale : replicas : 3 # set canary scale to a percentage of spec.replicas without changing traffic weight # (supported only with trafficRouting) - setCanaryScale : weight : 25 # set canary scale to match the canary traffic weight (default behavior) - setCanaryScale : matchTrafficWeight : true # an inline analysis step - analysis : templates : - templateName : success-rate # an inline experiment step - experiment : duration : 1h templates : - name : baseline specRef : stable - name : canary specRef : canary analyses : - name : mann-whitney templateName : mann-whitney # Anti-affinity configuration between desired and previous ReplicaSet. # Only one must be specified. antiAffinity : requiredDuringSchedulingIgnoredDuringExecution : {} preferredDuringSchedulingIgnoredDuringExecution : weight : 1 # Between 1 - 100 # Traffic routing specifies the ingress controller or service mesh # configuration to achieve advanced traffic splitting. If omitted, # will achieve traffic split via a weighted replica counts between # the canary and stable ReplicaSet. trafficRouting : # Istio traffic routing configuration istio : # Either virtualService or virtualServices can be configured. virtualService : name : rollout-vsvc # required routes : - primary # optional if there is a single route in VirtualService, required otherwise virtualServices : # One or more virtualServices can be configured - name : rollouts-vsvc1 # required routes : - primary # optional if there is a single route in VirtualService, required otherwise - name : rollouts-vsvc2 # required routes : - secondary # optional if there is a single route in VirtualService, required otherwise # NGINX Ingress Controller routing configuration nginx : stableIngress : primary-ingress # required annotationPrefix : customingress.nginx.ingress.kubernetes.io # optional additionalIngressAnnotations : # optional canary-by-header : X-Canary canary-by-header-value : iwantsit # ALB Ingress Controller routing configuration alb : ingress : ingress # required servicePort : 443 # required annotationPrefix : custom.alb.ingress.kubernetes.io # optional # Service Mesh Interface routing configuration smi : rootService : root-svc # optional trafficSplitName : rollout-example-traffic-split # optional # Add a delay in second before scaling down the canary pods when update # is aborted for canary strategy with traffic routing (not applicable for basic canary). # 0 means canary pods are not scaled down. Default is 30 seconds. abortScaleDownDelaySeconds : 30 status : pauseConditions : - reason : StepPause startTime : 2019-10-00T1234 - reason : BlueGreenPause startTime : 2019-10-00T1234 - reason : AnalysisRunInconclusive startTime : 2019-10-00T1234","title":"Rollout Specification"},{"location":"features/specification/#examples","text":"You can find examples of Rollouts at: The example directory The Argo Rollouts Demo application","title":"Examples"},{"location":"features/anti-affinity/anti-affinity/","text":"Anti Affinity \u00b6 Background \u00b6 Depending on a cluster's configuration, a Blue Green Rollout (or a Canary rollout that uses traffic management) can cause newly created pods to restart after deploying a new version. This can be problematic, especially for applications that cannot startup quickly or do not gracefully exit. This behavior occurs because cluster auto-scaler wants to scale down the extra capacity which was created to support a Rollout running in double capacity. When a node is scaled down, the pods it owns are deleted and recreated. This usually happens if a Rollout has its own dedicated instance group since a Rollout has a greater effect on cluster auto-scaling. Therefore, clusters with a large pool of shared nodes experience the behavior less often. For example, here is a Rollout is running with 8 pods spread across 2 nodes. Each node can hold 6 pods: When the spec.template of the Rollout changes, the controller creates a new ReplicaSet with the spec update and the total number of pods doubles. In this case, the number of pods increases to 16. Since each node can only hold 6 pods, the cluster autoscaler must increase the node count to 3 to accommodate all 16 pods. The resulting distribution of pods across nodes is shown here: Once the Rollout finishes progressing, the old version is scaled down. This leaves the cluster with more nodes than necessary, thus wasting resources (as shown below). The cluster auto-scaler terminates the extra node and the pods are rescheduled on the remaining 2 nodes. To reduce the chance of this behavior, a rollout can inject anti-affinity into the ReplicaSet. This prevents new pods from running on nodes which have the previous version's pods. You can learn more about anti-affinity here . Repeating the above example with anti-affinity enabled, here is what happens when the .spec.template of the Rollout changes. Due to anti-affinity, the new pods cannot be scheduled on nodes which run the old ReplicaSet's pods. As a result, the cluster auto-scaler must create 2 nodes to host the new ReplicaSet's pods. In this case, pods won't be started since the scaled-down nodes are guaranteed to not have the new pods. Enabling Anti-Affinity in Rollouts \u00b6 Anti-affinity is enabled by adding the anti-affinity struct to the Blue-Green or Canary strategy. When the anti-affinity struct is set, controller injects a PodAntiAffinity struct into the ReplicaSet's Affinity. This feature will not modify any of the ReplicaSet's pre-existing affinity rules. Users have a choice between these scheduling rules: RequiredDuringSchedulingIgnoredDuringExecution and PreferredDuringSchedulingIgnoredDuringExecution . RequiredDuringSchedulingIgnoredDuringExecution requires a new version's pods to be on a separate node than the previous versions. If this is not possible, the the new version's pods will not be scheduled. strategy : bluegreen : antiAffinity : requiredDuringSchedulingIgnoredDuringExecution : {} Unlike the Required strategy, PreferredDuringSchedulingIgnoredDuringExecution does not force a new version's pods to be on a separate node than the previous versions. The scheduler attempts to place the new version's pods on separate node(s). If that's not possible, the new version's pods will still be scheduled. The Weight is used to create a priority order for preferred anti-affinity rules. strategy : canary : antiAffinity : preferredDuringSchedulingIgnoredDuringExecution : weight : 1 # Between 1 - 100 Important The main downside to this approach is that deployments can take longer because new nodes are more likely to be created in order to schedule pods with respect to anti-affinity rules. This delay most frequently occurs when a rollout has its own dedicated instance group, since new nodes are more likely to be created to honor anti-affinity rules.","title":"Anti Affinity"},{"location":"features/anti-affinity/anti-affinity/#anti-affinity","text":"","title":"Anti Affinity"},{"location":"features/anti-affinity/anti-affinity/#background","text":"Depending on a cluster's configuration, a Blue Green Rollout (or a Canary rollout that uses traffic management) can cause newly created pods to restart after deploying a new version. This can be problematic, especially for applications that cannot startup quickly or do not gracefully exit. This behavior occurs because cluster auto-scaler wants to scale down the extra capacity which was created to support a Rollout running in double capacity. When a node is scaled down, the pods it owns are deleted and recreated. This usually happens if a Rollout has its own dedicated instance group since a Rollout has a greater effect on cluster auto-scaling. Therefore, clusters with a large pool of shared nodes experience the behavior less often. For example, here is a Rollout is running with 8 pods spread across 2 nodes. Each node can hold 6 pods: When the spec.template of the Rollout changes, the controller creates a new ReplicaSet with the spec update and the total number of pods doubles. In this case, the number of pods increases to 16. Since each node can only hold 6 pods, the cluster autoscaler must increase the node count to 3 to accommodate all 16 pods. The resulting distribution of pods across nodes is shown here: Once the Rollout finishes progressing, the old version is scaled down. This leaves the cluster with more nodes than necessary, thus wasting resources (as shown below). The cluster auto-scaler terminates the extra node and the pods are rescheduled on the remaining 2 nodes. To reduce the chance of this behavior, a rollout can inject anti-affinity into the ReplicaSet. This prevents new pods from running on nodes which have the previous version's pods. You can learn more about anti-affinity here . Repeating the above example with anti-affinity enabled, here is what happens when the .spec.template of the Rollout changes. Due to anti-affinity, the new pods cannot be scheduled on nodes which run the old ReplicaSet's pods. As a result, the cluster auto-scaler must create 2 nodes to host the new ReplicaSet's pods. In this case, pods won't be started since the scaled-down nodes are guaranteed to not have the new pods.","title":"Background"},{"location":"features/anti-affinity/anti-affinity/#enabling-anti-affinity-in-rollouts","text":"Anti-affinity is enabled by adding the anti-affinity struct to the Blue-Green or Canary strategy. When the anti-affinity struct is set, controller injects a PodAntiAffinity struct into the ReplicaSet's Affinity. This feature will not modify any of the ReplicaSet's pre-existing affinity rules. Users have a choice between these scheduling rules: RequiredDuringSchedulingIgnoredDuringExecution and PreferredDuringSchedulingIgnoredDuringExecution . RequiredDuringSchedulingIgnoredDuringExecution requires a new version's pods to be on a separate node than the previous versions. If this is not possible, the the new version's pods will not be scheduled. strategy : bluegreen : antiAffinity : requiredDuringSchedulingIgnoredDuringExecution : {} Unlike the Required strategy, PreferredDuringSchedulingIgnoredDuringExecution does not force a new version's pods to be on a separate node than the previous versions. The scheduler attempts to place the new version's pods on separate node(s). If that's not possible, the new version's pods will still be scheduled. The Weight is used to create a priority order for preferred anti-affinity rules. strategy : canary : antiAffinity : preferredDuringSchedulingIgnoredDuringExecution : weight : 1 # Between 1 - 100 Important The main downside to this approach is that deployments can take longer because new nodes are more likely to be created in order to schedule pods with respect to anti-affinity rules. This delay most frequently occurs when a rollout has its own dedicated instance group, since new nodes are more likely to be created to honor anti-affinity rules.","title":"Enabling Anti-Affinity in Rollouts"},{"location":"features/traffic-management/","text":"Traffic management \u00b6 Traffic management is controlling the data plane to have intelligent routing rules for an application. These routing rules can manipulate the flow of traffic to different versions of an application enabling Progressive Delivery. These controls limit the blast radius of a new release by ensuring a small percentage of users receive a new version while it is verified. There are various techniques to achieve traffic management: Raw percentages (i.e., 5% of traffic should go to the new version while the rest goes to the stable version) Header-based routing (i.e., send requests with a specific header to the new version) Mirrored traffic where all the traffic is copied and send to the new version in parallel (but the response is ignored) Traffic Management tools in Kubernetes \u00b6 The core Kubernetes objects do not have fine-grained tools needed to fulfill all the requirements of traffic management. At most, Kubernetes offers native load balancing capabilities through the Service object by offering an endpoint that routes traffic to a grouping of pods based on that Service's selector. Functionality like traffic mirroring or routing by headers is not possible with the default core Service object, and the only way to control the percentage of traffic to different versions of an application is by manipulating replica counts of those versions. Service Meshes fill this missing functionality in Kubernetes. They introduce new concepts and functionality to control the data plane through the use of CRDs and other core Kubernetes resources. How does Argo Rollouts enable traffic management? \u00b6 Argo Rollouts enables traffic management by manipulating the Service Mesh resources to match the intent of the Rollout. Argo Rollouts currently supports the following service meshes: AWS ALB Ingress Controller Ambassador Edge Stack Istio Nginx Ingress Controller Service Mesh Interface (SMI) Multiple Providers File a ticket here if you would like another implementation (or thumbs up it if that issue already exists) Regardless of the Service Mesh used, the Rollout object has to set a canary Service and a stable Service in its spec. Here is an example with those fields set: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service stableService : stable-service trafficRouting : ... The controller modifies these Services to route traffic to the appropriate canary and stable ReplicaSets as the Rollout progresses. These Services are used by the Service Mesh to define what group of pods should receive the canary and stable traffic. Additionally, the Argo Rollouts controller needs to treat the Rollout object differently when using traffic management. In particular, the Stable ReplicaSet owned by the Rollout remains fully scaled up as the Rollout progresses through the Canary steps. Since the traffic is controlled independently by the Service Mesh resources, the controller needs to make a best effort to ensure that the Stable and New ReplicaSets are not overwhelmed by the traffic sent to them. By leaving the Stable ReplicaSet scaled up, the controller is ensuring that the Stable ReplicaSet can handle 100% of the traffic at any time 1 . The New ReplicaSet follows the same behavior as without traffic management. The new ReplicaSet's replica count is equal to the latest SetWeight step percentage multiple by the total replica count of the Rollout. This calculation ensures that the canary version does not receive more traffic than it can handle. The Rollout has to assume that the application can handle 100% of traffic if it is fully scaled up. It should outsource to the HPA to detect if the Rollout needs to more replicas if 100% isn't enough. \u21a9","title":"Overview"},{"location":"features/traffic-management/#traffic-management","text":"Traffic management is controlling the data plane to have intelligent routing rules for an application. These routing rules can manipulate the flow of traffic to different versions of an application enabling Progressive Delivery. These controls limit the blast radius of a new release by ensuring a small percentage of users receive a new version while it is verified. There are various techniques to achieve traffic management: Raw percentages (i.e., 5% of traffic should go to the new version while the rest goes to the stable version) Header-based routing (i.e., send requests with a specific header to the new version) Mirrored traffic where all the traffic is copied and send to the new version in parallel (but the response is ignored)","title":"Traffic management"},{"location":"features/traffic-management/#traffic-management-tools-in-kubernetes","text":"The core Kubernetes objects do not have fine-grained tools needed to fulfill all the requirements of traffic management. At most, Kubernetes offers native load balancing capabilities through the Service object by offering an endpoint that routes traffic to a grouping of pods based on that Service's selector. Functionality like traffic mirroring or routing by headers is not possible with the default core Service object, and the only way to control the percentage of traffic to different versions of an application is by manipulating replica counts of those versions. Service Meshes fill this missing functionality in Kubernetes. They introduce new concepts and functionality to control the data plane through the use of CRDs and other core Kubernetes resources.","title":"Traffic Management tools in Kubernetes"},{"location":"features/traffic-management/#how-does-argo-rollouts-enable-traffic-management","text":"Argo Rollouts enables traffic management by manipulating the Service Mesh resources to match the intent of the Rollout. Argo Rollouts currently supports the following service meshes: AWS ALB Ingress Controller Ambassador Edge Stack Istio Nginx Ingress Controller Service Mesh Interface (SMI) Multiple Providers File a ticket here if you would like another implementation (or thumbs up it if that issue already exists) Regardless of the Service Mesh used, the Rollout object has to set a canary Service and a stable Service in its spec. Here is an example with those fields set: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service stableService : stable-service trafficRouting : ... The controller modifies these Services to route traffic to the appropriate canary and stable ReplicaSets as the Rollout progresses. These Services are used by the Service Mesh to define what group of pods should receive the canary and stable traffic. Additionally, the Argo Rollouts controller needs to treat the Rollout object differently when using traffic management. In particular, the Stable ReplicaSet owned by the Rollout remains fully scaled up as the Rollout progresses through the Canary steps. Since the traffic is controlled independently by the Service Mesh resources, the controller needs to make a best effort to ensure that the Stable and New ReplicaSets are not overwhelmed by the traffic sent to them. By leaving the Stable ReplicaSet scaled up, the controller is ensuring that the Stable ReplicaSet can handle 100% of the traffic at any time 1 . The New ReplicaSet follows the same behavior as without traffic management. The new ReplicaSet's replica count is equal to the latest SetWeight step percentage multiple by the total replica count of the Rollout. This calculation ensures that the canary version does not receive more traffic than it can handle. The Rollout has to assume that the application can handle 100% of traffic if it is fully scaled up. It should outsource to the HPA to detect if the Rollout needs to more replicas if 100% isn't enough. \u21a9","title":"How does Argo Rollouts enable traffic management?"},{"location":"features/traffic-management/alb/","text":"AWS Load Balancer Controller (ALB) \u00b6 Requirements \u00b6 AWS Load Balancer Controller v1.1.5 or greater Overview \u00b6 AWS Load Balancer Controller (also known as AWS ALB Ingress Controller) enables traffic management through an Ingress object, which configures an AWS Application Load Balancer (ALB) to route traffic to one or more Kubernetes services. ALBs provides advanced traffic splitting capability through the concept of weighted target groups . This feature is supported by the AWS Load Balancer Controller through annotations made to the Ingress object to configure \"actions.\" How it works \u00b6 ALBs are configured via Listeners, and Rules which contain Actions. Listeners define how traffic from a client comes in, and Rules define how to handle those requests with various Actions. One type of Action allows users to forward traffic to multiple TargetGroups (with each being defined as a Kubernetes service). You can read more about ALB concepts here . An Ingress which is managed by the AWS Load Balancer Controller, controls an ALB's Listener and Rules through the Ingress' annotations and spec. In order to split traffic among multiple target groups (e.g. different Kubernetes services), the AWS Load Balancer controller looks to a specific \"action\" annotation on the Ingress, alb.ingress.kubernetes.io/actions.<service-name> . This annotation is injected and updated automatically by a Rollout during an update according to the desired traffic weights. Usage \u00b6 To configure a Rollout to use the ALB integration and split traffic between the canary and stable services during updates, the Rollout should be configured with the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout ... spec : strategy : canary : # canaryService and stableService are references to Services which the Rollout will modify # to target the canary ReplicaSet and stable ReplicaSet respectively (required). canaryService : canary-service stableService : stable-service trafficRouting : alb : # The referenced ingress will be injected with a custom action annotation, directing # the AWS Load Balancer Controller to split traffic between the canary and stable # Service, according to the desired traffic weight (required). ingress : ingress # Reference to a Service that the Ingress must target in one of the rules (optional). # If omitted, uses canary.stableService. rootService : root-service # Service port is the port which the Service listens on (required). servicePort : 443 The referenced Ingress should be deployed with an ingress rule that matches the Rollout service: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : ingress annotations : kubernetes.io/ingress.class : alb spec : rules : - http : paths : - path : /* backend : # serviceName must match either: canary.trafficRouting.alb.rootService (if specified), # or canary.stableService (if rootService is omitted) serviceName : root-service # servicePort must be the value: use-annotation # This instructs AWS Load Balancer Controller to look to annotations on how to direct traffic servicePort : use-annotation During an update, the rollout controller injects the alb.ingress.kubernetes.io/actions.<SERVICE-NAME> annotation, containing a JSON payload understood by the AWS Load Balancer Controller, directing it to split traffic between the canaryService and stableService according to the current canary weight. The following is an example of our example Ingress after the rollout has injected the custom action annotation that splits traffic between the canary-service and stable-service, with a traffic weight of 10 and 90 respectively: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : ingress annotations : kubernetes.io/ingress.class : alb alb.ingress.kubernetes.io/actions.root-service : | { \"Type\":\"forward\", \"ForwardConfig\":{ \"TargetGroups\":[ { \"Weight\":10, \"ServiceName\":\"canary-service\", \"ServicePort\":\"80\" }, { \"Weight\":90, \"ServiceName\":\"stable-service\", \"ServicePort\":\"80\" } ] } } spec : rules : - http : paths : - path : /* backend : serviceName : root-service servicePort : use-annotation Note Argo rollouts additionally injects an annotation, rollouts.argoproj.io/managed-alb-actions , to the Ingress for bookkeeping purposes. The annotation indicates which actions are being managed by the Rollout object (since multiple Rollouts can reference one Ingress). Upon a rollout deletion, the rollout controller looks to this annotation to understand that this action is no longer managed, and is reset to point only the stable service with 100 weight. rootService \u00b6 By default, a rollout will inject the alb.ingress.kubernetes.io/actions.<SERVICE-NAME> annotation using the service/action name specified under spec.strategy.canary.stableService . However, it may be desirable to specify an explicit service/action name different from the stableService . For example, one pattern is to use a single Ingress containing three different rules to reach the canary, stable, and root service separately (e.g. for testing purposes). In this case, you may want to specify a \"root\" service as the service/action name instead of stable. To do so, reference a service under rootService under the alb specification: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : strategy : canary : canaryService : guestbook-canary stableService : guestbook-stable trafficRouting : alb : rootService : guestbook-root ... Sticky session \u00b6 Because at least two target groups (canary and stable) are used, target group stickiness requires additional configuration: Sticky session must be activated on the target group via apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : strategy : canary : ... trafficRouting : alb : stickinessConfig : enabled : true durationSeconds : 3600 ... More information can be found in the AWS ALB API Zero-Downtime Updates with AWS TargetGroup Verification \u00b6 Argo Rollouts contains two features to help ensure zero-downtime updates when used with the AWS LoadBalancer controller: TargetGroup IP verification and TargetGroup weight verification. Both features involve the Rollout controller performing additional safety checks to AWS, to verify the changes made to the Ingress object are reflected in the underlying AWS TargetGroup. TargetGroup IP Verification \u00b6 Note Target Group IP verification available since Argo Rollouts v1.1 The AWS LoadBalancer controller can run in one of two modes: Instance mode IP mode TargetGroup IP Verification is only applicable when the AWS LoadBalancer controller in IP mode. When using the AWS LoadBalancer controller in IP mode (e.g. using the AWS CNI), the ALB LoadBalancer targets individual Pod IPs, as opposed to K8s node instances. Targeting Pod IPs comes with an increased risk of downtime during an update, because the Pod IPs behind the underlying AWS TargetGroup can more easily become outdated from the actual availability and status of pods, causing HTTP 502 errors when the TargetGroup points to pods which have already been scaled down. To mitigate this risk, AWS recommends the use of pod readiness gate injection when running the AWS LoadBalancer in IP mode. Readiness gates allow for the AWS LoadBalancer controller to verify that TargetGroups are accurate before marking newly created Pods as \"ready\", preventing premature scale down of the older ReplicaSet. Pod readiness gate injection uses a mutating webhook which decides to inject readiness gates when a pod is created based on the following conditions: There exists a service matching the pod labels in the same namespace There exists at least one target group binding that refers to the matching service Another way to describe this is: the AWS LoadBalancer controller injects readiness gates onto Pods only if they are \"reachable\" from an ALB Ingress at the time of pod creation. A pod is considered reachable if an (ALB) Ingress references a Service which matches the pod labels. It ignores all other Pods. One challenge with this manner of pod readiness gate injection, is that modifications to the Service selector labels ( spec.selector ) do not allow for the AWS LoadBalancer controller to inject the readiness gates, because by that time the Pod was already created (and readiness gates are immutable). Note that this is an issue when you change Service selectors of any ALB Service, not just ones involved in Argo Rollouts. Because Argo Rollout's blue-green strategy works by modifying the activeService selector to the new ReplicaSet labels during promotion, it suffers from the issue where readiness gates for the spec.strategy.blueGreen.activeService fail to be injected. This means there is a possibility of downtime in the following problematic scenario during an update from V1 to V2: Update is triggered and V2 ReplicaSet stack is scaled up V2 ReplicaSet pods become fully available and ready to be promoted Rollout promotes V2 by updating the label selectors of the active service to point to the V2 stack (from V1) Due to unknown issues (e.g. AWS load balancer controller downtime, AWS rate limiting), registration of the V2 Pod IPs to the TargetGroup does not happen or is delayed. V1 ReplicaSet is scaled down to complete the update After step 5, when the V1 ReplicaSet is scaled down, the outdated TargetGroup would still be pointing to the V1 Pods IPs which no longer exist, causing downtime. To allow for zero-downtime updates, Argo Rollouts has the ability to perform TargetGroup IP verification as an additional safety measure during an update. When this feature is enabled, whenever a service selector modification is made, the Rollout controller blocks progression of the update until it can verify the TargetGroup is accurately targeting the new Pod IPs of the bluegreen.activeService . Verification is achieved by querying AWS APIs to describe the underlying TargetGroup, iterating its registered IPs, and ensuring all Pod IPs of the activeService's Endpoints list are registered in the TargetGroup. Verification must succeed before running postPromotionAnalysis or scaling down the old ReplicaSet. Similarly for the canary strategy, after updating the canary.stableService selector labels to point to the new ReplicaSet, the TargetGroup IP verification feature allows the controller to block the scale down of the old ReplicaSet until it verifies the Pods IP behind the stableService TargetGroup are accurate. TargetGroup Weight Verification \u00b6 Note TargetGroup weight verification available since Argo Rollouts v1.0 TargetGroup weight verification addresses a similar problem to TargetGroup IP verification, but instead of verifying that the Pod IPs of a service are reflected accurately in the TargetGroup, the controller verifies that the traffic weights are accurate from what was set in the ingress annotations. Weight verification is applicable to AWS LoadBalancer controllers which are running either in IP mode or Instance mode. After Argo Rollouts adjusts a canary weight by updating the Ingress annotation, it moves on to the next step. However, due to external factors (e.g. AWS rate limiting, AWS load balancer controller downtime) it is possible that the weight modifications made to the Ingress, did not take effect in the underlying TargetGroup. This is potentially dangerous as the controller will believe it is safe to scale down the old stable stack when in reality, the outdated TargetGroup may still be pointing to it. Using the TargetGroup weight verification feature, the rollout controller will additionally verify the canary weight after a setWeight canary step. It accomplishes this by querying AWS LoadBalancer APIs directly, to confirm that the Rules, Actions, and TargetGroups reflect the desire of Ingress annotation. Usage \u00b6 To enable AWS target group verification, add --aws-verify-target-group flag to the rollout-controller flags: apiVersion : apps/v1 kind : Deployment metadata : name : argo-rollouts spec : template : spec : containers : - name : argo-rollouts args : [ --aws-verify-target-group ] # NOTE: in v1.0, the --alb-verify-weight flag should be used instead For this feature to work, the argo-rollouts deployment requires the following AWS API permissions under the Elastic Load Balancing API : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"elasticloadbalancing:DescribeTargetGroups\" , \"elasticloadbalancing:DescribeLoadBalancers\" , \"elasticloadbalancing:DescribeListeners\" , \"elasticloadbalancing:DescribeRules\" , \"elasticloadbalancing:DescribeTags\" , \"elasticloadbalancing:DescribeTargetHealth\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] } There are various ways of granting AWS privileges to the argo-rollouts pods, which is highly dependent to your cluster's AWS environment, and out-of-scope of this documentation. Some solutions include: AWS access and secret keys kiam kube2iam EKS ServiceAccount IAM Roles Custom annotations-prefix \u00b6 The AWS Load Balancer Controller allows users to customize the annotation prefix used by the Ingress controller using a flag to the controller, --annotations-prefix (from the default of alb.ingress.kubernetes.io ). If your AWS Load Balancer Controller is customized to use a different annotation prefix, annotationPrefix field should be specified such that the Ingress object will be annotated in a manner understood by the cluster's aws load balancer controller. apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : strategy : canary : trafficRouting : alb : annotationPrefix : custom.alb.ingress.kubernetes.io Custom kubernetes.io/ingress.class \u00b6 By default, Argo Rollout will operate on Ingresses with the annotation: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : alb To configure the controller to operate on Ingresses with different kubernetes.io/ingress.class values, the controller can specify a different value through the --alb-ingress-classes flag in the controller command line arguments. Note that the --alb-ingress-classes flag can be specified multiple times if the Argo Rollouts controller should operate on multiple values. This may be desired when a cluster has multiple Ingress controllers that operate on different kubernetes.io/ingress.class values. If the controller needs to operate on any Ingress without the kubernetes.io/ingress.class annotation, the flag can be specified with an empty string (e.g. --alb-ingress-classes '' ).","title":"AWS ALB"},{"location":"features/traffic-management/alb/#aws-load-balancer-controller-alb","text":"","title":"AWS Load Balancer Controller (ALB)"},{"location":"features/traffic-management/alb/#requirements","text":"AWS Load Balancer Controller v1.1.5 or greater","title":"Requirements"},{"location":"features/traffic-management/alb/#overview","text":"AWS Load Balancer Controller (also known as AWS ALB Ingress Controller) enables traffic management through an Ingress object, which configures an AWS Application Load Balancer (ALB) to route traffic to one or more Kubernetes services. ALBs provides advanced traffic splitting capability through the concept of weighted target groups . This feature is supported by the AWS Load Balancer Controller through annotations made to the Ingress object to configure \"actions.\"","title":"Overview"},{"location":"features/traffic-management/alb/#how-it-works","text":"ALBs are configured via Listeners, and Rules which contain Actions. Listeners define how traffic from a client comes in, and Rules define how to handle those requests with various Actions. One type of Action allows users to forward traffic to multiple TargetGroups (with each being defined as a Kubernetes service). You can read more about ALB concepts here . An Ingress which is managed by the AWS Load Balancer Controller, controls an ALB's Listener and Rules through the Ingress' annotations and spec. In order to split traffic among multiple target groups (e.g. different Kubernetes services), the AWS Load Balancer controller looks to a specific \"action\" annotation on the Ingress, alb.ingress.kubernetes.io/actions.<service-name> . This annotation is injected and updated automatically by a Rollout during an update according to the desired traffic weights.","title":"How it works"},{"location":"features/traffic-management/alb/#usage","text":"To configure a Rollout to use the ALB integration and split traffic between the canary and stable services during updates, the Rollout should be configured with the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout ... spec : strategy : canary : # canaryService and stableService are references to Services which the Rollout will modify # to target the canary ReplicaSet and stable ReplicaSet respectively (required). canaryService : canary-service stableService : stable-service trafficRouting : alb : # The referenced ingress will be injected with a custom action annotation, directing # the AWS Load Balancer Controller to split traffic between the canary and stable # Service, according to the desired traffic weight (required). ingress : ingress # Reference to a Service that the Ingress must target in one of the rules (optional). # If omitted, uses canary.stableService. rootService : root-service # Service port is the port which the Service listens on (required). servicePort : 443 The referenced Ingress should be deployed with an ingress rule that matches the Rollout service: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : ingress annotations : kubernetes.io/ingress.class : alb spec : rules : - http : paths : - path : /* backend : # serviceName must match either: canary.trafficRouting.alb.rootService (if specified), # or canary.stableService (if rootService is omitted) serviceName : root-service # servicePort must be the value: use-annotation # This instructs AWS Load Balancer Controller to look to annotations on how to direct traffic servicePort : use-annotation During an update, the rollout controller injects the alb.ingress.kubernetes.io/actions.<SERVICE-NAME> annotation, containing a JSON payload understood by the AWS Load Balancer Controller, directing it to split traffic between the canaryService and stableService according to the current canary weight. The following is an example of our example Ingress after the rollout has injected the custom action annotation that splits traffic between the canary-service and stable-service, with a traffic weight of 10 and 90 respectively: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : ingress annotations : kubernetes.io/ingress.class : alb alb.ingress.kubernetes.io/actions.root-service : | { \"Type\":\"forward\", \"ForwardConfig\":{ \"TargetGroups\":[ { \"Weight\":10, \"ServiceName\":\"canary-service\", \"ServicePort\":\"80\" }, { \"Weight\":90, \"ServiceName\":\"stable-service\", \"ServicePort\":\"80\" } ] } } spec : rules : - http : paths : - path : /* backend : serviceName : root-service servicePort : use-annotation Note Argo rollouts additionally injects an annotation, rollouts.argoproj.io/managed-alb-actions , to the Ingress for bookkeeping purposes. The annotation indicates which actions are being managed by the Rollout object (since multiple Rollouts can reference one Ingress). Upon a rollout deletion, the rollout controller looks to this annotation to understand that this action is no longer managed, and is reset to point only the stable service with 100 weight.","title":"Usage"},{"location":"features/traffic-management/alb/#rootservice","text":"By default, a rollout will inject the alb.ingress.kubernetes.io/actions.<SERVICE-NAME> annotation using the service/action name specified under spec.strategy.canary.stableService . However, it may be desirable to specify an explicit service/action name different from the stableService . For example, one pattern is to use a single Ingress containing three different rules to reach the canary, stable, and root service separately (e.g. for testing purposes). In this case, you may want to specify a \"root\" service as the service/action name instead of stable. To do so, reference a service under rootService under the alb specification: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : strategy : canary : canaryService : guestbook-canary stableService : guestbook-stable trafficRouting : alb : rootService : guestbook-root ...","title":"rootService"},{"location":"features/traffic-management/alb/#sticky-session","text":"Because at least two target groups (canary and stable) are used, target group stickiness requires additional configuration: Sticky session must be activated on the target group via apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : strategy : canary : ... trafficRouting : alb : stickinessConfig : enabled : true durationSeconds : 3600 ... More information can be found in the AWS ALB API","title":"Sticky session"},{"location":"features/traffic-management/alb/#zero-downtime-updates-with-aws-targetgroup-verification","text":"Argo Rollouts contains two features to help ensure zero-downtime updates when used with the AWS LoadBalancer controller: TargetGroup IP verification and TargetGroup weight verification. Both features involve the Rollout controller performing additional safety checks to AWS, to verify the changes made to the Ingress object are reflected in the underlying AWS TargetGroup.","title":"Zero-Downtime Updates with AWS TargetGroup Verification"},{"location":"features/traffic-management/alb/#targetgroup-ip-verification","text":"Note Target Group IP verification available since Argo Rollouts v1.1 The AWS LoadBalancer controller can run in one of two modes: Instance mode IP mode TargetGroup IP Verification is only applicable when the AWS LoadBalancer controller in IP mode. When using the AWS LoadBalancer controller in IP mode (e.g. using the AWS CNI), the ALB LoadBalancer targets individual Pod IPs, as opposed to K8s node instances. Targeting Pod IPs comes with an increased risk of downtime during an update, because the Pod IPs behind the underlying AWS TargetGroup can more easily become outdated from the actual availability and status of pods, causing HTTP 502 errors when the TargetGroup points to pods which have already been scaled down. To mitigate this risk, AWS recommends the use of pod readiness gate injection when running the AWS LoadBalancer in IP mode. Readiness gates allow for the AWS LoadBalancer controller to verify that TargetGroups are accurate before marking newly created Pods as \"ready\", preventing premature scale down of the older ReplicaSet. Pod readiness gate injection uses a mutating webhook which decides to inject readiness gates when a pod is created based on the following conditions: There exists a service matching the pod labels in the same namespace There exists at least one target group binding that refers to the matching service Another way to describe this is: the AWS LoadBalancer controller injects readiness gates onto Pods only if they are \"reachable\" from an ALB Ingress at the time of pod creation. A pod is considered reachable if an (ALB) Ingress references a Service which matches the pod labels. It ignores all other Pods. One challenge with this manner of pod readiness gate injection, is that modifications to the Service selector labels ( spec.selector ) do not allow for the AWS LoadBalancer controller to inject the readiness gates, because by that time the Pod was already created (and readiness gates are immutable). Note that this is an issue when you change Service selectors of any ALB Service, not just ones involved in Argo Rollouts. Because Argo Rollout's blue-green strategy works by modifying the activeService selector to the new ReplicaSet labels during promotion, it suffers from the issue where readiness gates for the spec.strategy.blueGreen.activeService fail to be injected. This means there is a possibility of downtime in the following problematic scenario during an update from V1 to V2: Update is triggered and V2 ReplicaSet stack is scaled up V2 ReplicaSet pods become fully available and ready to be promoted Rollout promotes V2 by updating the label selectors of the active service to point to the V2 stack (from V1) Due to unknown issues (e.g. AWS load balancer controller downtime, AWS rate limiting), registration of the V2 Pod IPs to the TargetGroup does not happen or is delayed. V1 ReplicaSet is scaled down to complete the update After step 5, when the V1 ReplicaSet is scaled down, the outdated TargetGroup would still be pointing to the V1 Pods IPs which no longer exist, causing downtime. To allow for zero-downtime updates, Argo Rollouts has the ability to perform TargetGroup IP verification as an additional safety measure during an update. When this feature is enabled, whenever a service selector modification is made, the Rollout controller blocks progression of the update until it can verify the TargetGroup is accurately targeting the new Pod IPs of the bluegreen.activeService . Verification is achieved by querying AWS APIs to describe the underlying TargetGroup, iterating its registered IPs, and ensuring all Pod IPs of the activeService's Endpoints list are registered in the TargetGroup. Verification must succeed before running postPromotionAnalysis or scaling down the old ReplicaSet. Similarly for the canary strategy, after updating the canary.stableService selector labels to point to the new ReplicaSet, the TargetGroup IP verification feature allows the controller to block the scale down of the old ReplicaSet until it verifies the Pods IP behind the stableService TargetGroup are accurate.","title":"TargetGroup IP Verification"},{"location":"features/traffic-management/alb/#targetgroup-weight-verification","text":"Note TargetGroup weight verification available since Argo Rollouts v1.0 TargetGroup weight verification addresses a similar problem to TargetGroup IP verification, but instead of verifying that the Pod IPs of a service are reflected accurately in the TargetGroup, the controller verifies that the traffic weights are accurate from what was set in the ingress annotations. Weight verification is applicable to AWS LoadBalancer controllers which are running either in IP mode or Instance mode. After Argo Rollouts adjusts a canary weight by updating the Ingress annotation, it moves on to the next step. However, due to external factors (e.g. AWS rate limiting, AWS load balancer controller downtime) it is possible that the weight modifications made to the Ingress, did not take effect in the underlying TargetGroup. This is potentially dangerous as the controller will believe it is safe to scale down the old stable stack when in reality, the outdated TargetGroup may still be pointing to it. Using the TargetGroup weight verification feature, the rollout controller will additionally verify the canary weight after a setWeight canary step. It accomplishes this by querying AWS LoadBalancer APIs directly, to confirm that the Rules, Actions, and TargetGroups reflect the desire of Ingress annotation.","title":"TargetGroup Weight Verification"},{"location":"features/traffic-management/alb/#usage_1","text":"To enable AWS target group verification, add --aws-verify-target-group flag to the rollout-controller flags: apiVersion : apps/v1 kind : Deployment metadata : name : argo-rollouts spec : template : spec : containers : - name : argo-rollouts args : [ --aws-verify-target-group ] # NOTE: in v1.0, the --alb-verify-weight flag should be used instead For this feature to work, the argo-rollouts deployment requires the following AWS API permissions under the Elastic Load Balancing API : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"elasticloadbalancing:DescribeTargetGroups\" , \"elasticloadbalancing:DescribeLoadBalancers\" , \"elasticloadbalancing:DescribeListeners\" , \"elasticloadbalancing:DescribeRules\" , \"elasticloadbalancing:DescribeTags\" , \"elasticloadbalancing:DescribeTargetHealth\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] } There are various ways of granting AWS privileges to the argo-rollouts pods, which is highly dependent to your cluster's AWS environment, and out-of-scope of this documentation. Some solutions include: AWS access and secret keys kiam kube2iam EKS ServiceAccount IAM Roles","title":"Usage"},{"location":"features/traffic-management/alb/#custom-annotations-prefix","text":"The AWS Load Balancer Controller allows users to customize the annotation prefix used by the Ingress controller using a flag to the controller, --annotations-prefix (from the default of alb.ingress.kubernetes.io ). If your AWS Load Balancer Controller is customized to use a different annotation prefix, annotationPrefix field should be specified such that the Ingress object will be annotated in a manner understood by the cluster's aws load balancer controller. apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : strategy : canary : trafficRouting : alb : annotationPrefix : custom.alb.ingress.kubernetes.io","title":"Custom annotations-prefix"},{"location":"features/traffic-management/alb/#custom-kubernetesioingressclass","text":"By default, Argo Rollout will operate on Ingresses with the annotation: apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : alb To configure the controller to operate on Ingresses with different kubernetes.io/ingress.class values, the controller can specify a different value through the --alb-ingress-classes flag in the controller command line arguments. Note that the --alb-ingress-classes flag can be specified multiple times if the Argo Rollouts controller should operate on multiple values. This may be desired when a cluster has multiple Ingress controllers that operate on different kubernetes.io/ingress.class values. If the controller needs to operate on any Ingress without the kubernetes.io/ingress.class annotation, the flag can be specified with an empty string (e.g. --alb-ingress-classes '' ).","title":"Custom kubernetes.io/ingress.class"},{"location":"features/traffic-management/ambassador/","text":"Ambassador Edge Stack \u00b6 Ambassador Edge Stack provides the functionality you need at the edge your Kubernetes cluster (hence, an \"edge stack\"). This includes an API gateway, ingress controller, load balancer, developer portal, canary traffic routing and more. It provides a group of CRDs that users can configure to enable different functionalities. Argo-Rollouts provides an integration that leverages Ambassador's canary routing capability . This allows the traffic to your application to be gradually incremented while new versions are being deployed. How it works \u00b6 Ambassador Edge Stack provides a resource called Mapping that is used to configure how to route traffic to services. Ambassador canary deployment is achieved by creating 2 mappings with the same URL prefix pointing to different services. Consider the following example: apiVersion : getambassador.io/v2 kind : Mapping metadata : name : stable-mapping spec : prefix : /someapp rewrite : / service : someapp-stable:80 --- apiVersion : getambassador.io/v2 kind : Mapping metadata : name : canary-mapping spec : prefix : /someapp rewrite : / service : someapp-canary:80 weight : 30 In the example above we are configuring Ambassador to route 30% of the traffic coming from <public ingress>/someapp to the service someapp-canary and the rest of the traffic will go to the service someapp-stable . If users want to gradually increase the traffic to the canary service, they have to update the canary-mapping setting the weight to the desired value either manually or automating it somehow. With Argo-Rollouts there is no need to create the canary-mapping . The process of creating it and gradually updating its weight is fully automated by the Argo-Rollouts controller. The following example shows how to configure the Rollout resource to use Ambassador as a traffic router for canary deployments: apiVersion : argoproj.io/v1alpha1 kind : Rollout ... spec : strategy : canary : stableService : someapp-stable canaryService : someapp-canary trafficRouting : ambassador : mappings : - stable-mapping steps : - setWeight : 30 - pause : { duration : 60s } - setWeight : 60 - pause : { duration : 60s } Under spec.strategy.canary.trafficRouting.ambassador there are 2 possible attributes: mappings : Required. At least one Ambassador mapping must be provided for Argo-Rollouts to be able to manage the canary deployment. Multiple mappings are also supported in case there are multiple routes to the service (e.g., your service has multiple ports, or can be accessed via different URLs). If no mapping is provided Argo-Rollouts will send an error event and the rollout will be aborted. When Ambassador is configured in the trafficRouting attribute of the manifest, the Rollout controller will: 1. Create one canary mapping for each stable mapping provided in the Rollout manifest 1. Proceed with the steps according to the configuration updating the canary mapping weight 1. At the end of the process Argo-Rollout will delete all the canary mappings created Endpoint Resolver \u00b6 By default, Ambassador uses kube-proxy to route traffic to Pods. However we should configure it to bypass kube-proxy and route traffic directly to pods. This will provide true L7 load balancing which is desirable in a canary workflow. This approach is called endpoint routing and can be achieve by configuring endpoint resolvers . To configure Ambassador to use endpoint resolver it is necessary to apply the following resource in the cluster: apiVersion : getambassador.io/v2 kind : KubernetesEndpointResolver metadata : name : endpoint And then configure the mapping to use it setting the resolver attribute: apiVersion: getambassador.io/v2 kind: Mapping metadata: name: stable-mapping spec: resolver: endpoint prefix: /someapp rewrite: / service: someapp-stable:80 For more details about the Ambassador and Argo-Rollouts integration, see the Ambassador Argo documentation .","title":"Ambassador"},{"location":"features/traffic-management/ambassador/#ambassador-edge-stack","text":"Ambassador Edge Stack provides the functionality you need at the edge your Kubernetes cluster (hence, an \"edge stack\"). This includes an API gateway, ingress controller, load balancer, developer portal, canary traffic routing and more. It provides a group of CRDs that users can configure to enable different functionalities. Argo-Rollouts provides an integration that leverages Ambassador's canary routing capability . This allows the traffic to your application to be gradually incremented while new versions are being deployed.","title":"Ambassador Edge Stack"},{"location":"features/traffic-management/ambassador/#how-it-works","text":"Ambassador Edge Stack provides a resource called Mapping that is used to configure how to route traffic to services. Ambassador canary deployment is achieved by creating 2 mappings with the same URL prefix pointing to different services. Consider the following example: apiVersion : getambassador.io/v2 kind : Mapping metadata : name : stable-mapping spec : prefix : /someapp rewrite : / service : someapp-stable:80 --- apiVersion : getambassador.io/v2 kind : Mapping metadata : name : canary-mapping spec : prefix : /someapp rewrite : / service : someapp-canary:80 weight : 30 In the example above we are configuring Ambassador to route 30% of the traffic coming from <public ingress>/someapp to the service someapp-canary and the rest of the traffic will go to the service someapp-stable . If users want to gradually increase the traffic to the canary service, they have to update the canary-mapping setting the weight to the desired value either manually or automating it somehow. With Argo-Rollouts there is no need to create the canary-mapping . The process of creating it and gradually updating its weight is fully automated by the Argo-Rollouts controller. The following example shows how to configure the Rollout resource to use Ambassador as a traffic router for canary deployments: apiVersion : argoproj.io/v1alpha1 kind : Rollout ... spec : strategy : canary : stableService : someapp-stable canaryService : someapp-canary trafficRouting : ambassador : mappings : - stable-mapping steps : - setWeight : 30 - pause : { duration : 60s } - setWeight : 60 - pause : { duration : 60s } Under spec.strategy.canary.trafficRouting.ambassador there are 2 possible attributes: mappings : Required. At least one Ambassador mapping must be provided for Argo-Rollouts to be able to manage the canary deployment. Multiple mappings are also supported in case there are multiple routes to the service (e.g., your service has multiple ports, or can be accessed via different URLs). If no mapping is provided Argo-Rollouts will send an error event and the rollout will be aborted. When Ambassador is configured in the trafficRouting attribute of the manifest, the Rollout controller will: 1. Create one canary mapping for each stable mapping provided in the Rollout manifest 1. Proceed with the steps according to the configuration updating the canary mapping weight 1. At the end of the process Argo-Rollout will delete all the canary mappings created","title":"How it works"},{"location":"features/traffic-management/ambassador/#endpoint-resolver","text":"By default, Ambassador uses kube-proxy to route traffic to Pods. However we should configure it to bypass kube-proxy and route traffic directly to pods. This will provide true L7 load balancing which is desirable in a canary workflow. This approach is called endpoint routing and can be achieve by configuring endpoint resolvers . To configure Ambassador to use endpoint resolver it is necessary to apply the following resource in the cluster: apiVersion : getambassador.io/v2 kind : KubernetesEndpointResolver metadata : name : endpoint And then configure the mapping to use it setting the resolver attribute: apiVersion: getambassador.io/v2 kind: Mapping metadata: name: stable-mapping spec: resolver: endpoint prefix: /someapp rewrite: / service: someapp-stable:80 For more details about the Ambassador and Argo-Rollouts integration, see the Ambassador Argo documentation .","title":"Endpoint Resolver"},{"location":"features/traffic-management/istio/","text":"Istio \u00b6 Istio is a service mesh that offers a rich feature-set to control the flow of traffic to a web service. Istio offers this functionality through a set of CRDs, and Argo Rollouts automates the management of these resources to provide advanced traffic shaping capabilities to the different versions of the Rollout during an update. How it works \u00b6 Traffic splitting is accomplished in Istio by adjusting traffic weights defined in an Istio VirtualService . When using Argo Rollouts with Istio, a user deploys a VirtualService containing at least one HTTP route containing two HTTP route destinations : a route destination targeting the pods of canary ReplicaSet, and a route destination targeting the pods stable ReplicaSet. Istio provides two approaches for weighted traffic splitting, both approaches are available as options in Argo Rollouts: Host-level traffic splitting Subset-level traffic splitting Host-level Traffic Splitting \u00b6 The first approach to traffic splitting using Argo Rollouts and Istio, is splitting between two hostnames, or Kubernetes Services: a canary Service and a stable Service. This approach is similar to the way all other Argo Rollouts mesh/ingress-controller integrations work (e.g. ALB, SMI, Nginx). Using this approach, the user is required to deploy the following resources: Rollout Service (canary) Service (stable) VirtualService The Rollout should define the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : canaryService : canary-svc # required stableService : stable-svc # required trafficRouting : istio : virtualService : name : rollout-vsvc # required routes : - primary # optional if there is a single route in VirtualService, required otherwise steps : - setWeight : 5 - pause : duration : 10m The VirtualService must contain an HTTP route with a name referenced in the Rollout, containing two route destinations with host values that match the canaryService and stableService referenced in the Rollout. If the VirtualService is defined in a different namespace than the rollout, its name should be rollout-vsvc.<vsvc namespace name> . Note that Istio requires that all weights add to 100, so the initial weights can be 100% to stable, and 0% to canary. apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollout-vsvc spec : gateways : - istio-rollout-gateway hosts : - istio-rollout.dev.argoproj.io http : - name : primary # referenced in canary.trafficRouting.istio.virtualService.routes route : - destination : host : stable-svc # referenced in canary.stableService weight : 100 - destination : host : canary-svc # referenced in canary.canaryService weight : 0 Finally, a canary and stable Service should be deployed. The selector of these Services will be modified by the Rollout during an update to target the canary and stable ReplicaSet pods. Note that if the VirtualService and destination host resides in different namespaces (e.g., VirtualService and Rollout are not in the same namespace), the namespace should be included in the destination host (e.g. stable-svc.<namespace> ). apiVersion : v1 kind : Service metadata : name : canary-svc spec : ports : - port : 80 targetPort : http protocol : TCP name : http selector : app : rollouts-demo # This selector will be updated with the pod-template-hash of the canary ReplicaSet. e.g.: # rollouts-pod-template-hash: 7bf84f9696 --- apiVersion : v1 kind : Service metadata : name : stable-svc spec : ports : - port : 80 targetPort : http protocol : TCP name : http selector : app : rollouts-demo # This selector will be updated with the pod-template-hash of the stable ReplicaSet. e.g.: # rollouts-pod-template-hash: 123746c88d During the lifecycle of a Rollout update, Argo Rollouts will continuously: modify the canary Service spec.selector to contain the rollouts-pod-template-hash label of the canary ReplicaSet modify the stable Service spec.selector to contain the rollouts-pod-template-hash label of the stable ReplicaSet modify the VirtualService spec.http[].route[].weight to match the current desired canary weight Note Rollout does not make any other assumptions about the fields within the VirtualService or the Istio mesh. The user could specify additional configurations for the VirtualService like URI rewrite rules on the primary route or any other route if desired. The user can also create specific DestinationRules for each of the services. Subset-level Traffic Splitting \u00b6 Important Available since v1.0 The second approach to traffic splitting using Argo Rollouts and Istio, is splitting between two Istio DestinationRule Subsets : a canary subset and a stable subset. When splitting by DestinationRule subsets, the user is required to deploy the following resources: Rollout Service VirtualService DestinationRule The Rollout should define the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : trafficRouting : istio : virtualService : name : rollout-vsvc # required routes : - primary # optional if there is a single route in VirtualService, required otherwise destinationRule : name : rollout-destrule # required canarySubsetName : canary # required stableSubsetName : stable # required steps : - setWeight : 5 - pause : duration : 10m A single service should be defined, which targets the Rollout pods. Note that unlike the first approach, where traffic splitting is against multiple Services which are modified to contain the rollout-pod-template-hash of the canary/stable ReplicaSets, this Service is not modified by the rollout controller. apiVersion : v1 kind : Service metadata : name : rollout-example spec : ports : - port : 80 targetPort : http protocol : TCP name : http selector : app : rollout-example The VirtualService must contain an HTTP route with a name referenced in the Rollout, containing two route destinations with subset values that match the canarySubsetName and stableSubsetName referenced in the Rollout. Note that Istio requires that all weights add to 100, so the initial weights can be 100% to stable, and 0% to canary. apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollout-vsvc spec : gateways : - istio-rollout-gateway hosts : - istio-rollout.dev.argoproj.io http : - name : primary # referenced in canary.trafficRouting.istio.virtualService.routes route : - destination : host : rollout-example subset : stable # referenced in canary.trafficRouting.istio.destinationRule.stableSubsetName weight : 100 - destination : host : rollout-example subset : canary # referenced in canary.trafficRouting.istio.destinationRule.canarySubsetName weight : 0 Finally, the DestinationRule containing the canary and stable subsets referenced in the Rollout. apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : rollout-destrule spec : host : rollout-example subsets : - name : canary # referenced in canary.trafficRouting.istio.destinationRule.canarySubsetName labels : # labels will be injected with canary rollouts-pod-template-hash value app : rollout-example - name : stable # referenced in canary.trafficRouting.istio.destinationRule.stableSubsetName labels : # labels will be injected with stable rollouts-pod-template-hash value app : rollout-example During the lifecycle of a Rollout using Istio DestinationRule, Argo Rollouts will continuously: modify the VirtualService spec.http[].route[].weight to match the current desired canary weight modify the DestinationRule spec.subsets[].labels to contain the rollouts-pod-template-hash label of the canary and stable ReplicaSets Multicluster Setup \u00b6 If you have Istio multicluster setup where the primary Istio cluster is different than the cluster where the Argo Rollout controller is running, then you need to do the following setup: Create a ServiceAccount in the Istio primary cluster. apiVersion : v1 kind : ServiceAccount metadata : name : argo-rollouts-istio-primary namespace : <any-namespace-preferrably-config-namespace> Create a ClusterRole that provides access to Rollout controller in the Istio primary cluster. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : argo-rollouts-istio-primary rules : - apiGroups : - networking.istio.io resources : - virtualservices - destinationrules verbs : - get - list - watch - update - patch Note: If Argo Rollout controller is also installed in the Istio primary cluster, then you can reuse the argo-rollouts-clusterrole ClusterRole instead of creating a new one. Link the ClusterRole with the ServiceAccount in the Istio primary cluster. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : argo-rollouts-istio-primary roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : argo-rollouts-istio-primary subjects : - kind : ServiceAccount name : argo-rollouts-istio-primary namespace : <namespace-of-the-service-account> Now, use the following command to generate a secret for Rollout controller to access the Istio primary cluster. This secret will be applied to the cluster where Argo Rollout is running (i.e, Istio remote cluster), but will be generated from the Istio primary cluster. This secret can be generated right after Step 1, it only requires ServiceAccount to exist. Reference to the command . istioctl x create-remote-secret --type remote --name <cluster-name> \\ --namespace <namespace-of-the-service-account> \\ --service-account <service-account-created-in-step1> \\ --context = \"<ISTIO_PRIMARY_CLUSTER>\" | \\ kubectl apply -f - --context = \"<ARGO_ROLLOUT_CLUSTER/ISTIO_REMOTE_CLUSTER>\" Label the secret. kubectl label secret <istio-remote-secret> istio.argoproj.io/primary-cluster = \"true\" -n <namespace-of-the-secret> Comparison Between Approaches \u00b6 There are some advantages and disadvantages of host-level traffic splitting vs. subset-level traffic splitting. DNS requirements \u00b6 With host-level splitting, the VirtualService requires different host values to split among the two destinations. However, using two host values implies the use of different DNS names (one for the canary, the other for the stable). For north-south traffic, which reaches the Service through the Istio Gateway, having multiple DNS names to reach the canary vs. stable pods may not matter. However, for east-west or intra-cluster traffic, it forces microservice-to-microservice communication to choose whether to hit the stable or the canary DNS name, go through the gateway, or add DNS entries for the VirtualServices. In this situation, the DestinationRule subset traffic splitting would be a better option for intra-cluster canarying. Metrics \u00b6 Depending on the choice of host-level splitting vs. subset-level splitting, there will be different styles of prometheus metrics available. For example, if using host-level splitting, the metrics of the canary vs. stable would appear in the Istio Service metrics dashboard: On the other hand, when splitting via subsets, it would be necessary to query prometheus using different parameters, such as the workload name: Integrating with GitOps \u00b6 Earlier it was explained that VirtualServices should be deployed with an initial canary and stable weight of 0 and 100, respectively, such as in the following example: http : - name : primary route : - destination : host : stable-svc weight : 100 - destination : host : canary-svc weight : 0 This introduces a problem for users practicing GitOps. Since a Rollout will modify these VirtualService weights as the Rollout progresses through its steps, it unfortunately causes the VirtualService to become OutOfSync with the version in git. Additionally, if the VirtualService in git were to be applied while the Rollout is in this state (splitting traffic between the services), the apply would revert the weights back to the values in git (i.e. 100 to stable, 0 to canary). One protection which is implemented in Argo Rollouts, is that it continually watches for changes to managed VirtualServices. In the event that a kubectl apply were to happen using the VirtualService in git, the change would be detected immediately by the rollout controller, and the controller will instantly set the VirtualService weights back to the canary weight appropriate for the given step of the Rollout. But since there is momentary flapping of weights, this behavior should be understood. Some best practices to follow when using Argo CD with Argo Rollouts to prevent this behavior, is to leverage the following Argo CD features: Configure the application to ignore differences in the VirtualService. e.g.: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook spec : ignoreDifferences : - group : networking.istio.io kind : VirtualService jsonPointers : - /spec/http/0 Ignoring the differences in the VirtualServices HTTP route, prevents gitops differences in the VirtualService HTTP routes to contribute to the overall sync status of the Argo CD application. This adds the additional benefit of prevent auto-sync operations from being triggered. Configure the Application to only apply OutOfSync resources: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook spec : syncPolicy : syncOptions : - ApplyOutOfSyncOnly=true By default, when Argo CD syncs an application, it runs kubectl apply against all resources in git which are part of the application. The ApplyOutOfSyncOnly=true sync option indicates to Argo CD to skip applying resources which it already considers Synced , and only apply the ones which are OutOfSync . This option, when used in conjunction with the ignoreDifferences feature, provides a way to manage the conflict in the desired state of a VirtualService between Argo CD and Argo Rollouts. Argo CD also has an open issue here which would help address this problem. The proposed solution is to introduce an annotation to resources, which indicates to Argo CD to respect and preserve the differences at a specified path, in order to allow other controllers (e.g. Argo Rollouts) controller manage them instead. Alternatives Considered \u00b6 Rollout ownership over the Virtual Service \u00b6 An early design alternative was that instead of the controller modifying a referenced VirtualService, the Rollout controller would create, manage, and own a Virtual Service. While this approach is GitOps friendly, it introduces other issues: To provide the same flexibility as referencing VirtualService within a Rollout, the Rollout needs to inline a large portion of the Istio spec. However, networking is outside the responsibility of the Rollout and makes the Rollout spec unnecessarily complicated. If Istio introduces a feature, that feature will not be available in Argo Rollouts until implemented within Argo Rollouts. Both of these issues adds more complexity to the users and Argo Rollouts developers compared to referencing a Virtual Service. Istio support through the SMI Adapter for Istio \u00b6 SMI is the Service Mesh Interface, which serves as a standard interface for all common features of a service mesh. This feature is GitOps friendly, but native Istio has extra functionality that SMI does not currently provide.","title":"Istio"},{"location":"features/traffic-management/istio/#istio","text":"Istio is a service mesh that offers a rich feature-set to control the flow of traffic to a web service. Istio offers this functionality through a set of CRDs, and Argo Rollouts automates the management of these resources to provide advanced traffic shaping capabilities to the different versions of the Rollout during an update.","title":"Istio"},{"location":"features/traffic-management/istio/#how-it-works","text":"Traffic splitting is accomplished in Istio by adjusting traffic weights defined in an Istio VirtualService . When using Argo Rollouts with Istio, a user deploys a VirtualService containing at least one HTTP route containing two HTTP route destinations : a route destination targeting the pods of canary ReplicaSet, and a route destination targeting the pods stable ReplicaSet. Istio provides two approaches for weighted traffic splitting, both approaches are available as options in Argo Rollouts: Host-level traffic splitting Subset-level traffic splitting","title":"How it works"},{"location":"features/traffic-management/istio/#host-level-traffic-splitting","text":"The first approach to traffic splitting using Argo Rollouts and Istio, is splitting between two hostnames, or Kubernetes Services: a canary Service and a stable Service. This approach is similar to the way all other Argo Rollouts mesh/ingress-controller integrations work (e.g. ALB, SMI, Nginx). Using this approach, the user is required to deploy the following resources: Rollout Service (canary) Service (stable) VirtualService The Rollout should define the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : canaryService : canary-svc # required stableService : stable-svc # required trafficRouting : istio : virtualService : name : rollout-vsvc # required routes : - primary # optional if there is a single route in VirtualService, required otherwise steps : - setWeight : 5 - pause : duration : 10m The VirtualService must contain an HTTP route with a name referenced in the Rollout, containing two route destinations with host values that match the canaryService and stableService referenced in the Rollout. If the VirtualService is defined in a different namespace than the rollout, its name should be rollout-vsvc.<vsvc namespace name> . Note that Istio requires that all weights add to 100, so the initial weights can be 100% to stable, and 0% to canary. apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollout-vsvc spec : gateways : - istio-rollout-gateway hosts : - istio-rollout.dev.argoproj.io http : - name : primary # referenced in canary.trafficRouting.istio.virtualService.routes route : - destination : host : stable-svc # referenced in canary.stableService weight : 100 - destination : host : canary-svc # referenced in canary.canaryService weight : 0 Finally, a canary and stable Service should be deployed. The selector of these Services will be modified by the Rollout during an update to target the canary and stable ReplicaSet pods. Note that if the VirtualService and destination host resides in different namespaces (e.g., VirtualService and Rollout are not in the same namespace), the namespace should be included in the destination host (e.g. stable-svc.<namespace> ). apiVersion : v1 kind : Service metadata : name : canary-svc spec : ports : - port : 80 targetPort : http protocol : TCP name : http selector : app : rollouts-demo # This selector will be updated with the pod-template-hash of the canary ReplicaSet. e.g.: # rollouts-pod-template-hash: 7bf84f9696 --- apiVersion : v1 kind : Service metadata : name : stable-svc spec : ports : - port : 80 targetPort : http protocol : TCP name : http selector : app : rollouts-demo # This selector will be updated with the pod-template-hash of the stable ReplicaSet. e.g.: # rollouts-pod-template-hash: 123746c88d During the lifecycle of a Rollout update, Argo Rollouts will continuously: modify the canary Service spec.selector to contain the rollouts-pod-template-hash label of the canary ReplicaSet modify the stable Service spec.selector to contain the rollouts-pod-template-hash label of the stable ReplicaSet modify the VirtualService spec.http[].route[].weight to match the current desired canary weight Note Rollout does not make any other assumptions about the fields within the VirtualService or the Istio mesh. The user could specify additional configurations for the VirtualService like URI rewrite rules on the primary route or any other route if desired. The user can also create specific DestinationRules for each of the services.","title":"Host-level Traffic Splitting"},{"location":"features/traffic-management/istio/#subset-level-traffic-splitting","text":"Important Available since v1.0 The second approach to traffic splitting using Argo Rollouts and Istio, is splitting between two Istio DestinationRule Subsets : a canary subset and a stable subset. When splitting by DestinationRule subsets, the user is required to deploy the following resources: Rollout Service VirtualService DestinationRule The Rollout should define the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : trafficRouting : istio : virtualService : name : rollout-vsvc # required routes : - primary # optional if there is a single route in VirtualService, required otherwise destinationRule : name : rollout-destrule # required canarySubsetName : canary # required stableSubsetName : stable # required steps : - setWeight : 5 - pause : duration : 10m A single service should be defined, which targets the Rollout pods. Note that unlike the first approach, where traffic splitting is against multiple Services which are modified to contain the rollout-pod-template-hash of the canary/stable ReplicaSets, this Service is not modified by the rollout controller. apiVersion : v1 kind : Service metadata : name : rollout-example spec : ports : - port : 80 targetPort : http protocol : TCP name : http selector : app : rollout-example The VirtualService must contain an HTTP route with a name referenced in the Rollout, containing two route destinations with subset values that match the canarySubsetName and stableSubsetName referenced in the Rollout. Note that Istio requires that all weights add to 100, so the initial weights can be 100% to stable, and 0% to canary. apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollout-vsvc spec : gateways : - istio-rollout-gateway hosts : - istio-rollout.dev.argoproj.io http : - name : primary # referenced in canary.trafficRouting.istio.virtualService.routes route : - destination : host : rollout-example subset : stable # referenced in canary.trafficRouting.istio.destinationRule.stableSubsetName weight : 100 - destination : host : rollout-example subset : canary # referenced in canary.trafficRouting.istio.destinationRule.canarySubsetName weight : 0 Finally, the DestinationRule containing the canary and stable subsets referenced in the Rollout. apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : rollout-destrule spec : host : rollout-example subsets : - name : canary # referenced in canary.trafficRouting.istio.destinationRule.canarySubsetName labels : # labels will be injected with canary rollouts-pod-template-hash value app : rollout-example - name : stable # referenced in canary.trafficRouting.istio.destinationRule.stableSubsetName labels : # labels will be injected with stable rollouts-pod-template-hash value app : rollout-example During the lifecycle of a Rollout using Istio DestinationRule, Argo Rollouts will continuously: modify the VirtualService spec.http[].route[].weight to match the current desired canary weight modify the DestinationRule spec.subsets[].labels to contain the rollouts-pod-template-hash label of the canary and stable ReplicaSets","title":"Subset-level Traffic Splitting"},{"location":"features/traffic-management/istio/#multicluster-setup","text":"If you have Istio multicluster setup where the primary Istio cluster is different than the cluster where the Argo Rollout controller is running, then you need to do the following setup: Create a ServiceAccount in the Istio primary cluster. apiVersion : v1 kind : ServiceAccount metadata : name : argo-rollouts-istio-primary namespace : <any-namespace-preferrably-config-namespace> Create a ClusterRole that provides access to Rollout controller in the Istio primary cluster. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : argo-rollouts-istio-primary rules : - apiGroups : - networking.istio.io resources : - virtualservices - destinationrules verbs : - get - list - watch - update - patch Note: If Argo Rollout controller is also installed in the Istio primary cluster, then you can reuse the argo-rollouts-clusterrole ClusterRole instead of creating a new one. Link the ClusterRole with the ServiceAccount in the Istio primary cluster. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : argo-rollouts-istio-primary roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : argo-rollouts-istio-primary subjects : - kind : ServiceAccount name : argo-rollouts-istio-primary namespace : <namespace-of-the-service-account> Now, use the following command to generate a secret for Rollout controller to access the Istio primary cluster. This secret will be applied to the cluster where Argo Rollout is running (i.e, Istio remote cluster), but will be generated from the Istio primary cluster. This secret can be generated right after Step 1, it only requires ServiceAccount to exist. Reference to the command . istioctl x create-remote-secret --type remote --name <cluster-name> \\ --namespace <namespace-of-the-service-account> \\ --service-account <service-account-created-in-step1> \\ --context = \"<ISTIO_PRIMARY_CLUSTER>\" | \\ kubectl apply -f - --context = \"<ARGO_ROLLOUT_CLUSTER/ISTIO_REMOTE_CLUSTER>\" Label the secret. kubectl label secret <istio-remote-secret> istio.argoproj.io/primary-cluster = \"true\" -n <namespace-of-the-secret>","title":"Multicluster Setup"},{"location":"features/traffic-management/istio/#comparison-between-approaches","text":"There are some advantages and disadvantages of host-level traffic splitting vs. subset-level traffic splitting.","title":"Comparison Between Approaches"},{"location":"features/traffic-management/istio/#dns-requirements","text":"With host-level splitting, the VirtualService requires different host values to split among the two destinations. However, using two host values implies the use of different DNS names (one for the canary, the other for the stable). For north-south traffic, which reaches the Service through the Istio Gateway, having multiple DNS names to reach the canary vs. stable pods may not matter. However, for east-west or intra-cluster traffic, it forces microservice-to-microservice communication to choose whether to hit the stable or the canary DNS name, go through the gateway, or add DNS entries for the VirtualServices. In this situation, the DestinationRule subset traffic splitting would be a better option for intra-cluster canarying.","title":"DNS requirements"},{"location":"features/traffic-management/istio/#metrics","text":"Depending on the choice of host-level splitting vs. subset-level splitting, there will be different styles of prometheus metrics available. For example, if using host-level splitting, the metrics of the canary vs. stable would appear in the Istio Service metrics dashboard: On the other hand, when splitting via subsets, it would be necessary to query prometheus using different parameters, such as the workload name:","title":"Metrics"},{"location":"features/traffic-management/istio/#integrating-with-gitops","text":"Earlier it was explained that VirtualServices should be deployed with an initial canary and stable weight of 0 and 100, respectively, such as in the following example: http : - name : primary route : - destination : host : stable-svc weight : 100 - destination : host : canary-svc weight : 0 This introduces a problem for users practicing GitOps. Since a Rollout will modify these VirtualService weights as the Rollout progresses through its steps, it unfortunately causes the VirtualService to become OutOfSync with the version in git. Additionally, if the VirtualService in git were to be applied while the Rollout is in this state (splitting traffic between the services), the apply would revert the weights back to the values in git (i.e. 100 to stable, 0 to canary). One protection which is implemented in Argo Rollouts, is that it continually watches for changes to managed VirtualServices. In the event that a kubectl apply were to happen using the VirtualService in git, the change would be detected immediately by the rollout controller, and the controller will instantly set the VirtualService weights back to the canary weight appropriate for the given step of the Rollout. But since there is momentary flapping of weights, this behavior should be understood. Some best practices to follow when using Argo CD with Argo Rollouts to prevent this behavior, is to leverage the following Argo CD features: Configure the application to ignore differences in the VirtualService. e.g.: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook spec : ignoreDifferences : - group : networking.istio.io kind : VirtualService jsonPointers : - /spec/http/0 Ignoring the differences in the VirtualServices HTTP route, prevents gitops differences in the VirtualService HTTP routes to contribute to the overall sync status of the Argo CD application. This adds the additional benefit of prevent auto-sync operations from being triggered. Configure the Application to only apply OutOfSync resources: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : guestbook spec : syncPolicy : syncOptions : - ApplyOutOfSyncOnly=true By default, when Argo CD syncs an application, it runs kubectl apply against all resources in git which are part of the application. The ApplyOutOfSyncOnly=true sync option indicates to Argo CD to skip applying resources which it already considers Synced , and only apply the ones which are OutOfSync . This option, when used in conjunction with the ignoreDifferences feature, provides a way to manage the conflict in the desired state of a VirtualService between Argo CD and Argo Rollouts. Argo CD also has an open issue here which would help address this problem. The proposed solution is to introduce an annotation to resources, which indicates to Argo CD to respect and preserve the differences at a specified path, in order to allow other controllers (e.g. Argo Rollouts) controller manage them instead.","title":"Integrating with GitOps"},{"location":"features/traffic-management/istio/#alternatives-considered","text":"","title":"Alternatives Considered"},{"location":"features/traffic-management/istio/#rollout-ownership-over-the-virtual-service","text":"An early design alternative was that instead of the controller modifying a referenced VirtualService, the Rollout controller would create, manage, and own a Virtual Service. While this approach is GitOps friendly, it introduces other issues: To provide the same flexibility as referencing VirtualService within a Rollout, the Rollout needs to inline a large portion of the Istio spec. However, networking is outside the responsibility of the Rollout and makes the Rollout spec unnecessarily complicated. If Istio introduces a feature, that feature will not be available in Argo Rollouts until implemented within Argo Rollouts. Both of these issues adds more complexity to the users and Argo Rollouts developers compared to referencing a Virtual Service.","title":"Rollout ownership over the Virtual Service"},{"location":"features/traffic-management/istio/#istio-support-through-the-smi-adapter-for-istio","text":"SMI is the Service Mesh Interface, which serves as a standard interface for all common features of a service mesh. This feature is GitOps friendly, but native Istio has extra functionality that SMI does not currently provide.","title":"Istio support through the SMI Adapter for Istio"},{"location":"features/traffic-management/mixed/","text":"Multiple Providers \u00b6 Note Multiple trafficRouting is available since Argo Rollouts v1.2 The usage of multiple providers tries to cover scenarios where, for some reason, we have to use different providers on North-South and West-East traffic routing or any other hybrid architecture that requires the use of multiple providers. Examples of when you can use multiple providers \u00b6 Avoid injecting sidecars on your Ingress controller \u00b6 This is a common requirement of the service mesh and with multiple trafficRoutings you can leverage North-South traffic shifting to NGiNX and West-East traffic shifting to SMI, avoiding the need of adding the Ingress controller inside the mesh. Avoid manipulation of the host header at the Ingress \u00b6 Another common side effect of adding some of the Ingress controllers into the mesh, and is caused by the usage of those mesh host headers to be pointing into a mesh hostname in order to be routed. Avoid Big-Bang \u00b6 This takes place on existing fleets where downtime is very reduced or nearly impossible. To avoid big-bang-adoption the use of multiple providers can ease how teams can implement gradually new technologies. An example, where an existing fleet that is using a provider such as Ambassador and is already performing canary in a North-South fashion as part of their rollouts can gradually implement more providers such as Istio, SMI, etc. Hybrid Scenarios \u00b6 In this case, its very similar to avoiding the Big-Bang, either if it is part of the platform roadmap or a new redesign of the architecture, there are multiple scenarios where having the capacity of using multiple trafficRoutings is very much in need: gradual implementation, eased rollback of architecture or even for a fallback. Requirements \u00b6 The use of multiple providers requires that both providers comply with its minimum requirements independently. By example, if you want to use NGiNX and SMI you would need to have both SMI and NGiNX in place and produce the rollout configuration for both. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable smi : {}","title":"Multiple Providers"},{"location":"features/traffic-management/mixed/#multiple-providers","text":"Note Multiple trafficRouting is available since Argo Rollouts v1.2 The usage of multiple providers tries to cover scenarios where, for some reason, we have to use different providers on North-South and West-East traffic routing or any other hybrid architecture that requires the use of multiple providers.","title":"Multiple Providers"},{"location":"features/traffic-management/mixed/#examples-of-when-you-can-use-multiple-providers","text":"","title":"Examples of when you can use multiple providers"},{"location":"features/traffic-management/mixed/#avoid-injecting-sidecars-on-your-ingress-controller","text":"This is a common requirement of the service mesh and with multiple trafficRoutings you can leverage North-South traffic shifting to NGiNX and West-East traffic shifting to SMI, avoiding the need of adding the Ingress controller inside the mesh.","title":"Avoid injecting sidecars on your Ingress controller"},{"location":"features/traffic-management/mixed/#avoid-manipulation-of-the-host-header-at-the-ingress","text":"Another common side effect of adding some of the Ingress controllers into the mesh, and is caused by the usage of those mesh host headers to be pointing into a mesh hostname in order to be routed.","title":"Avoid manipulation of the host header at the Ingress"},{"location":"features/traffic-management/mixed/#avoid-big-bang","text":"This takes place on existing fleets where downtime is very reduced or nearly impossible. To avoid big-bang-adoption the use of multiple providers can ease how teams can implement gradually new technologies. An example, where an existing fleet that is using a provider such as Ambassador and is already performing canary in a North-South fashion as part of their rollouts can gradually implement more providers such as Istio, SMI, etc.","title":"Avoid Big-Bang"},{"location":"features/traffic-management/mixed/#hybrid-scenarios","text":"In this case, its very similar to avoiding the Big-Bang, either if it is part of the platform roadmap or a new redesign of the architecture, there are multiple scenarios where having the capacity of using multiple trafficRoutings is very much in need: gradual implementation, eased rollback of architecture or even for a fallback.","title":"Hybrid Scenarios"},{"location":"features/traffic-management/mixed/#requirements","text":"The use of multiple providers requires that both providers comply with its minimum requirements independently. By example, if you want to use NGiNX and SMI you would need to have both SMI and NGiNX in place and produce the rollout configuration for both. apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable smi : {}","title":"Requirements"},{"location":"features/traffic-management/nginx/","text":"Nginx \u00b6 The Nginx Ingress Controller enables traffic management through one or more Ingress objects to configure an Nginx deployment that routes traffic directly to pods. Each Nginx Ingress contains multiple annotations that modify the behavior of the Nginx Deployment. For traffic management between different versions of an application, the Nginx Ingress controller provides the capability to split traffic by introducing a second Ingress object (referred to as the canary Ingress) with some special annotations. You can read more about these canary annotations on the official canary annotations documentation page . The canary Ingress ignores any other non-canary nginx annotations. Instead, it leverages the annotation settings from the primary Ingress. The Rollout controller will always set the following two annotations on the canary Ingress (using your configured or the default nginx.ingress.kubernetes.io prefix): canary: true to indicate that this is the canary Ingress canary-weight: <num> to indicate what percentage of traffic to send to the canary. If all traffic is routed to the stable Service, this is set to 0 You can provide additional annotations to add to the canary Ingress via the additionalIngressAnnotations field to enable features like routing by header or cookie. Integration with Argo Rollouts \u00b6 There are a couple of required fields in a Rollout to send split traffic between versions using Nginx. Below is an example of a Rollout with those fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service # required stableService : stable-service # required trafficRouting : nginx : stableIngress : primary-ingress # required annotationPrefix : customingress.nginx.ingress.kubernetes.io # optional additionalIngressAnnotations : # optional canary-by-header : X-Canary canary-by-header-value : iwantsit The stable Ingress field is a reference to an Ingress in the same namespace of the Rollout. The Rollout requires the primary Ingress routes traffic to the stable Service. The Rollout checks that condition by confirming the Ingress has a backend that matches the Rollout's stableService. The controller routes traffic to the canary Service by creating a second Ingress with the canary annotations. As the Rollout progresses through the Canary steps, the controller updates the canary Ingress's canary annotations to reflect the desired state of the Rollout enabling traffic splitting between two different versions. Since the Nginx Ingress controller allows users to configure the annotation prefix used by the Ingress controller, Rollouts can specify the optional annotationPrefix field. The canary Ingress uses that prefix instead of the default nginx.ingress.kubernetes.io if the field set. Using Argo Rollouts with multiple NGINX ingress controllers \u00b6 As a default, the Argo Rollouts controller only operates on ingresses with the kubernetes.io/ingress.class annotation set to nginx . A user can configure the controller to operate on Ingresses with different kubernetes.io/ingress.class values by specifying the --nginx-ingress-classes flag. A user can list the --nginx-ingress-classes flag multiple times if the Argo Rollouts controller should operate on multiple values. This solves the case where a cluster has multiple Ingress controllers operating on different kubernetes.io/ingress.class values. If the user would like the controller to operate on any Ingress without the kubernetes.io/ingress.class annotation, a user should add the following --nginx-ingress-classes '' .","title":"NGINX"},{"location":"features/traffic-management/nginx/#nginx","text":"The Nginx Ingress Controller enables traffic management through one or more Ingress objects to configure an Nginx deployment that routes traffic directly to pods. Each Nginx Ingress contains multiple annotations that modify the behavior of the Nginx Deployment. For traffic management between different versions of an application, the Nginx Ingress controller provides the capability to split traffic by introducing a second Ingress object (referred to as the canary Ingress) with some special annotations. You can read more about these canary annotations on the official canary annotations documentation page . The canary Ingress ignores any other non-canary nginx annotations. Instead, it leverages the annotation settings from the primary Ingress. The Rollout controller will always set the following two annotations on the canary Ingress (using your configured or the default nginx.ingress.kubernetes.io prefix): canary: true to indicate that this is the canary Ingress canary-weight: <num> to indicate what percentage of traffic to send to the canary. If all traffic is routed to the stable Service, this is set to 0 You can provide additional annotations to add to the canary Ingress via the additionalIngressAnnotations field to enable features like routing by header or cookie.","title":"Nginx"},{"location":"features/traffic-management/nginx/#integration-with-argo-rollouts","text":"There are a couple of required fields in a Rollout to send split traffic between versions using Nginx. Below is an example of a Rollout with those fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout spec : ... strategy : canary : canaryService : canary-service # required stableService : stable-service # required trafficRouting : nginx : stableIngress : primary-ingress # required annotationPrefix : customingress.nginx.ingress.kubernetes.io # optional additionalIngressAnnotations : # optional canary-by-header : X-Canary canary-by-header-value : iwantsit The stable Ingress field is a reference to an Ingress in the same namespace of the Rollout. The Rollout requires the primary Ingress routes traffic to the stable Service. The Rollout checks that condition by confirming the Ingress has a backend that matches the Rollout's stableService. The controller routes traffic to the canary Service by creating a second Ingress with the canary annotations. As the Rollout progresses through the Canary steps, the controller updates the canary Ingress's canary annotations to reflect the desired state of the Rollout enabling traffic splitting between two different versions. Since the Nginx Ingress controller allows users to configure the annotation prefix used by the Ingress controller, Rollouts can specify the optional annotationPrefix field. The canary Ingress uses that prefix instead of the default nginx.ingress.kubernetes.io if the field set.","title":"Integration with Argo Rollouts"},{"location":"features/traffic-management/nginx/#using-argo-rollouts-with-multiple-nginx-ingress-controllers","text":"As a default, the Argo Rollouts controller only operates on ingresses with the kubernetes.io/ingress.class annotation set to nginx . A user can configure the controller to operate on Ingresses with different kubernetes.io/ingress.class values by specifying the --nginx-ingress-classes flag. A user can list the --nginx-ingress-classes flag multiple times if the Argo Rollouts controller should operate on multiple values. This solves the case where a cluster has multiple Ingress controllers operating on different kubernetes.io/ingress.class values. If the user would like the controller to operate on any Ingress without the kubernetes.io/ingress.class annotation, a user should add the following --nginx-ingress-classes '' .","title":"Using Argo Rollouts with multiple NGINX ingress controllers"},{"location":"features/traffic-management/smi/","text":"Service Mesh Interface (SMI) \u00b6 Important Available since v0.9.0 Service Mesh Interface (SMI) is a standard interface for service meshes on Kubernetes leveraged by many Service Mesh implementations (like Linkerd). SMI offers this functionality through a set of CRDs, and the Argo Rollouts controller creates these resources to manipulate the traffic routing into the desired state. The Argo Rollout controller achieves traffic shaping by creating and manipulating the TrafficSplit CR . A TrafficSplit describes the desired traffic routing for an application and relies on the underlying Service Meshes implement that desired state. Instead of worrying about the details of a specific service mesh, a user needs to specify a root Service that clients use to communicate and a list of backends consisting of a Service and weight. The Service Mesh implementing SMI uses this spec to route traffic to the backends Services based on the weights of the backends. For Rollout users, the Argo Rollout controller creates and manipulates the TrafficSplit using the following information: Canary Service: Name of the service that sends traffic only to the canary pods Stable Service: Name of the service that sends traffic only to the stable po ds Root Service: Name of the service that clients use to communicate. If a request comes to this root service not through a proxy, the standard Kubernetes service routing will be used. Below is an example of a Rollout with all the required fields configured: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : steps : - setWeight : 5 - pause : duration : 600 canaryService : canary-svc # required stableService : stable-svc # required trafficRouting : smi : rootService : root-svc # optional trafficSplitName : rollout-example-traffic-split # optional With the above configuration, the controller can automate dynamic traffic splitting. First, the controller manipulates the canary and stable Service listed in the Rollout to make them only receive traffic from the respective canary and stable ReplicaSets. The controller achieves this by adding the ReplicaSet's unique pod template hash to that Service's selector. With the stable and canary Services configured, the controller creates a TrafficSplit using these Services in the backend, and the weights of the backend are dynamically configured from the current desired weight of the Rollout's canary steps. The controller sets the TrafficSplit's root service to the stableService unless the Rollout has the rootService field specified. This configured TrafficSplit along with the Service and Rollout resources enable fine-grained percentages of traffic between two versions of an application. Optionally, the user can specify a name for the traffic split. If there is no name listed in the Rollout, the controller uses the Rollout's name for the TrafficSplit. If a TrafficSplit with that name already exists and isn't owned by that Rollout, the controller marks the Rollout as an error state. Here is the TrafficSplit created from the above Rollout: apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollout-example-traffic-split spec : service : root-svc # controller uses the stableService if Rollout does not specify the rootService field backends : - service : stable-svc weight : 95 - service : canary-svc weight : 5 As a Rollout progresses through all its steps, the controller updates the TrafficSplit's backend weights to reflect the current weight of the Rollout. When the Rollout has successfully finished executing all the steps, the controller modifies the stable Service's selector to point at the desired ReplicaSet and TrafficSplit's weight to send 100% of traffic to the stable Service. Note The controller defaults to using the v1alpha1 version of the TrafficSplit. The Argo Rollouts operator can change the api version used by specifying a --traffic-split-api-version flag in the controller args.","title":"SMI"},{"location":"features/traffic-management/smi/#service-mesh-interface-smi","text":"Important Available since v0.9.0 Service Mesh Interface (SMI) is a standard interface for service meshes on Kubernetes leveraged by many Service Mesh implementations (like Linkerd). SMI offers this functionality through a set of CRDs, and the Argo Rollouts controller creates these resources to manipulate the traffic routing into the desired state. The Argo Rollout controller achieves traffic shaping by creating and manipulating the TrafficSplit CR . A TrafficSplit describes the desired traffic routing for an application and relies on the underlying Service Meshes implement that desired state. Instead of worrying about the details of a specific service mesh, a user needs to specify a root Service that clients use to communicate and a list of backends consisting of a Service and weight. The Service Mesh implementing SMI uses this spec to route traffic to the backends Services based on the weights of the backends. For Rollout users, the Argo Rollout controller creates and manipulates the TrafficSplit using the following information: Canary Service: Name of the service that sends traffic only to the canary pods Stable Service: Name of the service that sends traffic only to the stable po ds Root Service: Name of the service that clients use to communicate. If a request comes to this root service not through a proxy, the standard Kubernetes service routing will be used. Below is an example of a Rollout with all the required fields configured: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollout-example spec : ... strategy : canary : steps : - setWeight : 5 - pause : duration : 600 canaryService : canary-svc # required stableService : stable-svc # required trafficRouting : smi : rootService : root-svc # optional trafficSplitName : rollout-example-traffic-split # optional With the above configuration, the controller can automate dynamic traffic splitting. First, the controller manipulates the canary and stable Service listed in the Rollout to make them only receive traffic from the respective canary and stable ReplicaSets. The controller achieves this by adding the ReplicaSet's unique pod template hash to that Service's selector. With the stable and canary Services configured, the controller creates a TrafficSplit using these Services in the backend, and the weights of the backend are dynamically configured from the current desired weight of the Rollout's canary steps. The controller sets the TrafficSplit's root service to the stableService unless the Rollout has the rootService field specified. This configured TrafficSplit along with the Service and Rollout resources enable fine-grained percentages of traffic between two versions of an application. Optionally, the user can specify a name for the traffic split. If there is no name listed in the Rollout, the controller uses the Rollout's name for the TrafficSplit. If a TrafficSplit with that name already exists and isn't owned by that Rollout, the controller marks the Rollout as an error state. Here is the TrafficSplit created from the above Rollout: apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollout-example-traffic-split spec : service : root-svc # controller uses the stableService if Rollout does not specify the rootService field backends : - service : stable-svc weight : 95 - service : canary-svc weight : 5 As a Rollout progresses through all its steps, the controller updates the TrafficSplit's backend weights to reflect the current weight of the Rollout. When the Rollout has successfully finished executing all the steps, the controller modifies the stable Service's selector to point at the desired ReplicaSet and TrafficSplit's weight to send 100% of traffic to the stable Service. Note The controller defaults to using the v1alpha1 version of the TrafficSplit. The Argo Rollouts operator can change the api version used by specifying a --traffic-split-api-version flag in the controller args.","title":"Service Mesh Interface (SMI)"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts/","text":"Rollouts \u00b6 Manage argo rollouts Synopsis \u00b6 This command consists of multiple subcommands which can be used to manage Argo Rollouts. kubectl argo rollouts COMMAND [ flags ] Examples \u00b6 # Get guestbook rollout and watch progress kubectl argo rollouts get rollout guestbook -w # Pause the guestbook rollout kubectl argo rollouts pause guestbook # Promote the guestbook rollout kubectl argo rollouts promote guestbook # Abort the guestbook rollout kubectl argo rollouts abort guestbook # Retry the guestbook rollout kubectl argo rollouts retry guestbook Options \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use -h, --help help for kubectl-argo-rollouts --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts abort - Abort a rollout rollouts completion - Generate completion script rollouts create - Create a Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource rollouts dashboard - Start UI dashboard rollouts get - Get details about rollouts and experiments rollouts lint - Lint and validate a Rollout rollouts list - List rollouts or experiments rollouts notifications - Set of CLI commands that helps manage notifications settings rollouts pause - Pause a rollout rollouts promote - Promote a rollout rollouts restart - Restart the pods of a rollout rollouts retry - Retry a rollout or experiment rollouts set - Update various values on resources rollouts status - Show the status of a rollout rollouts terminate - Terminate an AnalysisRun or Experiment rollouts undo - Undo a rollout rollouts version - Print version","title":"Rollouts"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts/#rollouts","text":"Manage argo rollouts","title":"Rollouts"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts/#synopsis","text":"This command consists of multiple subcommands which can be used to manage Argo Rollouts. kubectl argo rollouts COMMAND [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts/#examples","text":"# Get guestbook rollout and watch progress kubectl argo rollouts get rollout guestbook -w # Pause the guestbook rollout kubectl argo rollouts pause guestbook # Promote the guestbook rollout kubectl argo rollouts promote guestbook # Abort the guestbook rollout kubectl argo rollouts abort guestbook # Retry the guestbook rollout kubectl argo rollouts retry guestbook","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts/#options","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use -h, --help help for kubectl-argo-rollouts --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts/#available-commands","text":"rollouts abort - Abort a rollout rollouts completion - Generate completion script rollouts create - Create a Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource rollouts dashboard - Start UI dashboard rollouts get - Get details about rollouts and experiments rollouts lint - Lint and validate a Rollout rollouts list - List rollouts or experiments rollouts notifications - Set of CLI commands that helps manage notifications settings rollouts pause - Pause a rollout rollouts promote - Promote a rollout rollouts restart - Restart the pods of a rollout rollouts retry - Retry a rollout or experiment rollouts set - Update various values on resources rollouts status - Show the status of a rollout rollouts terminate - Terminate an AnalysisRun or Experiment rollouts undo - Undo a rollout rollouts version - Print version","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/","text":"Rollouts Abort \u00b6 Abort a rollout Synopsis \u00b6 This command stops progressing the current rollout and reverts all steps. The previous ReplicaSet will be active. Note the 'spec.template' still represents the new rollout version. If the Rollout leaves the aborted state, it will try to go to the new version. Updating the 'spec.template' back to the previous version will fully revert the rollout. kubectl argo rollouts abort ROLLOUT_NAME [ flags ] Examples \u00b6 # Abort a rollout kubectl argo rollouts abort guestbook Options \u00b6 -h, --help help for abort Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Abort"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/#rollouts-abort","text":"Abort a rollout","title":"Rollouts Abort"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/#synopsis","text":"This command stops progressing the current rollout and reverts all steps. The previous ReplicaSet will be active. Note the 'spec.template' still represents the new rollout version. If the Rollout leaves the aborted state, it will try to go to the new version. Updating the 'spec.template' back to the previous version will fully revert the rollout. kubectl argo rollouts abort ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/#examples","text":"# Abort a rollout kubectl argo rollouts abort guestbook","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/#options","text":"-h, --help help for abort","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_abort/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_completion/","text":"Rollouts Completion \u00b6 Generate completion script Synopsis \u00b6 To load completions: Bash : $ source < ( yourprogram completion bash ) # To load completions for each session, execute once: # Linux: $ yourprogram completion bash > / etc / bash_completion . d / yourprogram # macOS: $ yourprogram completion bash > / usr / local / etc / bash_completion . d / yourprogram Zsh : # If shell completion is not already enabled in your environment, # you will need to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" >> ~/. zshrc # To load completions for each session, execute once: $ yourprogram completion zsh > \"${fpath[1]}/_yourprogram\" # You will need to start a new shell for this setup to take effect. fish : $ yourprogram completion fish | source # To load completions for each session, execute once: $ yourprogram completion fish > ~/. config / fish / completions / yourprogram . fish PowerShell : PS > yourprogram completion powershell | Out - String | Invoke - Expression # To load completions for every new session, run: PS > yourprogram completion powershell > yourprogram . ps1 # and source this file from your PowerShell profile. kubectl argo rollouts completion [ bash | zsh | fish | powershell ] Options \u00b6 -h, --help help for completion Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Completion"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_completion/#rollouts-completion","text":"Generate completion script","title":"Rollouts Completion"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_completion/#synopsis","text":"To load completions: Bash : $ source < ( yourprogram completion bash ) # To load completions for each session, execute once: # Linux: $ yourprogram completion bash > / etc / bash_completion . d / yourprogram # macOS: $ yourprogram completion bash > / usr / local / etc / bash_completion . d / yourprogram Zsh : # If shell completion is not already enabled in your environment, # you will need to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" >> ~/. zshrc # To load completions for each session, execute once: $ yourprogram completion zsh > \"${fpath[1]}/_yourprogram\" # You will need to start a new shell for this setup to take effect. fish : $ yourprogram completion fish | source # To load completions for each session, execute once: $ yourprogram completion fish > ~/. config / fish / completions / yourprogram . fish PowerShell : PS > yourprogram completion powershell | Out - String | Invoke - Expression # To load completions for every new session, run: PS > yourprogram completion powershell > yourprogram . ps1 # and source this file from your PowerShell profile. kubectl argo rollouts completion [ bash | zsh | fish | powershell ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_completion/#options","text":"-h, --help help for completion","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_completion/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_completion/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/","text":"Rollouts Create \u00b6 Create a Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource Synopsis \u00b6 This command creates a new Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource from a file. kubectl argo rollouts create [ flags ] Examples \u00b6 # Create an experiment and watch it kubectl argo rollouts create -f my-experiment.yaml -w Options \u00b6 -f, --filename stringArray Files to use to create the resource -h, --help help for create --no-color Do not colorize output -w, --watch Watch live updates to the resource after creating Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts create analysisrun - Create an AnalysisRun from an AnalysisTemplate or a ClusterAnalysisTemplate See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Create"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#rollouts-create","text":"Create a Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource","title":"Rollouts Create"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#synopsis","text":"This command creates a new Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource from a file. kubectl argo rollouts create [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#examples","text":"# Create an experiment and watch it kubectl argo rollouts create -f my-experiment.yaml -w","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#options","text":"-f, --filename stringArray Files to use to create the resource -h, --help help for create --no-color Do not colorize output -w, --watch Watch live updates to the resource after creating","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#available-commands","text":"rollouts create analysisrun - Create an AnalysisRun from an AnalysisTemplate or a ClusterAnalysisTemplate","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/","text":"Rollouts Create Analysisrun \u00b6 Create an AnalysisRun from an AnalysisTemplate or a ClusterAnalysisTemplate Synopsis \u00b6 This command creates a new AnalysisRun from an existing AnalysisTemplate resources or from an AnalysisTemplate file. kubectl argo rollouts create analysisrun [ flags ] Examples \u00b6 # Create an AnalysisRun from a local AnalysisTemplate file kubectl argo rollouts create analysisrun --from-file my-analysis-template.yaml # Create an AnalysisRun from a AnalysisTemplate in the cluster kubectl argo rollouts create analysisrun --from my-analysis-template # Create an AnalysisRun from a local ClusterAnalysisTemplate file kubectl argo rollouts create analysisrun --global --from my-analysis-cluster-template.yaml # Create an AnalysisRun from a ClusterAnalysisTemplate in the cluster kubectl argo rollouts create analysisrun --global --from my-analysis-cluster-template Options \u00b6 -a, --argument stringArray Arguments to the parameter template --from string Create an AnalysisRun from an AnalysisTemplate or ClusterAnalysisTemplate in the cluster --from-file string Create an AnalysisRun from an AnalysisTemplate or ClusterAnalysisTemplate in a local file --generate-name string Use the specified generateName for the run --global Use a ClusterAnalysisTemplate instead of a AnalysisTemplate -h, --help help for analysisrun --instance-id string Instance-ID for the AnalysisRun --name string Use the specified name for the run Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts create - Create a Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource","title":"Rollouts Create Analysisrun"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/#rollouts-create-analysisrun","text":"Create an AnalysisRun from an AnalysisTemplate or a ClusterAnalysisTemplate","title":"Rollouts Create Analysisrun"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/#synopsis","text":"This command creates a new AnalysisRun from an existing AnalysisTemplate resources or from an AnalysisTemplate file. kubectl argo rollouts create analysisrun [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/#examples","text":"# Create an AnalysisRun from a local AnalysisTemplate file kubectl argo rollouts create analysisrun --from-file my-analysis-template.yaml # Create an AnalysisRun from a AnalysisTemplate in the cluster kubectl argo rollouts create analysisrun --from my-analysis-template # Create an AnalysisRun from a local ClusterAnalysisTemplate file kubectl argo rollouts create analysisrun --global --from my-analysis-cluster-template.yaml # Create an AnalysisRun from a ClusterAnalysisTemplate in the cluster kubectl argo rollouts create analysisrun --global --from my-analysis-cluster-template","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/#options","text":"-a, --argument stringArray Arguments to the parameter template --from string Create an AnalysisRun from an AnalysisTemplate or ClusterAnalysisTemplate in the cluster --from-file string Create an AnalysisRun from an AnalysisTemplate or ClusterAnalysisTemplate in a local file --generate-name string Use the specified generateName for the run --global Use a ClusterAnalysisTemplate instead of a AnalysisTemplate -h, --help help for analysisrun --instance-id string Instance-ID for the AnalysisRun --name string Use the specified name for the run","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_create_analysisrun/#see-also","text":"rollouts create - Create a Rollout, Experiment, AnalysisTemplate, ClusterAnalysisTemplate, or AnalysisRun resource","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/","text":"Rollouts Dashboard \u00b6 Start UI dashboard Synopsis \u00b6 Start UI dashboard kubectl argo rollouts dashboard [ flags ] Options \u00b6 -h, --help help for dashboard Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Dashboard"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/#rollouts-dashboard","text":"Start UI dashboard","title":"Rollouts Dashboard"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/#synopsis","text":"Start UI dashboard kubectl argo rollouts dashboard [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/#options","text":"-h, --help help for dashboard","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_dashboard/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/","text":"Rollouts Get \u00b6 Get details about rollouts and experiments Synopsis \u00b6 This command consists of multiple subcommands which can be used to get extended information about a rollout or experiment. kubectl argo rollouts get <rollout | experiment> RESOURCE_NAME [ flags ] Examples \u00b6 # Get a rollout kubectl argo rollouts get rollout guestbook # Watch a rollouts progress kubectl argo rollouts get rollout guestbook -w # Get an experiment kubectl argo rollouts get experiment my-experiment Options \u00b6 -h, --help help for get Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts get experiment - Get details about an Experiment rollouts get rollout - Get details about a rollout See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Get"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#rollouts-get","text":"Get details about rollouts and experiments","title":"Rollouts Get"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#synopsis","text":"This command consists of multiple subcommands which can be used to get extended information about a rollout or experiment. kubectl argo rollouts get <rollout | experiment> RESOURCE_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#examples","text":"# Get a rollout kubectl argo rollouts get rollout guestbook # Watch a rollouts progress kubectl argo rollouts get rollout guestbook -w # Get an experiment kubectl argo rollouts get experiment my-experiment","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#options","text":"-h, --help help for get","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#available-commands","text":"rollouts get experiment - Get details about an Experiment rollouts get rollout - Get details about a rollout","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/","text":"Rollouts Get Experiment \u00b6 Get details about an Experiment Synopsis \u00b6 Get details about and visual representation of a experiment. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. Tree view icons Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job kubectl argo rollouts get experiment EXPERIMENT_NAME [ flags ] Examples \u00b6 # Get an experiment kubectl argo rollouts get experiment my-experiment # Watch experiment progress kubectl argo rollouts get experiment my-experiment -w Options \u00b6 -h, --help help for experiment --no-color Do not colorize output -w, --watch Watch live updates to the rollout Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts get - Get details about rollouts and experiments","title":"Rollouts Get Experiment"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/#rollouts-get-experiment","text":"Get details about an Experiment","title":"Rollouts Get Experiment"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/#synopsis","text":"Get details about and visual representation of a experiment. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. Tree view icons Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job kubectl argo rollouts get experiment EXPERIMENT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/#examples","text":"# Get an experiment kubectl argo rollouts get experiment my-experiment # Watch experiment progress kubectl argo rollouts get experiment my-experiment -w","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/#options","text":"-h, --help help for experiment --no-color Do not colorize output -w, --watch Watch live updates to the rollout","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_experiment/#see-also","text":"rollouts get - Get details about rollouts and experiments","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/","text":"Rollouts Get Rollout \u00b6 Get details about a rollout Synopsis \u00b6 Get details about and visual representation of a rollout. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. Tree view icons Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job kubectl argo rollouts get rollout ROLLOUT_NAME [ flags ] Examples \u00b6 # Get a rollout kubectl argo rollouts get rollout guestbook # Watch progress of a rollout kubectl argo rollouts get rollout guestbook -w Options \u00b6 -h, --help help for rollout --no-color Do not colorize output --timeout-seconds int Timeout after specified seconds -w, --watch Watch live updates to the rollout Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts get - Get details about rollouts and experiments","title":"Rollouts Get Rollout"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/#rollouts-get-rollout","text":"Get details about a rollout","title":"Rollouts Get Rollout"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/#synopsis","text":"Get details about and visual representation of a rollout. It returns a bunch of metadata on a resource and a tree view of the child resources created by the parent. Tree view icons Icon Kind \u27f3 Rollout \u03a3 Experiment \u03b1 AnalysisRun # Revision \u29c9 ReplicaSet \u25a1 Pod \u229e Job kubectl argo rollouts get rollout ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/#examples","text":"# Get a rollout kubectl argo rollouts get rollout guestbook # Watch progress of a rollout kubectl argo rollouts get rollout guestbook -w","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/#options","text":"-h, --help help for rollout --no-color Do not colorize output --timeout-seconds int Timeout after specified seconds -w, --watch Watch live updates to the rollout","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_get_rollout/#see-also","text":"rollouts get - Get details about rollouts and experiments","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/","text":"Rollouts Lint \u00b6 Lint and validate a Rollout Synopsis \u00b6 This command lints and validates a new Rollout resource from a file. kubectl argo rollouts lint [ flags ] Examples \u00b6 # Lint a rollout kubectl argo rollouts lint -f my-rollout.yaml Options \u00b6 -f, --filename string File to lint -h, --help help for lint Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Lint"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/#rollouts-lint","text":"Lint and validate a Rollout","title":"Rollouts Lint"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/#synopsis","text":"This command lints and validates a new Rollout resource from a file. kubectl argo rollouts lint [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/#examples","text":"# Lint a rollout kubectl argo rollouts lint -f my-rollout.yaml","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/#options","text":"-f, --filename string File to lint -h, --help help for lint","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_lint/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/","text":"Rollouts List \u00b6 List rollouts or experiments Synopsis \u00b6 This command consists of multiple subcommands which can be used to lists all of the rollouts or experiments for a specified namespace (uses current namespace context if namespace not specified). kubectl argo rollouts list <rollout | experiment> [ flags ] Examples \u00b6 # List rollouts kubectl argo rollouts list rollouts # List experiments kubectl argo rollouts list experiments Options \u00b6 -h, --help help for list Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts list experiments - List experiments rollouts list rollouts - List rollouts See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts List"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#rollouts-list","text":"List rollouts or experiments","title":"Rollouts List"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#synopsis","text":"This command consists of multiple subcommands which can be used to lists all of the rollouts or experiments for a specified namespace (uses current namespace context if namespace not specified). kubectl argo rollouts list <rollout | experiment> [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#examples","text":"# List rollouts kubectl argo rollouts list rollouts # List experiments kubectl argo rollouts list experiments","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#options","text":"-h, --help help for list","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#available-commands","text":"rollouts list experiments - List experiments rollouts list rollouts - List rollouts","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/","text":"Rollouts List Experiments \u00b6 List experiments Synopsis \u00b6 This command lists all of the experiments for a specified namespace (uses current namespace context if namespace not specified). kubectl argo rollouts list experiments [ flags ] Examples \u00b6 # List rollouts kubectl argo rollouts list experiments # List rollouts from all namespaces kubectl argo rollouts list experiments --all-namespaces # List rollouts and watch for changes kubectl argo rollouts list experiments --watch Options \u00b6 --all-namespaces Include all namespaces -h, --help help for experiments Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts list - List rollouts or experiments","title":"Rollouts List Experiments"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/#rollouts-list-experiments","text":"List experiments","title":"Rollouts List Experiments"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/#synopsis","text":"This command lists all of the experiments for a specified namespace (uses current namespace context if namespace not specified). kubectl argo rollouts list experiments [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/#examples","text":"# List rollouts kubectl argo rollouts list experiments # List rollouts from all namespaces kubectl argo rollouts list experiments --all-namespaces # List rollouts and watch for changes kubectl argo rollouts list experiments --watch","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/#options","text":"--all-namespaces Include all namespaces -h, --help help for experiments","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_experiments/#see-also","text":"rollouts list - List rollouts or experiments","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/","text":"Rollouts List Rollouts \u00b6 List rollouts Synopsis \u00b6 This command lists all of the rollouts for a specified namespace (uses current namespace context if namespace not specified). kubectl argo rollouts list rollouts [ flags ] Examples \u00b6 # List rollouts kubectl argo rollouts list rollouts # List rollouts from all namespaces kubectl argo rollouts list rollouts --all-namespaces # List rollouts and watch for changes kubectl argo rollouts list rollouts --watch Options \u00b6 -A, --all-namespaces Include all namespaces -h, --help help for rollouts --name string Only show rollout with specified name --timestamps Print timestamps on updates -w, --watch Watch for changes Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts list - List rollouts or experiments","title":"Rollouts List Rollouts"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/#rollouts-list-rollouts","text":"List rollouts","title":"Rollouts List Rollouts"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/#synopsis","text":"This command lists all of the rollouts for a specified namespace (uses current namespace context if namespace not specified). kubectl argo rollouts list rollouts [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/#examples","text":"# List rollouts kubectl argo rollouts list rollouts # List rollouts from all namespaces kubectl argo rollouts list rollouts --all-namespaces # List rollouts and watch for changes kubectl argo rollouts list rollouts --watch","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/#options","text":"-A, --all-namespaces Include all namespaces -h, --help help for rollouts --name string Only show rollout with specified name --timestamps Print timestamps on updates -w, --watch Watch for changes","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_list_rollouts/#see-also","text":"rollouts list - List rollouts or experiments","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/","text":"Rollouts Notifications \u00b6 Set of CLI commands that helps manage notifications settings Synopsis \u00b6 Set of CLI commands that helps manage notifications settings kubectl argo rollouts notifications [ flags ] Options \u00b6 --config-map string argo-rollouts-notification-configmap.yaml file path -h, --help help for notifications --password string Password for basic authentication to the API server --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --username string Username for basic authentication to the API server Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts notifications template - Notification templates related commands rollouts notifications trigger - Notification triggers related commands See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Notifications"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/#rollouts-notifications","text":"Set of CLI commands that helps manage notifications settings","title":"Rollouts Notifications"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/#synopsis","text":"Set of CLI commands that helps manage notifications settings kubectl argo rollouts notifications [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/#options","text":"--config-map string argo-rollouts-notification-configmap.yaml file path -h, --help help for notifications --password string Password for basic authentication to the API server --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --username string Username for basic authentication to the API server","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/#available-commands","text":"rollouts notifications template - Notification templates related commands rollouts notifications trigger - Notification triggers related commands","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/","text":"Rollouts Notifications Template \u00b6 Notification templates related commands Synopsis \u00b6 Notification templates related commands kubectl argo rollouts notifications template [ flags ] Options \u00b6 -h, --help help for template Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Available Commands \u00b6 rollouts notifications template get - Prints information about configured templates rollouts notifications template notify - Generates notification using the specified template and send it to specified recipients See Also \u00b6 rollouts notifications - Set of CLI commands that helps manage notifications settings","title":"Rollouts Notifications Template"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/#rollouts-notifications-template","text":"Notification templates related commands","title":"Rollouts Notifications Template"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/#synopsis","text":"Notification templates related commands kubectl argo rollouts notifications template [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/#options","text":"-h, --help help for template","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/#available-commands","text":"rollouts notifications template get - Prints information about configured templates rollouts notifications template notify - Generates notification using the specified template and send it to specified recipients","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template/#see-also","text":"rollouts notifications - Set of CLI commands that helps manage notifications settings","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/","text":"Rollouts Notifications Template Get \u00b6 Prints information about configured templates Synopsis \u00b6 Prints information about configured templates kubectl argo rollouts notifications template get [ flags ] Examples \u00b6 # prints all templates kubectl argo rollouts notifications template get # print YAML formatted app-sync-succeeded template definition kubectl argo rollouts notifications template get app-sync-succeeded -o = yaml Options \u00b6 -h, --help help for get -o, --output string Output format. One of:json|yaml|wide|name (default \"wide\") Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server See Also \u00b6 rollouts notifications template - Notification templates related commands","title":"Rollouts Notifications Template Get"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/#rollouts-notifications-template-get","text":"Prints information about configured templates","title":"Rollouts Notifications Template Get"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/#synopsis","text":"Prints information about configured templates kubectl argo rollouts notifications template get [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/#examples","text":"# prints all templates kubectl argo rollouts notifications template get # print YAML formatted app-sync-succeeded template definition kubectl argo rollouts notifications template get app-sync-succeeded -o = yaml","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/#options","text":"-h, --help help for get -o, --output string Output format. One of:json|yaml|wide|name (default \"wide\")","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_get/#see-also","text":"rollouts notifications template - Notification templates related commands","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/","text":"Rollouts Notifications Template Notify \u00b6 Generates notification using the specified template and send it to specified recipients Synopsis \u00b6 Generates notification using the specified template and send it to specified recipients kubectl argo rollouts notifications template notify NAME RESOURCE_NAME [ flags ] Examples \u00b6 # Trigger notification using in-cluster config map and secret kubectl argo rollouts notifications template notify app-sync-succeeded guestbook --recipient slack:my-slack-channel # Render notification render generated notification in console kubectl argo rollouts notifications template notify app-sync-succeeded guestbook Options \u00b6 -h, --help help for notify --recipient stringArray List of recipients (default [console:stdout]) Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server See Also \u00b6 rollouts notifications template - Notification templates related commands","title":"Rollouts Notifications Template Notify"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/#rollouts-notifications-template-notify","text":"Generates notification using the specified template and send it to specified recipients","title":"Rollouts Notifications Template Notify"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/#synopsis","text":"Generates notification using the specified template and send it to specified recipients kubectl argo rollouts notifications template notify NAME RESOURCE_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/#examples","text":"# Trigger notification using in-cluster config map and secret kubectl argo rollouts notifications template notify app-sync-succeeded guestbook --recipient slack:my-slack-channel # Render notification render generated notification in console kubectl argo rollouts notifications template notify app-sync-succeeded guestbook","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/#options","text":"-h, --help help for notify --recipient stringArray List of recipients (default [console:stdout])","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_template_notify/#see-also","text":"rollouts notifications template - Notification templates related commands","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/","text":"Rollouts Notifications Trigger \u00b6 Notification triggers related commands Synopsis \u00b6 Notification triggers related commands kubectl argo rollouts notifications trigger [ flags ] Options \u00b6 -h, --help help for trigger Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server Available Commands \u00b6 rollouts notifications trigger get - Prints information about configured triggers rollouts notifications trigger run - Evaluates specified trigger condition and prints the result See Also \u00b6 rollouts notifications - Set of CLI commands that helps manage notifications settings","title":"Rollouts Notifications Trigger"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/#rollouts-notifications-trigger","text":"Notification triggers related commands","title":"Rollouts Notifications Trigger"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/#synopsis","text":"Notification triggers related commands kubectl argo rollouts notifications trigger [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/#options","text":"-h, --help help for trigger","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/#available-commands","text":"rollouts notifications trigger get - Prints information about configured triggers rollouts notifications trigger run - Evaluates specified trigger condition and prints the result","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger/#see-also","text":"rollouts notifications - Set of CLI commands that helps manage notifications settings","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/","text":"Rollouts Notifications Trigger Get \u00b6 Prints information about configured triggers Synopsis \u00b6 Prints information about configured triggers kubectl argo rollouts notifications trigger get [ flags ] Examples \u00b6 # prints all triggers kubectl argo rollouts notifications trigger get # print YAML formatted on-sync-failed trigger definition kubectl argo rollouts notifications trigger get on-sync-failed -o = yaml Options \u00b6 -h, --help help for get -o, --output string Output format. One of:json|yaml|wide|name (default \"wide\") Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server See Also \u00b6 rollouts notifications trigger - Notification triggers related commands","title":"Rollouts Notifications Trigger Get"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/#rollouts-notifications-trigger-get","text":"Prints information about configured triggers","title":"Rollouts Notifications Trigger Get"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/#synopsis","text":"Prints information about configured triggers kubectl argo rollouts notifications trigger get [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/#examples","text":"# prints all triggers kubectl argo rollouts notifications trigger get # print YAML formatted on-sync-failed trigger definition kubectl argo rollouts notifications trigger get on-sync-failed -o = yaml","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/#options","text":"-h, --help help for get -o, --output string Output format. One of:json|yaml|wide|name (default \"wide\")","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_get/#see-also","text":"rollouts notifications trigger - Notification triggers related commands","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/","text":"Rollouts Notifications Trigger Run \u00b6 Evaluates specified trigger condition and prints the result Synopsis \u00b6 Evaluates specified trigger condition and prints the result kubectl argo rollouts notifications trigger run NAME RESOURCE_NAME [ flags ] Examples \u00b6 # Execute trigger configured in 'argocd-notification-cm' ConfigMap kubectl argo rollouts notifications trigger run on-sync-status-unknown ./sample-app.yaml # Execute trigger using my-config-map.yaml instead of 'argo-rollouts-notification-configmap' ConfigMap kubectl argo rollouts notifications trigger run on-sync-status-unknown ./sample-app.yaml \\ --config-map ./my-config-map.yaml Options \u00b6 -h, --help help for run Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server See Also \u00b6 rollouts notifications trigger - Notification triggers related commands","title":"Rollouts Notifications Trigger Run"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/#rollouts-notifications-trigger-run","text":"Evaluates specified trigger condition and prints the result","title":"Rollouts Notifications Trigger Run"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/#synopsis","text":"Evaluates specified trigger condition and prints the result kubectl argo rollouts notifications trigger run NAME RESOURCE_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/#examples","text":"# Execute trigger configured in 'argocd-notification-cm' ConfigMap kubectl argo rollouts notifications trigger run on-sync-status-unknown ./sample-app.yaml # Execute trigger using my-config-map.yaml instead of 'argo-rollouts-notification-configmap' ConfigMap kubectl argo rollouts notifications trigger run on-sync-status-unknown ./sample-app.yaml \\ --config-map ./my-config-map.yaml","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/#options","text":"-h, --help help for run","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --config-map string argo-rollouts-notification-configmap.yaml file path --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to a kube config. Only required if out-of-cluster --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --password string Password for basic authentication to the API server --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") --secret string argo-rollouts-notification-secret.yaml file path. Use empty secret if provided value is ':empty' --server string The address and port of the Kubernetes API server --tls-server-name string If provided, this name will be used to validate server certificate. If this is not provided, hostname used to contact the server is used. --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use --username string Username for basic authentication to the API server","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_notifications_trigger_run/#see-also","text":"rollouts notifications trigger - Notification triggers related commands","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/","text":"Rollouts Pause \u00b6 Pause a rollout Synopsis \u00b6 Set the rollout paused state to 'true' kubectl argo rollouts pause ROLLOUT_NAME [ flags ] Examples \u00b6 # Pause a rollout kubectl argo rollouts pause guestbook Options \u00b6 -h, --help help for pause Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Pause"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/#rollouts-pause","text":"Pause a rollout","title":"Rollouts Pause"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/#synopsis","text":"Set the rollout paused state to 'true' kubectl argo rollouts pause ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/#examples","text":"# Pause a rollout kubectl argo rollouts pause guestbook","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/#options","text":"-h, --help help for pause","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_pause/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/","text":"Rollouts Promote \u00b6 Promote a rollout Synopsis \u00b6 Promote a rollout Promotes a rollout paused at a canary step, or a paused blue-green pre-promotion. To skip analysis, pauses and steps entirely, use '--full' to fully promote the rollout kubectl argo rollouts promote ROLLOUT_NAME [ flags ] Examples \u00b6 # Promote a paused rollout kubectl argo rollouts promote guestbook # Fully promote a rollout to desired version, skipping analysis, pauses, and steps kubectl argo rollouts promote guestbook --full Options \u00b6 --full Perform a full promotion, skipping analysis, pauses, and steps -h, --help help for promote Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Promote"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/#rollouts-promote","text":"Promote a rollout","title":"Rollouts Promote"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/#synopsis","text":"Promote a rollout Promotes a rollout paused at a canary step, or a paused blue-green pre-promotion. To skip analysis, pauses and steps entirely, use '--full' to fully promote the rollout kubectl argo rollouts promote ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/#examples","text":"# Promote a paused rollout kubectl argo rollouts promote guestbook # Fully promote a rollout to desired version, skipping analysis, pauses, and steps kubectl argo rollouts promote guestbook --full","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/#options","text":"--full Perform a full promotion, skipping analysis, pauses, and steps -h, --help help for promote","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_promote/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/","text":"Rollouts Restart \u00b6 Restart the pods of a rollout Synopsis \u00b6 Restart the pods of a rollout kubectl argo rollouts restart ROLLOUT [ flags ] Examples \u00b6 # Restart the pods of a rollout in now kubectl argo rollouts restart ROLLOUT_NAME # Restart the pods of a rollout in ten seconds kubectl argo rollouts restart ROLLOUT_NAME --in 10s Options \u00b6 -h, --help help for restart -i, --in string Amount of time before a restart. (e.g. 30s, 5m, 1h) Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Restart"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/#rollouts-restart","text":"Restart the pods of a rollout","title":"Rollouts Restart"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/#synopsis","text":"Restart the pods of a rollout kubectl argo rollouts restart ROLLOUT [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/#examples","text":"# Restart the pods of a rollout in now kubectl argo rollouts restart ROLLOUT_NAME # Restart the pods of a rollout in ten seconds kubectl argo rollouts restart ROLLOUT_NAME --in 10s","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/#options","text":"-h, --help help for restart -i, --in string Amount of time before a restart. (e.g. 30s, 5m, 1h)","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_restart/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/","text":"Rollouts Retry \u00b6 Retry a rollout or experiment Synopsis \u00b6 This command consists of multiple subcommands which can be used to restart an aborted rollout or a failed experiment. kubectl argo rollouts retry <rollout | experiment> RESOURCE_NAME [ flags ] Examples \u00b6 # Retry an aborted rollout kubectl argo rollouts retry rollout guestbook # Retry a failed experiment kubectl argo rollouts retry experiment my-experiment Options \u00b6 -h, --help help for retry Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts retry experiment - Retry an experiment rollouts retry rollout - Retry an aborted rollout See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Retry"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#rollouts-retry","text":"Retry a rollout or experiment","title":"Rollouts Retry"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#synopsis","text":"This command consists of multiple subcommands which can be used to restart an aborted rollout or a failed experiment. kubectl argo rollouts retry <rollout | experiment> RESOURCE_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#examples","text":"# Retry an aborted rollout kubectl argo rollouts retry rollout guestbook # Retry a failed experiment kubectl argo rollouts retry experiment my-experiment","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#options","text":"-h, --help help for retry","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#available-commands","text":"rollouts retry experiment - Retry an experiment rollouts retry rollout - Retry an aborted rollout","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/","text":"Rollouts Retry Experiment \u00b6 Retry an experiment Synopsis \u00b6 Retry a failed experiment. kubectl argo rollouts retry experiment EXPERIMENT_NAME [ flags ] Examples \u00b6 # Retry an experiment kubectl argo rollouts retry experiment my-experiment Options \u00b6 -h, --help help for experiment Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts retry - Retry a rollout or experiment","title":"Rollouts Retry Experiment"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/#rollouts-retry-experiment","text":"Retry an experiment","title":"Rollouts Retry Experiment"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/#synopsis","text":"Retry a failed experiment. kubectl argo rollouts retry experiment EXPERIMENT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/#examples","text":"# Retry an experiment kubectl argo rollouts retry experiment my-experiment","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/#options","text":"-h, --help help for experiment","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_experiment/#see-also","text":"rollouts retry - Retry a rollout or experiment","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/","text":"Rollouts Retry Rollout \u00b6 Retry an aborted rollout Synopsis \u00b6 Retry an aborted rollout kubectl argo rollouts retry rollout ROLLOUT_NAME [ flags ] Examples \u00b6 # Retry an aborted rollout kubectl argo rollouts retry rollout guestbook Options \u00b6 -h, --help help for rollout Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts retry - Retry a rollout or experiment","title":"Rollouts Retry Rollout"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/#rollouts-retry-rollout","text":"Retry an aborted rollout","title":"Rollouts Retry Rollout"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/#synopsis","text":"Retry an aborted rollout kubectl argo rollouts retry rollout ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/#examples","text":"# Retry an aborted rollout kubectl argo rollouts retry rollout guestbook","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/#options","text":"-h, --help help for rollout","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_retry_rollout/#see-also","text":"rollouts retry - Retry a rollout or experiment","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/","text":"Rollouts Set \u00b6 Update various values on resources Synopsis \u00b6 This command consists of multiple subcommands which can be used to update rollout resources. kubectl argo rollouts set COMMAND [ flags ] Examples \u00b6 # Set rollout image kubectl argo rollouts set image my-rollout demo = argoproj/rollouts-demo:yellow Options \u00b6 -h, --help help for set Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts set image - Update the image of a rollout See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Set"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#rollouts-set","text":"Update various values on resources","title":"Rollouts Set"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#synopsis","text":"This command consists of multiple subcommands which can be used to update rollout resources. kubectl argo rollouts set COMMAND [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#examples","text":"# Set rollout image kubectl argo rollouts set image my-rollout demo = argoproj/rollouts-demo:yellow","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#options","text":"-h, --help help for set","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#available-commands","text":"rollouts set image - Update the image of a rollout","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/","text":"Rollouts Set Image \u00b6 Update the image of a rollout Synopsis \u00b6 Update the image of a rollout kubectl argo rollouts set image ROLLOUT_NAME CONTAINER = IMAGE [ flags ] Examples \u00b6 # Set rollout image kubectl argo rollouts set image my-rollout www = image:v2 Options \u00b6 -h, --help help for image Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts set - Update various values on resources","title":"Rollouts Set Image"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/#rollouts-set-image","text":"Update the image of a rollout","title":"Rollouts Set Image"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/#synopsis","text":"Update the image of a rollout kubectl argo rollouts set image ROLLOUT_NAME CONTAINER = IMAGE [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/#examples","text":"# Set rollout image kubectl argo rollouts set image my-rollout www = image:v2","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/#options","text":"-h, --help help for image","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_set_image/#see-also","text":"rollouts set - Update various values on resources","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/","text":"Rollouts Status \u00b6 Show the status of a rollout Synopsis \u00b6 Watch rollout until it finishes or the timeout is exceeded. Returns success if the rollout is healthy upon completion and an error otherwise. kubectl argo rollouts status ROLLOUT_NAME [ flags ] Examples \u00b6 # Watch the rollout until it succeeds kubectl argo rollouts status guestbook # Watch the rollout until it succeeds, fail if it takes more than 60 seconds kubectl argo rollouts status --timeout 60s guestbook Options \u00b6 -h, --help help for status -t, --timeout duration The length of time to watch before giving up. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). Zero means wait forever -w, --watch Watch the status of the rollout until it's done (default true) Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Status"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/#rollouts-status","text":"Show the status of a rollout","title":"Rollouts Status"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/#synopsis","text":"Watch rollout until it finishes or the timeout is exceeded. Returns success if the rollout is healthy upon completion and an error otherwise. kubectl argo rollouts status ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/#examples","text":"# Watch the rollout until it succeeds kubectl argo rollouts status guestbook # Watch the rollout until it succeeds, fail if it takes more than 60 seconds kubectl argo rollouts status --timeout 60s guestbook","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/#options","text":"-h, --help help for status -t, --timeout duration The length of time to watch before giving up. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). Zero means wait forever -w, --watch Watch the status of the rollout until it's done (default true)","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_status/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/","text":"Rollouts Terminate \u00b6 Terminate an AnalysisRun or Experiment Synopsis \u00b6 This command consists of multiple subcommands which can be used to terminate an AnalysisRun or Experiment that is in progress. kubectl argo rollouts terminate <analysisrun | experiment> RESOURCE_NAME [ flags ] Examples \u00b6 # Terminate an analysisRun kubectl argo rollouts terminate analysisrun guestbook-877894d5b-4-success-rate.1 # Terminate a failed experiment kubectl argo rollouts terminate experiment my-experiment Options \u00b6 -h, --help help for terminate Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use Available Commands \u00b6 rollouts terminate analysisrun - Terminate an AnalysisRun rollouts terminate experiment - Terminate an experiment See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Terminate"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#rollouts-terminate","text":"Terminate an AnalysisRun or Experiment","title":"Rollouts Terminate"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#synopsis","text":"This command consists of multiple subcommands which can be used to terminate an AnalysisRun or Experiment that is in progress. kubectl argo rollouts terminate <analysisrun | experiment> RESOURCE_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#examples","text":"# Terminate an analysisRun kubectl argo rollouts terminate analysisrun guestbook-877894d5b-4-success-rate.1 # Terminate a failed experiment kubectl argo rollouts terminate experiment my-experiment","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#options","text":"-h, --help help for terminate","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#available-commands","text":"rollouts terminate analysisrun - Terminate an AnalysisRun rollouts terminate experiment - Terminate an experiment","title":"Available Commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/","text":"Rollouts Terminate Analysisrun \u00b6 Terminate an AnalysisRun Synopsis \u00b6 This command terminates an AnalysisRun. kubectl argo rollouts terminate analysisrun ANALYSISRUN_NAME [ flags ] Examples \u00b6 # Terminate an AnalysisRun kubectl argo rollouts terminate analysis guestbook-877894d5b-4-success-rate.1 Options \u00b6 -h, --help help for analysisrun Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts terminate - Terminate an AnalysisRun or Experiment","title":"Rollouts Terminate Analysisrun"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/#rollouts-terminate-analysisrun","text":"Terminate an AnalysisRun","title":"Rollouts Terminate Analysisrun"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/#synopsis","text":"This command terminates an AnalysisRun. kubectl argo rollouts terminate analysisrun ANALYSISRUN_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/#examples","text":"# Terminate an AnalysisRun kubectl argo rollouts terminate analysis guestbook-877894d5b-4-success-rate.1","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/#options","text":"-h, --help help for analysisrun","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_analysisrun/#see-also","text":"rollouts terminate - Terminate an AnalysisRun or Experiment","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/","text":"Rollouts Terminate Experiment \u00b6 Terminate an experiment Synopsis \u00b6 This command terminates an Experiment. kubectl argo rollouts terminate experiment EXPERIMENT_NAME [ flags ] Examples \u00b6 # Terminate an experiment kubectl argo rollouts terminate experiment my-experiment Options \u00b6 -h, --help help for experiment Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts terminate - Terminate an AnalysisRun or Experiment","title":"Rollouts Terminate Experiment"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/#rollouts-terminate-experiment","text":"Terminate an experiment","title":"Rollouts Terminate Experiment"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/#synopsis","text":"This command terminates an Experiment. kubectl argo rollouts terminate experiment EXPERIMENT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/#examples","text":"# Terminate an experiment kubectl argo rollouts terminate experiment my-experiment","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/#options","text":"-h, --help help for experiment","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_terminate_experiment/#see-also","text":"rollouts terminate - Terminate an AnalysisRun or Experiment","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/","text":"Rollouts Undo \u00b6 Undo a rollout Synopsis \u00b6 Rollback to the previous rollout. kubectl argo rollouts undo ROLLOUT_NAME [ flags ] Examples \u00b6 # Undo a rollout kubectl argo rollouts undo guestbook # Undo a rollout revision 3 kubectl argo rollouts undo guestbook --to-revision = 3 Options \u00b6 -h, --help help for undo --to-revision int The revision to rollback to. Default to 0 (last revision). Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Undo"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/#rollouts-undo","text":"Undo a rollout","title":"Rollouts Undo"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/#synopsis","text":"Rollback to the previous rollout. kubectl argo rollouts undo ROLLOUT_NAME [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/#examples","text":"# Undo a rollout kubectl argo rollouts undo guestbook # Undo a rollout revision 3 kubectl argo rollouts undo guestbook --to-revision = 3","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/#options","text":"-h, --help help for undo --to-revision int The revision to rollback to. Default to 0 (last revision).","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_undo/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/","text":"Rollouts Version \u00b6 Print version Synopsis \u00b6 Show the version and build information of the Argo Rollouts plugin. kubectl argo rollouts version [ flags ] Examples \u00b6 # Get full version info kubectl argo rollouts version # Get just plugin version number kubectl argo rollouts version --short Options \u00b6 -h, --help help for version --short print just the version number Options inherited from parent commands \u00b6 --as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use See Also \u00b6 rollouts - Manage argo rollouts","title":"Rollouts Version"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/#rollouts-version","text":"Print version","title":"Rollouts Version"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/#synopsis","text":"Show the version and build information of the Argo Rollouts plugin. kubectl argo rollouts version [ flags ]","title":"Synopsis"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/#examples","text":"# Get full version info kubectl argo rollouts version # Get just plugin version number kubectl argo rollouts version --short","title":"Examples"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/#options","text":"-h, --help help for version --short print just the version number","title":"Options"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/#options-inherited-from-parent-commands","text":"--as string Username to impersonate for the operation --as-group stringArray Group to impersonate for the operation, this flag can be repeated to specify multiple groups. --cache-dir string Default cache directory (default \"/home/runner/.kube/cache\") --certificate-authority string Path to a cert file for the certificate authority --client-certificate string Path to a client certificate file for TLS --client-key string Path to a client key file for TLS --cluster string The name of the kubeconfig cluster to use --context string The name of the kubeconfig context to use --insecure-skip-tls-verify If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure -v, --kloglevel int Log level for kubernetes client library --kubeconfig string Path to the kubeconfig file to use for CLI requests. --loglevel string Log level for kubectl argo rollouts (default \"info\") -n, --namespace string If present, the namespace scope for this CLI request --request-timeout string The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\") -s, --server string The address and port of the Kubernetes API server --tls-server-name string Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used --token string Bearer token for authentication to the API server --user string The name of the kubeconfig user to use","title":"Options inherited from parent commands"},{"location":"generated/kubectl-argo-rollouts/kubectl-argo-rollouts_version/#see-also","text":"rollouts - Manage argo rollouts","title":"See Also"},{"location":"generated/notification-services/alertmanager/","text":"Alertmanager \u00b6 Parameters \u00b6 The notification service is used to push events to Alertmanager , and the following settings need to be specified: targets - the alertmanager service address, array type scheme - optional, default is \"http\", e.g. http or https apiPath - optional, default is \"/api/v2/alerts\" insecureSkipVerify - optional, default is \"false\", when scheme is https whether to skip the verification of ca basicAuth - optional, server auth bearerToken - optional, server auth timeout - optional, the timeout in seconds used when sending alerts, default is \"3 seconds\" basicAuth or bearerToken is used for authentication, you can choose one. If the two are set at the same time, basicAuth takes precedence over bearerToken . Example \u00b6 Prometheus Alertmanager config \u00b6 global : resolve_timeout : 5m route : group_by : [ 'alertname' ] group_wait : 10s group_interval : 10s repeat_interval : 1h receiver : 'default' receivers : - name : 'default' webhook_configs : - send_resolved : false url : 'http://10.5.39.39:10080/api/alerts/webhook' You should turn off \"send_resolved\" or you will receive unnecessary recovery notifications after \"resolve_timeout\". Send one alertmanager without auth \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:9093 Send alertmanager cluster with custom api path \u00b6 If your alertmanager has changed the default api, you can customize \"apiPath\". apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:443 scheme: https apiPath: /api/events insecureSkipVerify: true Send high availability alertmanager with auth \u00b6 Store auth token in argocd-notifications-secret Secret and use configure in argocd-notifications-cm ConfigMap. apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : alertmanager-username : <username> alertmanager-password : <password> alertmanager-bearer-token : <token> with basicAuth apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:19093 - 10.5.39.39:29093 - 10.5.39.39:39093 scheme: https apiPath: /api/v2/alerts insecureSkipVerify: true basicAuth: username: $alertmanager-username password: $alertmanager-password with bearerToken apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:19093 - 10.5.39.39:29093 - 10.5.39.39:39093 scheme: https apiPath: /api/v2/alerts insecureSkipVerify: true bearerToken: $alertmanager-bearer-token Templates \u00b6 labels - at least one label pair required, implement different notification strategies according to alertmanager routing annotations - optional, specifies a set of information labels, which can be used to store longer additional information, but only for display generatorURL - optional, default is '{{.app.spec.source.repoURL}}', backlink used to identify the entity that caused this alert in the client the label or annotations or generatorURL values can be templated. context : | argocdUrl: https://example.com/argocd template.app-deployed : | message: Application {{.app.metadata.name}} has been healthy. alertmanager: labels: fault_priority: \"P5\" event_bucket: \"deploy\" event_status: \"succeed\" recipient: \"{{.recipient}}\" annotations: application: '<a href=\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\">{{.app.metadata.name}}</a>' author: \"{{(call .repo.GetCommitMetadata .app.status.sync.revision).Author}}\" message: \"{{(call .repo.GetCommitMetadata .app.status.sync.revision).Message}}\" You can do targeted push on Alertmanager according to labels. template.app-deployed : | message: Application {{.app.metadata.name}} has been healthy. alertmanager: labels: alertname: app-deployed fault_priority: \"P5\" event_bucket: \"deploy\" There is a special label alertname . If you don\u2019t set its value, it will be equal to the template name by default.","title":"Alertmanager"},{"location":"generated/notification-services/alertmanager/#alertmanager","text":"","title":"Alertmanager"},{"location":"generated/notification-services/alertmanager/#parameters","text":"The notification service is used to push events to Alertmanager , and the following settings need to be specified: targets - the alertmanager service address, array type scheme - optional, default is \"http\", e.g. http or https apiPath - optional, default is \"/api/v2/alerts\" insecureSkipVerify - optional, default is \"false\", when scheme is https whether to skip the verification of ca basicAuth - optional, server auth bearerToken - optional, server auth timeout - optional, the timeout in seconds used when sending alerts, default is \"3 seconds\" basicAuth or bearerToken is used for authentication, you can choose one. If the two are set at the same time, basicAuth takes precedence over bearerToken .","title":"Parameters"},{"location":"generated/notification-services/alertmanager/#example","text":"","title":"Example"},{"location":"generated/notification-services/alertmanager/#prometheus-alertmanager-config","text":"global : resolve_timeout : 5m route : group_by : [ 'alertname' ] group_wait : 10s group_interval : 10s repeat_interval : 1h receiver : 'default' receivers : - name : 'default' webhook_configs : - send_resolved : false url : 'http://10.5.39.39:10080/api/alerts/webhook' You should turn off \"send_resolved\" or you will receive unnecessary recovery notifications after \"resolve_timeout\".","title":"Prometheus Alertmanager config"},{"location":"generated/notification-services/alertmanager/#send-one-alertmanager-without-auth","text":"apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:9093","title":"Send one alertmanager without auth"},{"location":"generated/notification-services/alertmanager/#send-alertmanager-cluster-with-custom-api-path","text":"If your alertmanager has changed the default api, you can customize \"apiPath\". apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:443 scheme: https apiPath: /api/events insecureSkipVerify: true","title":"Send alertmanager cluster with custom api path"},{"location":"generated/notification-services/alertmanager/#send-high-availability-alertmanager-with-auth","text":"Store auth token in argocd-notifications-secret Secret and use configure in argocd-notifications-cm ConfigMap. apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : alertmanager-username : <username> alertmanager-password : <password> alertmanager-bearer-token : <token> with basicAuth apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:19093 - 10.5.39.39:29093 - 10.5.39.39:39093 scheme: https apiPath: /api/v2/alerts insecureSkipVerify: true basicAuth: username: $alertmanager-username password: $alertmanager-password with bearerToken apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.alertmanager : | targets: - 10.5.39.39:19093 - 10.5.39.39:29093 - 10.5.39.39:39093 scheme: https apiPath: /api/v2/alerts insecureSkipVerify: true bearerToken: $alertmanager-bearer-token","title":"Send high availability alertmanager with auth"},{"location":"generated/notification-services/alertmanager/#templates","text":"labels - at least one label pair required, implement different notification strategies according to alertmanager routing annotations - optional, specifies a set of information labels, which can be used to store longer additional information, but only for display generatorURL - optional, default is '{{.app.spec.source.repoURL}}', backlink used to identify the entity that caused this alert in the client the label or annotations or generatorURL values can be templated. context : | argocdUrl: https://example.com/argocd template.app-deployed : | message: Application {{.app.metadata.name}} has been healthy. alertmanager: labels: fault_priority: \"P5\" event_bucket: \"deploy\" event_status: \"succeed\" recipient: \"{{.recipient}}\" annotations: application: '<a href=\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\">{{.app.metadata.name}}</a>' author: \"{{(call .repo.GetCommitMetadata .app.status.sync.revision).Author}}\" message: \"{{(call .repo.GetCommitMetadata .app.status.sync.revision).Message}}\" You can do targeted push on Alertmanager according to labels. template.app-deployed : | message: Application {{.app.metadata.name}} has been healthy. alertmanager: labels: alertname: app-deployed fault_priority: \"P5\" event_bucket: \"deploy\" There is a special label alertname . If you don\u2019t set its value, it will be equal to the template name by default.","title":"Templates"},{"location":"generated/notification-services/email/","text":"Email \u00b6 Parameters \u00b6 The Email notification service sends email notifications using SMTP protocol and requires specifying the following settings: host - the SMTP server host name port - the SMTP server port username - username password - password from - from email address html - optional bool, true or false insecure_skip_verify - optional bool, true or false Example \u00b6 The following snippet contains sample Gmail service configuration: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.email.gmail : | username: $email-username password: $email-password host: smtp.gmail.com port: 465 from: $email-username Without authentication: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.email.example : | host: smtp.example.com port: 587 from: $email-username Template \u00b6 Notification templates support specifying subject for email notifications: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : template.app-sync-succeeded : | email: subject: Application {{.app.metadata.name}} has been successfully synced. message: | {{if eq .serviceType \"slack\"}}:white_check_mark:{{end}} Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}. Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .","title":"Email"},{"location":"generated/notification-services/email/#email","text":"","title":"Email"},{"location":"generated/notification-services/email/#parameters","text":"The Email notification service sends email notifications using SMTP protocol and requires specifying the following settings: host - the SMTP server host name port - the SMTP server port username - username password - password from - from email address html - optional bool, true or false insecure_skip_verify - optional bool, true or false","title":"Parameters"},{"location":"generated/notification-services/email/#example","text":"The following snippet contains sample Gmail service configuration: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.email.gmail : | username: $email-username password: $email-password host: smtp.gmail.com port: 465 from: $email-username Without authentication: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.email.example : | host: smtp.example.com port: 587 from: $email-username","title":"Example"},{"location":"generated/notification-services/email/#template","text":"Notification templates support specifying subject for email notifications: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : template.app-sync-succeeded : | email: subject: Application {{.app.metadata.name}} has been successfully synced. message: | {{if eq .serviceType \"slack\"}}:white_check_mark:{{end}} Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}. Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .","title":"Template"},{"location":"generated/notification-services/github/","text":"GitHub \u00b6 Parameters \u00b6 The GitHub notification service changes commit status using GitHub Apps and requires specifying the following settings: appID - the app id installationID - the app installation id privateKey - the app private key enterpriseBaseURL - optional URL, e.g. https://git.example.com/ Configuration \u00b6 Create a GitHub Apps using https://github.com/settings/apps/new Change repository permissions to enable write commit statuses Generate a private key, and download it automatically Install app to account Store privateKey in argocd-notifications-secret Secret and configure GitHub integration in argocd-notifications-cm ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.github : | appID: <app-id> installationID: <installation-id> privateKey: $github-privateKey apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : github-privateKey : | -----BEGIN RSA PRIVATE KEY----- (snip) -----END RSA PRIVATE KEY----- Create subscription for your GitHub integration apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.github : \"\" Templates \u00b6 If the message is set to 140 characters or more, it will be truncate. template.app-deployed : | message: | Application {{.app.metadata.name}} is now running new version of deployments manifests. github: status: state: success label: \"continuous-delivery/{{.app.metadata.name}}\" targetURL: \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\"","title":"GitHub"},{"location":"generated/notification-services/github/#github","text":"","title":"GitHub"},{"location":"generated/notification-services/github/#parameters","text":"The GitHub notification service changes commit status using GitHub Apps and requires specifying the following settings: appID - the app id installationID - the app installation id privateKey - the app private key enterpriseBaseURL - optional URL, e.g. https://git.example.com/","title":"Parameters"},{"location":"generated/notification-services/github/#configuration","text":"Create a GitHub Apps using https://github.com/settings/apps/new Change repository permissions to enable write commit statuses Generate a private key, and download it automatically Install app to account Store privateKey in argocd-notifications-secret Secret and configure GitHub integration in argocd-notifications-cm ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.github : | appID: <app-id> installationID: <installation-id> privateKey: $github-privateKey apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : github-privateKey : | -----BEGIN RSA PRIVATE KEY----- (snip) -----END RSA PRIVATE KEY----- Create subscription for your GitHub integration apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.github : \"\"","title":"Configuration"},{"location":"generated/notification-services/github/#templates","text":"If the message is set to 140 characters or more, it will be truncate. template.app-deployed : | message: | Application {{.app.metadata.name}} is now running new version of deployments manifests. github: status: state: success label: \"continuous-delivery/{{.app.metadata.name}}\" targetURL: \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\"","title":"Templates"},{"location":"generated/notification-services/googlechat/","text":"Google Chat \u00b6 Parameters \u00b6 The Google Chat notification service send message notifications to a google chat webhook. This service uses the following settings: webhooks - a map of the form webhookName: webhookUrl Configuration \u00b6 Open Google chat and go to the space to which you want to send messages From the menu at the top of the page, select Configure Webhooks Under Incoming Webhooks , click Add Webhook Give a name to the webhook, optionally add an image and click Save Copy the URL next to your webhook Store the URL in argocd-notification-secret and declare it in argocd-notifications-cm apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.googlechat : | webhooks: spaceName: $space-webhook-url apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : space-webhook-url : https://chat.googleapis.com/v1/spaces/<space_id>/messages?key=<key>&token=<token> Create a subscription for your space apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.googlechat : spaceName Templates \u00b6 You can send simple text or card messages to a Google Chat space. A simple text message template can be defined as follows: template.app-sync-succeeded : | message: The app {{ .app.metadata.name }} has succesfully synced! A card message can be defined as follows: template.app-sync-succeeded : | googlechat: cards: | - header: title: ArgoCD Bot Notification sections: - widgets: - textParagraph: text: The app {{ .app.metadata.name }} has succesfully synced! - widgets: - keyValue: topLabel: Repository content: {{ call .repo.RepoURLToHTTPS .app.spec.source.repoURL }} - keyValue: topLabel: Revision content: {{ .app.spec.source.targetRevision }} - keyValue: topLabel: Author content: {{ (call .repo.GetCommitMetadata .app.status.sync.revision).Author }} The card message can be written in JSON too.","title":"Google Chat"},{"location":"generated/notification-services/googlechat/#google-chat","text":"","title":"Google Chat"},{"location":"generated/notification-services/googlechat/#parameters","text":"The Google Chat notification service send message notifications to a google chat webhook. This service uses the following settings: webhooks - a map of the form webhookName: webhookUrl","title":"Parameters"},{"location":"generated/notification-services/googlechat/#configuration","text":"Open Google chat and go to the space to which you want to send messages From the menu at the top of the page, select Configure Webhooks Under Incoming Webhooks , click Add Webhook Give a name to the webhook, optionally add an image and click Save Copy the URL next to your webhook Store the URL in argocd-notification-secret and declare it in argocd-notifications-cm apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.googlechat : | webhooks: spaceName: $space-webhook-url apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : space-webhook-url : https://chat.googleapis.com/v1/spaces/<space_id>/messages?key=<key>&token=<token> Create a subscription for your space apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.googlechat : spaceName","title":"Configuration"},{"location":"generated/notification-services/googlechat/#templates","text":"You can send simple text or card messages to a Google Chat space. A simple text message template can be defined as follows: template.app-sync-succeeded : | message: The app {{ .app.metadata.name }} has succesfully synced! A card message can be defined as follows: template.app-sync-succeeded : | googlechat: cards: | - header: title: ArgoCD Bot Notification sections: - widgets: - textParagraph: text: The app {{ .app.metadata.name }} has succesfully synced! - widgets: - keyValue: topLabel: Repository content: {{ call .repo.RepoURLToHTTPS .app.spec.source.repoURL }} - keyValue: topLabel: Revision content: {{ .app.spec.source.targetRevision }} - keyValue: topLabel: Author content: {{ (call .repo.GetCommitMetadata .app.status.sync.revision).Author }} The card message can be written in JSON too.","title":"Templates"},{"location":"generated/notification-services/grafana/","text":"Grafana \u00b6 To be able to create Grafana annotation with argocd-notifications you have to create an API Key inside your Grafana . Login to your Grafana instance as admin On the left menu, go to Configuration / API Keys Click \"Add API Key\" Fill the Key with name ArgoCD Notification , role Editor and Time to Live 10y (for example) Click on Add button Store apiKey in argocd-notifications-secret Secret and Copy your API Key and define it in argocd-notifications-cm ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.grafana : | apiUrl: https://grafana.example.com/api apiKey: $grafana-api-key apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : grafana-api-key : api-key Create subscription for your Grafana integration apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.grafana : tag1|tag2 # list of tags separated with | Change the annotations settings","title":"Grafana"},{"location":"generated/notification-services/grafana/#grafana","text":"To be able to create Grafana annotation with argocd-notifications you have to create an API Key inside your Grafana . Login to your Grafana instance as admin On the left menu, go to Configuration / API Keys Click \"Add API Key\" Fill the Key with name ArgoCD Notification , role Editor and Time to Live 10y (for example) Click on Add button Store apiKey in argocd-notifications-secret Secret and Copy your API Key and define it in argocd-notifications-cm ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.grafana : | apiUrl: https://grafana.example.com/api apiKey: $grafana-api-key apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : grafana-api-key : api-key Create subscription for your Grafana integration apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.grafana : tag1|tag2 # list of tags separated with | Change the annotations settings","title":"Grafana"},{"location":"generated/notification-services/mattermost/","text":"Mattermost \u00b6 Parameters \u00b6 apiURL - the server url, e.g. https://mattermost.example.com token - the bot token insecureSkipVerify - optional bool, true or false Configuration \u00b6 Create a bot account and copy token after creating it Invite team Store token in argocd-notifications-secret Secret and configure Mattermost integration in argocd-notifications-cm ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.mattermost : | apiURL: <api-url> token: $mattermost-token apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : mattermost-token : token Copy channel id Create subscription for your Mattermost integration apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.mattermost : <channel-id> Templates \u00b6 You can reuse the template of slack. Mattermost is compatible with attachments of Slack. See Mattermost Integration Guide . template.app-deployed : | message: | Application {{.app.metadata.name}} is now running new version of deployments manifests. mattermost: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }]","title":"Mattermost"},{"location":"generated/notification-services/mattermost/#mattermost","text":"","title":"Mattermost"},{"location":"generated/notification-services/mattermost/#parameters","text":"apiURL - the server url, e.g. https://mattermost.example.com token - the bot token insecureSkipVerify - optional bool, true or false","title":"Parameters"},{"location":"generated/notification-services/mattermost/#configuration","text":"Create a bot account and copy token after creating it Invite team Store token in argocd-notifications-secret Secret and configure Mattermost integration in argocd-notifications-cm ConfigMap apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.mattermost : | apiURL: <api-url> token: $mattermost-token apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : mattermost-token : token Copy channel id Create subscription for your Mattermost integration apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.mattermost : <channel-id>","title":"Configuration"},{"location":"generated/notification-services/mattermost/#templates","text":"You can reuse the template of slack. Mattermost is compatible with attachments of Slack. See Mattermost Integration Guide . template.app-deployed : | message: | Application {{.app.metadata.name}} is now running new version of deployments manifests. mattermost: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }]","title":"Templates"},{"location":"generated/notification-services/opsgenie/","text":"Opsgenie \u00b6 To be able to send notifications with argocd-notifications you have to create an API Integration inside your Opsgenie Team . Login to Opsgenie at https://app.opsgenie.com or https://app.eu.opsgenie.com (if you have an account in the european union) Make sure you already have a team, if not follow this guide https://docs.opsgenie.com/docs/teams Click \"Teams\" in the Menu on the left Select the team that you want to notify In the teams configuration menu select \"Integrations\" click \"Add Integration\" in the top right corner Select \"API\" integration Give your integration a name, copy the \"API key\" and safe it somewhere for later Make sure the checkboxes for \"Create and Update Access\" and \"enable\" are selected, disable the other checkboxes to remove unnecessary permissions Click \"Safe Integration\" at the bottom Check your browser for the correct server apiURL. If it is \"app.opsgenie.com\" then use the us/international api url api.opsgenie.com in the next step, otherwise use api.eu.opsgenie.com (european api). You are finished with configuring opsgenie. Now you need to configure argocd-notifications. Use the apiUrl, the team name and the apiKey to configure the opsgenie integration in the argocd-notifications-secret secret. apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.opsgenie : | apiUrl: <api-url> apiKeys: <your-team>: <integration-api-key>","title":"Opsgenie"},{"location":"generated/notification-services/opsgenie/#opsgenie","text":"To be able to send notifications with argocd-notifications you have to create an API Integration inside your Opsgenie Team . Login to Opsgenie at https://app.opsgenie.com or https://app.eu.opsgenie.com (if you have an account in the european union) Make sure you already have a team, if not follow this guide https://docs.opsgenie.com/docs/teams Click \"Teams\" in the Menu on the left Select the team that you want to notify In the teams configuration menu select \"Integrations\" click \"Add Integration\" in the top right corner Select \"API\" integration Give your integration a name, copy the \"API key\" and safe it somewhere for later Make sure the checkboxes for \"Create and Update Access\" and \"enable\" are selected, disable the other checkboxes to remove unnecessary permissions Click \"Safe Integration\" at the bottom Check your browser for the correct server apiURL. If it is \"app.opsgenie.com\" then use the us/international api url api.opsgenie.com in the next step, otherwise use api.eu.opsgenie.com (european api). You are finished with configuring opsgenie. Now you need to configure argocd-notifications. Use the apiUrl, the team name and the apiKey to configure the opsgenie integration in the argocd-notifications-secret secret. apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.opsgenie : | apiUrl: <api-url> apiKeys: <your-team>: <integration-api-key>","title":"Opsgenie"},{"location":"generated/notification-services/overview/","text":"The notification services represent integration with services such as slack, email or custom webhook. Services are configured in argocd-notifications-cm ConfigMap using service.<type>.(<custom-name>) keys and might reference sensitive data from argocd-notifications-secret Secret. Following example demonstrates slack service configuration: service.slack : | token: $slack-token The slack indicates that service sends slack notification; name is missing and defaults to slack . Sensitive Data \u00b6 Sensitive data like authentication tokens should be stored in <secret-name> Secret and can be referenced in service configuration using $<secret-key> format. For example $slack-token referencing value of key slack-token in <secret-name> Secret. Custom Names \u00b6 Service custom names allow configuring two instances of the same service type. service.slack.workspace1 : | token: $slack-token-workspace1 service.slack.workspace2 : | token: $slack-token-workspace2 apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.workspace1 : my-channel notifications.argoproj.io/subscribe.on-sync-succeeded.workspace2 : my-channel Service Types \u00b6 Email GitHub Slack Mattermost Opsgenie Grafana Webhook Telegram Teams Google Chat Rocket.Chat Pushover Alertmanager","title":"Overview"},{"location":"generated/notification-services/overview/#sensitive-data","text":"Sensitive data like authentication tokens should be stored in <secret-name> Secret and can be referenced in service configuration using $<secret-key> format. For example $slack-token referencing value of key slack-token in <secret-name> Secret.","title":"Sensitive Data"},{"location":"generated/notification-services/overview/#custom-names","text":"Service custom names allow configuring two instances of the same service type. service.slack.workspace1 : | token: $slack-token-workspace1 service.slack.workspace2 : | token: $slack-token-workspace2 apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.workspace1 : my-channel notifications.argoproj.io/subscribe.on-sync-succeeded.workspace2 : my-channel","title":"Custom Names"},{"location":"generated/notification-services/overview/#service-types","text":"Email GitHub Slack Mattermost Opsgenie Grafana Webhook Telegram Teams Google Chat Rocket.Chat Pushover Alertmanager","title":"Service Types"},{"location":"generated/notification-services/pushover/","text":"Pushover \u00b6 Create an app at pushover.net . Store the API key in <secret-name> Secret and define the secret name in <config-map-name> ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.pushover : | token: $pushover-token apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : pushover-token : avtc41pn13asmra6zaiyf7dh6cgx97 Add your user key to your Application resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.pushover : uumy8u4owy7bgkapp6mc5mvhfsvpcd","title":"Pushover"},{"location":"generated/notification-services/pushover/#pushover","text":"Create an app at pushover.net . Store the API key in <secret-name> Secret and define the secret name in <config-map-name> ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.pushover : | token: $pushover-token apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : pushover-token : avtc41pn13asmra6zaiyf7dh6cgx97 Add your user key to your Application resource: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.pushover : uumy8u4owy7bgkapp6mc5mvhfsvpcd","title":"Pushover"},{"location":"generated/notification-services/rocketchat/","text":"Rocket.Chat \u00b6 Parameters \u00b6 The Rocket.Chat notification service configuration includes following settings: email - the Rocker.Chat user's email password - the Rocker.Chat user's password alias - optional alias that should be used to post message icon - optional message icon avatar - optional message avatar serverUrl - optional Rocket.Chat server url Configuration \u00b6 Login to your RocketChat instance Go to user management Add new user with bot role. Also note that Require password change checkbox mus be not checked Copy username and password that you was created for bot user Create a public or private channel, or a team, for this example my_channel Add your bot to this channel otherwise it won't work Store email and password in argocd_notifications-secret Secret apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : rocketchat-email : <email> rocketchat-password : <password> Finally, use these credentials to configure the RocketChat integration in the argocd-configmap config map: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.rocketchat : | email: $rocketchat-email password: $rocketchat-password Create a subscription for your Rocket.Chat integration: Note: channel, team or user must be prefixed with # or @ elsewhere we will be interpretative destination as a room ID apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.rocketchat : #my_channel Templates \u00b6 Notification templates can be customized with RocketChat attachments . Note: Attachments structure in Rocketchat is same with Slack attachments feature . The message attachments can be specified in attachments string fields under rocketchat field: template.app-sync-status : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. rocketchat: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }]","title":"Rocket.Chat"},{"location":"generated/notification-services/rocketchat/#rocketchat","text":"","title":"Rocket.Chat"},{"location":"generated/notification-services/rocketchat/#parameters","text":"The Rocket.Chat notification service configuration includes following settings: email - the Rocker.Chat user's email password - the Rocker.Chat user's password alias - optional alias that should be used to post message icon - optional message icon avatar - optional message avatar serverUrl - optional Rocket.Chat server url","title":"Parameters"},{"location":"generated/notification-services/rocketchat/#configuration","text":"Login to your RocketChat instance Go to user management Add new user with bot role. Also note that Require password change checkbox mus be not checked Copy username and password that you was created for bot user Create a public or private channel, or a team, for this example my_channel Add your bot to this channel otherwise it won't work Store email and password in argocd_notifications-secret Secret apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : rocketchat-email : <email> rocketchat-password : <password> Finally, use these credentials to configure the RocketChat integration in the argocd-configmap config map: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.rocketchat : | email: $rocketchat-email password: $rocketchat-password Create a subscription for your Rocket.Chat integration: Note: channel, team or user must be prefixed with # or @ elsewhere we will be interpretative destination as a room ID apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.rocketchat : #my_channel","title":"Configuration"},{"location":"generated/notification-services/rocketchat/#templates","text":"Notification templates can be customized with RocketChat attachments . Note: Attachments structure in Rocketchat is same with Slack attachments feature . The message attachments can be specified in attachments string fields under rocketchat field: template.app-sync-status : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. rocketchat: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }]","title":"Templates"},{"location":"generated/notification-services/slack/","text":"Slack \u00b6 If you want to send message using incoming webhook, you can use webhook . Parameters \u00b6 The Slack notification service configuration includes following settings: token - the app token apiURL - optional, the server url, e.g. https://example.com/api username - optional, the app username icon - optional, the app icon, e.g. :robot_face: or https://example.com/image.png insecureSkipVerify - optional bool, true or false Configuration \u00b6 Create Slack Application using https://api.slack.com/apps?new_app=1 Once application is created navigate to Enter OAuth & Permissions Click Permissions under Add features and functionality section and add chat:write scope. To use the optional username and icon overrides in the Slack notification service also add the chat:write.customize scope. Scroll back to the top, click 'Install App to Workspace' button and confirm the installation. Once installation is completed copy the OAuth token. Create a public or private channel, for this example my_channel Invite your slack bot to this channel otherwise slack bot won't be able to deliver notifications to this channel Store Oauth access token in argocd-notifications-secret secret apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : slack-token : <Oauth-access-token> Define service type slack in data section of argocd-notifications-cm configmap: service apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.slack : | token: $slack-token Add annotation in application yaml file to enable notifications for specific argocd app apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.slack : my_channel Templates \u00b6 Notification templates can be customized to leverage slack message blocks and attachments feature . The message blocks and attachments can be specified in blocks and attachments string fields under slack field: template.app-sync-status : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. slack: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] The messages can be aggregated to the slack threads by grouping key which can be specified in a groupingKey string field under slack field. groupingKey is used across each template and works independently on each slack channel. When multiple applications will be updated at the same time or frequently, the messages in slack channel can be easily read by aggregating with git commit hash, application name, etc. Furthermore, the messages can be broadcast to the channel at the specific template by notifyBroadcast field. template.app-sync-status : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. slack: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] # Aggregate the messages to the thread by git commit hash groupingKey: \"{{.app.status.sync.revision}}\" notifyBroadcast: false template.app-sync-failed : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. slack: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#ff0000\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] # Aggregate the messages to the thread by git commit hash groupingKey: \"{{.app.status.sync.revision}}\" notifyBroadcast: true","title":"Slack"},{"location":"generated/notification-services/slack/#slack","text":"If you want to send message using incoming webhook, you can use webhook .","title":"Slack"},{"location":"generated/notification-services/slack/#parameters","text":"The Slack notification service configuration includes following settings: token - the app token apiURL - optional, the server url, e.g. https://example.com/api username - optional, the app username icon - optional, the app icon, e.g. :robot_face: or https://example.com/image.png insecureSkipVerify - optional bool, true or false","title":"Parameters"},{"location":"generated/notification-services/slack/#configuration","text":"Create Slack Application using https://api.slack.com/apps?new_app=1 Once application is created navigate to Enter OAuth & Permissions Click Permissions under Add features and functionality section and add chat:write scope. To use the optional username and icon overrides in the Slack notification service also add the chat:write.customize scope. Scroll back to the top, click 'Install App to Workspace' button and confirm the installation. Once installation is completed copy the OAuth token. Create a public or private channel, for this example my_channel Invite your slack bot to this channel otherwise slack bot won't be able to deliver notifications to this channel Store Oauth access token in argocd-notifications-secret secret apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : slack-token : <Oauth-access-token> Define service type slack in data section of argocd-notifications-cm configmap: service apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.slack : | token: $slack-token Add annotation in application yaml file to enable notifications for specific argocd app apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.slack : my_channel","title":"Configuration"},{"location":"generated/notification-services/slack/#templates","text":"Notification templates can be customized to leverage slack message blocks and attachments feature . The message blocks and attachments can be specified in blocks and attachments string fields under slack field: template.app-sync-status : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. slack: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] The messages can be aggregated to the slack threads by grouping key which can be specified in a groupingKey string field under slack field. groupingKey is used across each template and works independently on each slack channel. When multiple applications will be updated at the same time or frequently, the messages in slack channel can be easily read by aggregating with git commit hash, application name, etc. Furthermore, the messages can be broadcast to the channel at the specific template by notifyBroadcast field. template.app-sync-status : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. slack: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] # Aggregate the messages to the thread by git commit hash groupingKey: \"{{.app.status.sync.revision}}\" notifyBroadcast: false template.app-sync-failed : | message: | Application {{.app.metadata.name}} sync is {{.app.status.sync.status}}. Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}. slack: attachments: | [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#ff0000\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] # Aggregate the messages to the thread by git commit hash groupingKey: \"{{.app.status.sync.revision}}\" notifyBroadcast: true","title":"Templates"},{"location":"generated/notification-services/teams/","text":"Teams \u00b6 Parameters \u00b6 The Teams notification service send message notifications using Teams bot and requires specifying the following settings: recipientUrls - the webhook url map, e.g. channelName: https://example.com Configuration \u00b6 Open Teams and goto Apps Find Incoming Webhook microsoft app and click on it Press Add to a team -> select team and channel -> press Set up a connector Enter webhook name and upload image (optional) Press Create then copy webhook url and store it in argocd-notifications-secret and define it in argocd-notifications-cm apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.teams : | recipientUrls: channelName: $channel-teams-url apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : channel-teams-url : https://example.com Create subscription for your Teams integration: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.teams : channelName Templates \u00b6 Notification templates can be customized to leverage teams message sections, facts, themeColor, summary and potentialAction feature . template.app-sync-succeeded : | teams: themeColor: \"#000080\" sections: | [{ \"facts\": [ { \"name\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\" }, { \"name\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\" } ] }] potentialAction: |- [{ \"@type\":\"OpenUri\", \"name\":\"Operation Details\", \"targets\":[{ \"os\":\"default\", \"uri\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\" }] }] title: Application {{.app.metadata.name}} has been successfully synced text: Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}. summary: \"{{.app.metadata.name}} sync succeeded\" facts field \u00b6 You can use facts field instead of sections field. template.app-sync-succeeded : | teams: facts: | [{ \"name\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\" }, { \"name\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\" }] theme color field \u00b6 You can set theme color as hex string for the message. template.app-sync-succeeded : | teams: themeColor: \"#000080\" summary field \u00b6 You can set a summary of the message that will be shown on Notifcation & Activity Feed template.app-sync-succeeded : | teams: summary: \"Sync Succeeded\"","title":"Teams"},{"location":"generated/notification-services/teams/#teams","text":"","title":"Teams"},{"location":"generated/notification-services/teams/#parameters","text":"The Teams notification service send message notifications using Teams bot and requires specifying the following settings: recipientUrls - the webhook url map, e.g. channelName: https://example.com","title":"Parameters"},{"location":"generated/notification-services/teams/#configuration","text":"Open Teams and goto Apps Find Incoming Webhook microsoft app and click on it Press Add to a team -> select team and channel -> press Set up a connector Enter webhook name and upload image (optional) Press Create then copy webhook url and store it in argocd-notifications-secret and define it in argocd-notifications-cm apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.teams : | recipientUrls: channelName: $channel-teams-url apiVersion : v1 kind : Secret metadata : name : <secret-name> stringData : channel-teams-url : https://example.com Create subscription for your Teams integration: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.teams : channelName","title":"Configuration"},{"location":"generated/notification-services/teams/#templates","text":"Notification templates can be customized to leverage teams message sections, facts, themeColor, summary and potentialAction feature . template.app-sync-succeeded : | teams: themeColor: \"#000080\" sections: | [{ \"facts\": [ { \"name\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\" }, { \"name\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\" } ] }] potentialAction: |- [{ \"@type\":\"OpenUri\", \"name\":\"Operation Details\", \"targets\":[{ \"os\":\"default\", \"uri\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true\" }] }] title: Application {{.app.metadata.name}} has been successfully synced text: Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}. summary: \"{{.app.metadata.name}} sync succeeded\"","title":"Templates"},{"location":"generated/notification-services/teams/#facts-field","text":"You can use facts field instead of sections field. template.app-sync-succeeded : | teams: facts: | [{ \"name\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\" }, { \"name\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\" }]","title":"facts field"},{"location":"generated/notification-services/teams/#theme-color-field","text":"You can set theme color as hex string for the message. template.app-sync-succeeded : | teams: themeColor: \"#000080\"","title":"theme color field"},{"location":"generated/notification-services/teams/#summary-field","text":"You can set a summary of the message that will be shown on Notifcation & Activity Feed template.app-sync-succeeded : | teams: summary: \"Sync Succeeded\"","title":"summary field"},{"location":"generated/notification-services/telegram/","text":"Telegram \u00b6 Get an API token using @Botfather . Store token in <secret-name> Secret and configure telegram integration in <config-map-name> ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.telegram : | token: $telegram-token Create new Telegram channel . Add your bot as an administrator. Use this channel username (public channel) or chatID (private channel) in the subscription for your Telegram integration: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.telegram : username apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.telegram : -1000000000000","title":"Telegram"},{"location":"generated/notification-services/telegram/#telegram","text":"Get an API token using @Botfather . Store token in <secret-name> Secret and configure telegram integration in <config-map-name> ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.telegram : | token: $telegram-token Create new Telegram channel . Add your bot as an administrator. Use this channel username (public channel) or chatID (private channel) in the subscription for your Telegram integration: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.telegram : username apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.on-sync-succeeded.telegram : -1000000000000","title":"Telegram"},{"location":"generated/notification-services/webhook/","text":"Configuration \u00b6 The webhook notification service allows sending a generic HTTP request using the templatized request body and URL. Using Webhook you might trigger a Jenkins job, update Github commit status. Use the following steps to configure webhook: 1 Register webhook in argocd-notifications-cm ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.<webhook-name> : | url: https://<hostname>/<optional-path> headers: #optional headers - name: <header-name> value: <header-value> basicAuth: #optional username password username: <username> password: <api-key> 2 Define template that customizes webhook request method, path and body: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : template.github-commit-status : | webhook: <webhook-name>: method: POST # one of: GET, POST, PUT, PATCH. Default value: GET path: <optional-path-template> body: | <optional-body-template> trigger.<trigger-name> : | - when: app.status.operationState.phase in ['Succeeded'] send: [github-commit-status] 3 Create subscription for webhook integration: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.<webhook-name> : \"\" Examples \u00b6 Set Github commit status \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.github : | url: https://api.github.com headers: #optional headers - name: Authorization value: token $github-token 2 Define template that customizes webhook request method, path and body: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.github : | url: https://api.github.com headers: #optional headers - name: Authorization value: token $github-token template.github-commit-status : | webhook: github: method: POST path: /repos/{{call .repo.FullNameByRepoURL .app.spec.source.repoURL}}/statuses/{{.app.status.operationState.operation.sync.revision}} body: | { {{if eq .app.status.operationState.phase \"Running\"}} \"state\": \"pending\"{{end}} {{if eq .app.status.operationState.phase \"Succeeded\"}} \"state\": \"success\"{{end}} {{if eq .app.status.operationState.phase \"Error\"}} \"state\": \"error\"{{end}} {{if eq .app.status.operationState.phase \"Failed\"}} \"state\": \"error\"{{end}}, \"description\": \"ArgoCD\", \"target_url\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"context\": \"continuous-delivery/{{.app.metadata.name}}\" } Start Jenkins Job \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.jenkins : | url: http://<jenkins-host>/job/<job-name>/build?token=<job-secret> basicAuth: username: <username> password: <api-key> type : Opaque Send form-data \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.form : | url: https://form.example.com headers: - name: Content-Type value: application/x-www-form-urlencoded template.form-data : | webhook: form: method: POST body: key1=value1&key2=value2 Send Slack \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.slack_webhook : | url: https://hooks.slack.com/services/xxxxx headers: - name: Content-Type value: application/json template.send-slack : | webhook: slack_webhook: method: POST body: | { \"attachments\": [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] }","title":"Webhook"},{"location":"generated/notification-services/webhook/#configuration","text":"The webhook notification service allows sending a generic HTTP request using the templatized request body and URL. Using Webhook you might trigger a Jenkins job, update Github commit status. Use the following steps to configure webhook: 1 Register webhook in argocd-notifications-cm ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.<webhook-name> : | url: https://<hostname>/<optional-path> headers: #optional headers - name: <header-name> value: <header-value> basicAuth: #optional username password username: <username> password: <api-key> 2 Define template that customizes webhook request method, path and body: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : template.github-commit-status : | webhook: <webhook-name>: method: POST # one of: GET, POST, PUT, PATCH. Default value: GET path: <optional-path-template> body: | <optional-body-template> trigger.<trigger-name> : | - when: app.status.operationState.phase in ['Succeeded'] send: [github-commit-status] 3 Create subscription for webhook integration: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : annotations : notifications.argoproj.io/subscribe.<trigger-name>.<webhook-name> : \"\"","title":"Configuration"},{"location":"generated/notification-services/webhook/#examples","text":"","title":"Examples"},{"location":"generated/notification-services/webhook/#set-github-commit-status","text":"apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.github : | url: https://api.github.com headers: #optional headers - name: Authorization value: token $github-token 2 Define template that customizes webhook request method, path and body: apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.github : | url: https://api.github.com headers: #optional headers - name: Authorization value: token $github-token template.github-commit-status : | webhook: github: method: POST path: /repos/{{call .repo.FullNameByRepoURL .app.spec.source.repoURL}}/statuses/{{.app.status.operationState.operation.sync.revision}} body: | { {{if eq .app.status.operationState.phase \"Running\"}} \"state\": \"pending\"{{end}} {{if eq .app.status.operationState.phase \"Succeeded\"}} \"state\": \"success\"{{end}} {{if eq .app.status.operationState.phase \"Error\"}} \"state\": \"error\"{{end}} {{if eq .app.status.operationState.phase \"Failed\"}} \"state\": \"error\"{{end}}, \"description\": \"ArgoCD\", \"target_url\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"context\": \"continuous-delivery/{{.app.metadata.name}}\" }","title":"Set Github commit status"},{"location":"generated/notification-services/webhook/#start-jenkins-job","text":"apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.jenkins : | url: http://<jenkins-host>/job/<job-name>/build?token=<job-secret> basicAuth: username: <username> password: <api-key> type : Opaque","title":"Start Jenkins Job"},{"location":"generated/notification-services/webhook/#send-form-data","text":"apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.form : | url: https://form.example.com headers: - name: Content-Type value: application/x-www-form-urlencoded template.form-data : | webhook: form: method: POST body: key1=value1&key2=value2","title":"Send form-data"},{"location":"generated/notification-services/webhook/#send-slack","text":"apiVersion : v1 kind : ConfigMap metadata : name : <config-map-name> data : service.webhook.slack_webhook : | url: https://hooks.slack.com/services/xxxxx headers: - name: Content-Type value: application/json template.send-slack : | webhook: slack_webhook: method: POST body: | { \"attachments\": [{ \"title\": \"{{.app.metadata.name}}\", \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\", \"color\": \"#18be52\", \"fields\": [{ \"title\": \"Sync Status\", \"value\": \"{{.app.status.sync.status}}\", \"short\": true }, { \"title\": \"Repository\", \"value\": \"{{.app.spec.source.repoURL}}\", \"short\": true }] }] }","title":"Send Slack"},{"location":"getting-started/alb/","text":"Getting Started - AWS Load Balancer Controller \u00b6 This guide covers how Argo Rollouts integrates with the AWS Load Balancer Controller for traffic shaping. This guide builds upon the concepts of the basic getting started guide . Requirements \u00b6 Kubernetes cluster with AWS ALB Ingress Controller installed Tip See the Load Balancer Controller Installation instructions on how to install the AWS Load Balancer Controller 1. Deploy the Rollout, Services, and Ingress \u00b6 When an AWS ALB Ingress is used as the traffic router, the Rollout canary strategy must define the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # canaryService and stableService are references to Services which the Rollout will modify # to target the canary ReplicaSet and stable ReplicaSet respectively (required). canaryService : rollouts-demo-canary stableService : rollouts-demo-stable trafficRouting : alb : # The referenced ingress will be injected with a custom action annotation, directing # the AWS Load Balancer Controller to split traffic between the canary and stable # Service, according to the desired traffic weight (required). ingress : rollouts-demo-ingress # Reference to a Service that the Ingress must target in one of the rules (optional). # If omitted, uses canary.stableService. rootService : rollouts-demo-root # Service port is the port which the Service listens on (required). servicePort : 443 ... The Ingress referenced by the Rollout must have a rule which matches one of Rollout services. This should be canary.trafficRouting.alb.rootService (if specified), otherwise the rollout will use canary.stableService . apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rollouts-demo-ingress annotations : kubernetes.io/ingress.class : alb spec : rules : - http : paths : - path : /* backend : # serviceName must match either: canary.trafficRouting.alb.rootService (if specified), # or canary.stableService (if rootService is omitted) serviceName : rollouts-demo-root # servicePort must be the value: use-annotation # This instructs AWS Load Balancer Controller to look to annotations on how to direct traffic servicePort : use-annotation During an update, the Ingress will be injected with a custom action annotation , which directs the ALB to splits traffic between the stable and canary Services referenced by the Rollout. In this example, those Services are named: rollouts-demo-stable and rollouts-demo-canary respectively. Run the following commands to deploy: A Rollout Three Services (root, stable, canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-root NodePort 10 .100.16.123 <none> 80 :30225/TCP 2m43s rollouts-demo-canary NodePort 10 .100.16.64 <none> 80 :30224/TCP 2m43s rollouts-demo-stable NodePort 10 .100.146.232 <none> 80 :31135/TCP 2m43s $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE rollouts-demo-ingress * b0548428-default-rolloutsd-6951-1972570952.ap-northeast-1.elb.amazonaws.com 80 6m36s kubectl argo rollouts get rollout rollouts-demo 2. Perform an update \u00b6 Update the rollout by changing the image, and wait for it to reach the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. To understand how this works, inspect the listener rules for the ALB. When looking at the listener rules, we see that the forward action weights have been modified by the controller to reflect the current weight of the canary. The controller has added rollouts-pod-template-hash selector to the Services and attached the same label to the Pods. Therefore, you can split the traffic by simply forwarding the requests to the Services according to the weights. As the Rollout progresses through steps, the forward action weights will be adjusted to match the current setWeight of the steps.","title":"AWS ALB"},{"location":"getting-started/alb/#getting-started-aws-load-balancer-controller","text":"This guide covers how Argo Rollouts integrates with the AWS Load Balancer Controller for traffic shaping. This guide builds upon the concepts of the basic getting started guide .","title":"Getting Started - AWS Load Balancer Controller"},{"location":"getting-started/alb/#requirements","text":"Kubernetes cluster with AWS ALB Ingress Controller installed Tip See the Load Balancer Controller Installation instructions on how to install the AWS Load Balancer Controller","title":"Requirements"},{"location":"getting-started/alb/#1-deploy-the-rollout-services-and-ingress","text":"When an AWS ALB Ingress is used as the traffic router, the Rollout canary strategy must define the following fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # canaryService and stableService are references to Services which the Rollout will modify # to target the canary ReplicaSet and stable ReplicaSet respectively (required). canaryService : rollouts-demo-canary stableService : rollouts-demo-stable trafficRouting : alb : # The referenced ingress will be injected with a custom action annotation, directing # the AWS Load Balancer Controller to split traffic between the canary and stable # Service, according to the desired traffic weight (required). ingress : rollouts-demo-ingress # Reference to a Service that the Ingress must target in one of the rules (optional). # If omitted, uses canary.stableService. rootService : rollouts-demo-root # Service port is the port which the Service listens on (required). servicePort : 443 ... The Ingress referenced by the Rollout must have a rule which matches one of Rollout services. This should be canary.trafficRouting.alb.rootService (if specified), otherwise the rollout will use canary.stableService . apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rollouts-demo-ingress annotations : kubernetes.io/ingress.class : alb spec : rules : - http : paths : - path : /* backend : # serviceName must match either: canary.trafficRouting.alb.rootService (if specified), # or canary.stableService (if rootService is omitted) serviceName : rollouts-demo-root # servicePort must be the value: use-annotation # This instructs AWS Load Balancer Controller to look to annotations on how to direct traffic servicePort : use-annotation During an update, the Ingress will be injected with a custom action annotation , which directs the ALB to splits traffic between the stable and canary Services referenced by the Rollout. In this example, those Services are named: rollouts-demo-stable and rollouts-demo-canary respectively. Run the following commands to deploy: A Rollout Three Services (root, stable, canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/alb/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-root NodePort 10 .100.16.123 <none> 80 :30225/TCP 2m43s rollouts-demo-canary NodePort 10 .100.16.64 <none> 80 :30224/TCP 2m43s rollouts-demo-stable NodePort 10 .100.146.232 <none> 80 :31135/TCP 2m43s $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE rollouts-demo-ingress * b0548428-default-rolloutsd-6951-1972570952.ap-northeast-1.elb.amazonaws.com 80 6m36s kubectl argo rollouts get rollout rollouts-demo","title":"1. Deploy the Rollout, Services, and Ingress"},{"location":"getting-started/alb/#2-perform-an-update","text":"Update the rollout by changing the image, and wait for it to reach the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. To understand how this works, inspect the listener rules for the ALB. When looking at the listener rules, we see that the forward action weights have been modified by the controller to reflect the current weight of the canary. The controller has added rollouts-pod-template-hash selector to the Services and attached the same label to the Pods. Therefore, you can split the traffic by simply forwarding the requests to the Services according to the weights. As the Rollout progresses through steps, the forward action weights will be adjusted to match the current setWeight of the steps.","title":"2. Perform an update"},{"location":"getting-started/ambassador/","text":"Argo Rollouts and Ambassador Quick Start \u00b6 This tutorial will walk you through the process of configuring Argo Rollouts to work with Ambassador to facilitate canary releases. All files used in this guide are available in the examples directory of this repository. Requirements \u00b6 Kubernetes cluster Argo-Rollouts installed in the cluster Note If using Ambassador Edge Stack or Emissary-ingress 2.0+, you will need to install Argo-Rollouts version v1.1+, and you will need to supply --ambassador-api-version x.getambassador.io/v3alpha1 to your argo-rollouts deployment. 1. Install and configure Ambassador Edge Stack \u00b6 If you don't have Ambassador in your cluster you can install it following the Edge Stack documentation . By default, Edge Stack routes via Kubernetes services. For best performance with canaries, we recommend you use endpoint routing. Enable endpoint routing on your cluster by saving the following configuration in a file called resolver.yaml : apiVersion: getambassador.io/v2 kind: KubernetesEndpointResolver metadata: name: endpoint Apply this configuration to your cluster: kubectl apply -f resolver.yaml . 2. Create the Kubernetes Services \u00b6 We'll create two Kubernetes services, named echo-stable and echo-canary . Save this configuration to the file echo-service.yaml . apiVersion: v1 kind: Service metadata: labels: app: echo name: echo-stable spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: echo --- apiVersion: v1 kind: Service metadata: labels: app: echo name: echo-canary spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: echo We'll also create an Edge Stack route to the services. Save the following configuration to a file called echo-mapping.yaml . apiVersion: getambassador.io/v2 kind: Mapping metadata: name: echo spec: prefix: /echo rewrite: /echo service: echo-stable:80 resolver: endpoint Apply both of these configurations to the Kubernetes cluster: kubectl apply -f echo-service.yaml kubectl apply -f echo-mapping.yaml 3. Deploy the Echo Service \u00b6 Create a Rollout resource and save it to a file called rollout.yaml . Note the trafficRouting attribute, which tells Argo to use Ambassador Edge Stack for routing. apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: echo-rollout spec: selector: matchLabels: app: echo template: metadata: labels: app: echo spec: containers: - image: hashicorp/http-echo args: - \"-text=VERSION 1\" - -listen=:8080 imagePullPolicy: Always name: echo-v1 ports: - containerPort: 8080 strategy: canary: stableService: echo-stable canaryService: echo-canary trafficRouting: ambassador: mappings: - echo steps: - setWeight: 30 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 100 - pause: {duration: 10} Apply the rollout to your cluster kubectl apply -f rollout.yaml . Note that no canary rollout will occur, as this is the first version of the service being deployed. 4. Test the service \u00b6 We'll now test that this rollout works as expected. Open a new terminal window. We'll use it to send requests to the cluster. Get the external IP address for Edge Stack: export AMBASSADOR_LB_ENDPOINT=$(kubectl -n ambassador get svc ambassador -o \"go-template={{range .status.loadBalancer.ingress}}{{or .ip .hostname}}{{end}}\") Send a request to the echo service: curl -Lk \"https://$AMBASSADOR_LB_ENDPOINT/echo/\" You should get a response of \"VERSION 1\". 5. Rollout a new version \u00b6 It's time to rollout a new version of the service. Update the echo container in the rollout.yaml to display \"VERSION 2\": apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: echo-rollout spec: selector: matchLabels: app: echo template: metadata: labels: app: echo spec: containers: - image: hashicorp/http-echo args: - \"-text=VERSION 2\" - -listen=:8080 imagePullPolicy: Always name: echo-v1 ports: - containerPort: 8080 strategy: canary: stableService: echo-stable canaryService: echo-canary trafficRouting: ambassador: mappings: - echo steps: - setWeight: 30 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 100 - pause: {duration: 10} Apply the rollout to the cluster by typing kubectl apply -f rollout.yaml . This will rollout a version 2 of the service by routing 30% of traffic to the service for 30 seconds, followed by 60% of traffic for another 30 seconds. You can monitor the status of your rollout at the command line: kubectl argo rollouts get rollout echo-rollout --watch Will display an output similar to the following: Name: echo-rollout Namespace: default Status: \u0965 Paused Message: CanaryPauseStep Strategy: Canary Step: 1/6 SetWeight: 30 ActualWeight: 30 Images: hashicorp/http-echo (canary, stable) Replicas: Desired: 1 Current: 2 Updated: 1 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO \u27f3 echo-rollout Rollout \u0965 Paused 2d21h \u251c\u2500\u2500# revision:3 \u2502 \u2514\u2500\u2500\u29c9 echo-rollout-64fb847897 ReplicaSet \u2714 Healthy 2s canary \u2502 \u2514\u2500\u2500\u25a1 echo-rollout-64fb847897-49sg6 Pod \u2714 Running 2s ready:1/1 \u251c\u2500\u2500# revision:2 \u2502 \u2514\u2500\u2500\u29c9 echo-rollout-578bfdb4b8 ReplicaSet \u2714 Healthy 3h5m stable \u2502 \u2514\u2500\u2500\u25a1 echo-rollout-578bfdb4b8-86z6n Pod \u2714 Running 3h5m ready:1/1 \u2514\u2500\u2500# revision:1 \u2514\u2500\u2500\u29c9 echo-rollout-948d9c9f9 ReplicaSet \u2022 ScaledDown 2d21h In your other terminal window, you can verify that the canary is progressing appropriately by sending requests in a loop: while true; do curl -k https://$AMBASSADOR_LB_ENDPOINT/echo/; sleep 0.2; done This will display a running list of responses from the service that will gradually transition from VERSION 1 strings to VERSION 2 strings. For more details about the Ambassador and Argo-Rollouts integration, see the Ambassador Argo documentation .","title":"Ambassador"},{"location":"getting-started/ambassador/#argo-rollouts-and-ambassador-quick-start","text":"This tutorial will walk you through the process of configuring Argo Rollouts to work with Ambassador to facilitate canary releases. All files used in this guide are available in the examples directory of this repository.","title":"Argo Rollouts and Ambassador Quick Start"},{"location":"getting-started/ambassador/#requirements","text":"Kubernetes cluster Argo-Rollouts installed in the cluster Note If using Ambassador Edge Stack or Emissary-ingress 2.0+, you will need to install Argo-Rollouts version v1.1+, and you will need to supply --ambassador-api-version x.getambassador.io/v3alpha1 to your argo-rollouts deployment.","title":"Requirements"},{"location":"getting-started/ambassador/#1-install-and-configure-ambassador-edge-stack","text":"If you don't have Ambassador in your cluster you can install it following the Edge Stack documentation . By default, Edge Stack routes via Kubernetes services. For best performance with canaries, we recommend you use endpoint routing. Enable endpoint routing on your cluster by saving the following configuration in a file called resolver.yaml : apiVersion: getambassador.io/v2 kind: KubernetesEndpointResolver metadata: name: endpoint Apply this configuration to your cluster: kubectl apply -f resolver.yaml .","title":"1. Install and configure Ambassador Edge Stack"},{"location":"getting-started/ambassador/#2-create-the-kubernetes-services","text":"We'll create two Kubernetes services, named echo-stable and echo-canary . Save this configuration to the file echo-service.yaml . apiVersion: v1 kind: Service metadata: labels: app: echo name: echo-stable spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: echo --- apiVersion: v1 kind: Service metadata: labels: app: echo name: echo-canary spec: type: ClusterIP ports: - name: http port: 80 protocol: TCP targetPort: 8080 selector: app: echo We'll also create an Edge Stack route to the services. Save the following configuration to a file called echo-mapping.yaml . apiVersion: getambassador.io/v2 kind: Mapping metadata: name: echo spec: prefix: /echo rewrite: /echo service: echo-stable:80 resolver: endpoint Apply both of these configurations to the Kubernetes cluster: kubectl apply -f echo-service.yaml kubectl apply -f echo-mapping.yaml","title":"2. Create the Kubernetes Services"},{"location":"getting-started/ambassador/#3-deploy-the-echo-service","text":"Create a Rollout resource and save it to a file called rollout.yaml . Note the trafficRouting attribute, which tells Argo to use Ambassador Edge Stack for routing. apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: echo-rollout spec: selector: matchLabels: app: echo template: metadata: labels: app: echo spec: containers: - image: hashicorp/http-echo args: - \"-text=VERSION 1\" - -listen=:8080 imagePullPolicy: Always name: echo-v1 ports: - containerPort: 8080 strategy: canary: stableService: echo-stable canaryService: echo-canary trafficRouting: ambassador: mappings: - echo steps: - setWeight: 30 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 100 - pause: {duration: 10} Apply the rollout to your cluster kubectl apply -f rollout.yaml . Note that no canary rollout will occur, as this is the first version of the service being deployed.","title":"3. Deploy the Echo Service"},{"location":"getting-started/ambassador/#4-test-the-service","text":"We'll now test that this rollout works as expected. Open a new terminal window. We'll use it to send requests to the cluster. Get the external IP address for Edge Stack: export AMBASSADOR_LB_ENDPOINT=$(kubectl -n ambassador get svc ambassador -o \"go-template={{range .status.loadBalancer.ingress}}{{or .ip .hostname}}{{end}}\") Send a request to the echo service: curl -Lk \"https://$AMBASSADOR_LB_ENDPOINT/echo/\" You should get a response of \"VERSION 1\".","title":"4. Test the service"},{"location":"getting-started/ambassador/#5-rollout-a-new-version","text":"It's time to rollout a new version of the service. Update the echo container in the rollout.yaml to display \"VERSION 2\": apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: echo-rollout spec: selector: matchLabels: app: echo template: metadata: labels: app: echo spec: containers: - image: hashicorp/http-echo args: - \"-text=VERSION 2\" - -listen=:8080 imagePullPolicy: Always name: echo-v1 ports: - containerPort: 8080 strategy: canary: stableService: echo-stable canaryService: echo-canary trafficRouting: ambassador: mappings: - echo steps: - setWeight: 30 - pause: {duration: 30s} - setWeight: 60 - pause: {duration: 30s} - setWeight: 100 - pause: {duration: 10} Apply the rollout to the cluster by typing kubectl apply -f rollout.yaml . This will rollout a version 2 of the service by routing 30% of traffic to the service for 30 seconds, followed by 60% of traffic for another 30 seconds. You can monitor the status of your rollout at the command line: kubectl argo rollouts get rollout echo-rollout --watch Will display an output similar to the following: Name: echo-rollout Namespace: default Status: \u0965 Paused Message: CanaryPauseStep Strategy: Canary Step: 1/6 SetWeight: 30 ActualWeight: 30 Images: hashicorp/http-echo (canary, stable) Replicas: Desired: 1 Current: 2 Updated: 1 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO \u27f3 echo-rollout Rollout \u0965 Paused 2d21h \u251c\u2500\u2500# revision:3 \u2502 \u2514\u2500\u2500\u29c9 echo-rollout-64fb847897 ReplicaSet \u2714 Healthy 2s canary \u2502 \u2514\u2500\u2500\u25a1 echo-rollout-64fb847897-49sg6 Pod \u2714 Running 2s ready:1/1 \u251c\u2500\u2500# revision:2 \u2502 \u2514\u2500\u2500\u29c9 echo-rollout-578bfdb4b8 ReplicaSet \u2714 Healthy 3h5m stable \u2502 \u2514\u2500\u2500\u25a1 echo-rollout-578bfdb4b8-86z6n Pod \u2714 Running 3h5m ready:1/1 \u2514\u2500\u2500# revision:1 \u2514\u2500\u2500\u29c9 echo-rollout-948d9c9f9 ReplicaSet \u2022 ScaledDown 2d21h In your other terminal window, you can verify that the canary is progressing appropriately by sending requests in a loop: while true; do curl -k https://$AMBASSADOR_LB_ENDPOINT/echo/; sleep 0.2; done This will display a running list of responses from the service that will gradually transition from VERSION 1 strings to VERSION 2 strings. For more details about the Ambassador and Argo-Rollouts integration, see the Ambassador Argo documentation .","title":"5. Rollout a new version"},{"location":"getting-started/istio/","text":"Getting Started - Istio \u00b6 This guide covers how Argo Rollouts integrates with the Istio Service Mesh for traffic shaping. This guide builds upon the concepts of the basic getting started guide . Requirements \u00b6 Kubernetes cluster with Istio installed Tip See the environment setup guide for Istio on how to setup a local minikube environment with Istio 1. Deploy the Rollout, Services, Istio VirtualService, and Istio Gateway \u00b6 When Istio is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller updates to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller updates to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : istio : virtualServices : # One or more virtualServices can be configured # Reference to a VirtualService which the controller updates with canary weights - name : rollouts-demo-vsvc1 # Optional if there is a single HTTP route in the VirtualService, otherwise required routes : - http-primary # Optional if there is a single HTTPS/TLS route in the VirtualService, otherwise required tlsRoutes : # Below fields are optional but if defined, they should match exactly with at least one of the TLS route match rules in your VirtualService - port : 443 # Only required if you want to match any rule in your VirtualService which contains this port # Only required if you want to match any rule in your VirtualService which contain all these SNI hosts sniHosts : - reviews.bookinfo.com - localhost - name : rollouts-demo-vsvc2 # Optional if there is a single HTTP route in the VirtualService, otherwise required routes : - http-secondary # Optional if there is a single HTTPS/TLS route in the VirtualService, otherwise required tlsRoutes : # Below fields are optional but if defined, they should match exactly with at least one of the TLS route match rules in your VirtualService - port : 443 # Only required if you want to match any rule in your VirtualService which contains this port # Only required if you want to match any rule in your VirtualService which contain all these SNI hosts sniHosts : - reviews.bookinfo.com - localhost ... The VirtualService and route referenced in either trafficRouting.istio.virtualService or trafficRouting.istio.virtualServices . trafficRouting.istio.virtualServices helps in adding one or more virtualServices unlike trafficRouting.istio.virtualService where only single virtualService can be added. This is required to have either HTTP or TLS, or both route specs that splits between the stable and the canary services referenced in the rollout. If the route is HTTPS/TLS, we can match it based on the given port number and/or SNI hosts. Note that both of them are optional and only needed if you want to match any rule in your VirtualService which contains these. In this guide, the two services are: rollouts-demo-stable and rollouts-demo-canary respectively. The weights for these two services should initially be set to 100% on the stable service and 0% on the canary service. During an update, these values will get modified by the controller. If there are multiple VirtualService then weight values for stable and canary service of each VirtualService will be modified by the controller simultaneously. Note that since we have both the HTTP and HTTPS routes in our rollout spec and they match the VirtualService specs, weights will get modified for both these routes. apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollouts-demo-vsvc1 spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc1.local http : - name : http-primary # Should match rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.routes route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService port : number : 15372 weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService port : number : 15372 weight : 0 tls : - match : - port : 443 # Should match the port number of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes sniHosts : # Should match all the SNI hosts of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes - reviews.bookinfo.com - localhost route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService weight : 0 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollouts-demo-vsvc2 spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc2.local http : - name : http-secondary # Should match rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.routes route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService port : number : 15373 weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService port : number : 15373 weight : 0 tls : - match : - port : 443 # Should match the port number of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes sniHosts : # Should match all the SNI hosts of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes - reviews.bookinfo.com route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService weight : 0 Run the following commands to deploy: A Rollout Two Services (stable and canary) One or more Istio VirtualServices An Istio Gateway kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/multipleVirtualsvc.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/gateway.yaml After applying the manifests you should see the following rollout, services, virtualservices, and gateway resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .103.146.137 <none> 80 /TCP 37s rollouts-demo-stable ClusterIP 10 .101.158.227 <none> 80 /TCP 37s $ kubectl get virtualservice NAME GATEWAYS HOSTS AGE rollouts-demo-vsvc1 [ rollouts-demo-gateway ] [ rollouts-demo-vsvc1.local ] 54s rollouts-demo-vsvc2 [ rollouts-demo-gateway ] [ rollouts-demo-vsvc2.local ] 54s $ kubectl get gateway NAME AGE rollouts-demo-gateway 71s kubectl argo rollouts get rollout rollouts-demo 2. Perform an update \u00b6 Update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. To understand how this works, inspect the VirtualService which the Rollout was referencing. When looking at both the VirtualService, we see that the route destination weights have been modified by the controller to reflect the current weight of the canary. apiVersion : networking.istio.io/v1beta1 kind : VirtualService metadata : name : rollouts-demo-vsvc1 namespace : default spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc1.local http : - name : http-primary route : - destination : host : rollouts-demo-stable port : number : 15372 weight : 95 - destination : host : rollouts-demo-canary port : number : 15372 weight : 5 tls : - match : - port : 443 sniHosts : - reviews.bookinfo.com - localhost route : - destination : host : rollouts-demo-stable weight : 95 - destination : host : rollouts-demo-canary weight : 5 apiVersion : networking.istio.io/v1beta1 kind : VirtualService metadata : name : rollouts-demo-vsvc2 namespace : default spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc2.local http : - name : http-primary route : - destination : host : rollouts-demo-stable port : number : 15373 weight : 95 - destination : host : rollouts-demo-canary port : number : 15373 weight : 5 tls : - match : - port : 443 sniHosts : - reviews.bookinfo.com route : - destination : host : rollouts-demo-stable weight : 95 - destination : host : rollouts-demo-canary weight : 5 As the Rollout progresses through steps, the HTTP and/or TLS route(s) destination weights will be adjusted to match the current setWeight of the steps.","title":"Istio"},{"location":"getting-started/istio/#getting-started-istio","text":"This guide covers how Argo Rollouts integrates with the Istio Service Mesh for traffic shaping. This guide builds upon the concepts of the basic getting started guide .","title":"Getting Started - Istio"},{"location":"getting-started/istio/#requirements","text":"Kubernetes cluster with Istio installed Tip See the environment setup guide for Istio on how to setup a local minikube environment with Istio","title":"Requirements"},{"location":"getting-started/istio/#1-deploy-the-rollout-services-istio-virtualservice-and-istio-gateway","text":"When Istio is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller updates to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller updates to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : istio : virtualServices : # One or more virtualServices can be configured # Reference to a VirtualService which the controller updates with canary weights - name : rollouts-demo-vsvc1 # Optional if there is a single HTTP route in the VirtualService, otherwise required routes : - http-primary # Optional if there is a single HTTPS/TLS route in the VirtualService, otherwise required tlsRoutes : # Below fields are optional but if defined, they should match exactly with at least one of the TLS route match rules in your VirtualService - port : 443 # Only required if you want to match any rule in your VirtualService which contains this port # Only required if you want to match any rule in your VirtualService which contain all these SNI hosts sniHosts : - reviews.bookinfo.com - localhost - name : rollouts-demo-vsvc2 # Optional if there is a single HTTP route in the VirtualService, otherwise required routes : - http-secondary # Optional if there is a single HTTPS/TLS route in the VirtualService, otherwise required tlsRoutes : # Below fields are optional but if defined, they should match exactly with at least one of the TLS route match rules in your VirtualService - port : 443 # Only required if you want to match any rule in your VirtualService which contains this port # Only required if you want to match any rule in your VirtualService which contain all these SNI hosts sniHosts : - reviews.bookinfo.com - localhost ... The VirtualService and route referenced in either trafficRouting.istio.virtualService or trafficRouting.istio.virtualServices . trafficRouting.istio.virtualServices helps in adding one or more virtualServices unlike trafficRouting.istio.virtualService where only single virtualService can be added. This is required to have either HTTP or TLS, or both route specs that splits between the stable and the canary services referenced in the rollout. If the route is HTTPS/TLS, we can match it based on the given port number and/or SNI hosts. Note that both of them are optional and only needed if you want to match any rule in your VirtualService which contains these. In this guide, the two services are: rollouts-demo-stable and rollouts-demo-canary respectively. The weights for these two services should initially be set to 100% on the stable service and 0% on the canary service. During an update, these values will get modified by the controller. If there are multiple VirtualService then weight values for stable and canary service of each VirtualService will be modified by the controller simultaneously. Note that since we have both the HTTP and HTTPS routes in our rollout spec and they match the VirtualService specs, weights will get modified for both these routes. apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollouts-demo-vsvc1 spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc1.local http : - name : http-primary # Should match rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.routes route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService port : number : 15372 weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService port : number : 15372 weight : 0 tls : - match : - port : 443 # Should match the port number of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes sniHosts : # Should match all the SNI hosts of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes - reviews.bookinfo.com - localhost route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService weight : 0 apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : rollouts-demo-vsvc2 spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc2.local http : - name : http-secondary # Should match rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.routes route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService port : number : 15373 weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService port : number : 15373 weight : 0 tls : - match : - port : 443 # Should match the port number of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes sniHosts : # Should match all the SNI hosts of the route defined in rollout.spec.strategy.canary.trafficRouting.istio.virtualServices.tlsRoutes - reviews.bookinfo.com route : - destination : host : rollouts-demo-stable # Should match rollout.spec.strategy.canary.stableService weight : 100 - destination : host : rollouts-demo-canary # Should match rollout.spec.strategy.canary.canaryService weight : 0 Run the following commands to deploy: A Rollout Two Services (stable and canary) One or more Istio VirtualServices An Istio Gateway kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/multipleVirtualsvc.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/istio/gateway.yaml After applying the manifests you should see the following rollout, services, virtualservices, and gateway resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .103.146.137 <none> 80 /TCP 37s rollouts-demo-stable ClusterIP 10 .101.158.227 <none> 80 /TCP 37s $ kubectl get virtualservice NAME GATEWAYS HOSTS AGE rollouts-demo-vsvc1 [ rollouts-demo-gateway ] [ rollouts-demo-vsvc1.local ] 54s rollouts-demo-vsvc2 [ rollouts-demo-gateway ] [ rollouts-demo-vsvc2.local ] 54s $ kubectl get gateway NAME AGE rollouts-demo-gateway 71s kubectl argo rollouts get rollout rollouts-demo","title":"1. Deploy the Rollout, Services, Istio VirtualService, and Istio Gateway"},{"location":"getting-started/istio/#2-perform-an-update","text":"Update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. To understand how this works, inspect the VirtualService which the Rollout was referencing. When looking at both the VirtualService, we see that the route destination weights have been modified by the controller to reflect the current weight of the canary. apiVersion : networking.istio.io/v1beta1 kind : VirtualService metadata : name : rollouts-demo-vsvc1 namespace : default spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc1.local http : - name : http-primary route : - destination : host : rollouts-demo-stable port : number : 15372 weight : 95 - destination : host : rollouts-demo-canary port : number : 15372 weight : 5 tls : - match : - port : 443 sniHosts : - reviews.bookinfo.com - localhost route : - destination : host : rollouts-demo-stable weight : 95 - destination : host : rollouts-demo-canary weight : 5 apiVersion : networking.istio.io/v1beta1 kind : VirtualService metadata : name : rollouts-demo-vsvc2 namespace : default spec : gateways : - rollouts-demo-gateway hosts : - rollouts-demo-vsvc2.local http : - name : http-primary route : - destination : host : rollouts-demo-stable port : number : 15373 weight : 95 - destination : host : rollouts-demo-canary port : number : 15373 weight : 5 tls : - match : - port : 443 sniHosts : - reviews.bookinfo.com route : - destination : host : rollouts-demo-stable weight : 95 - destination : host : rollouts-demo-canary weight : 5 As the Rollout progresses through steps, the HTTP and/or TLS route(s) destination weights will be adjusted to match the current setWeight of the steps.","title":"2. Perform an update"},{"location":"getting-started/mixed/","text":"Getting Started - Multiple Providers (Service Mesh Interface and NGiNX Ingress) \u00b6 Important Available since v1.2 This guide covers how Argo Rollouts integrates with multiple TrafficRoutings, using Linkerd and NGINX Ingress Controller for traffic shaping, but you should be able to produce any other combination between the existing trafficRouting options. This guide builds upon the concepts of the basic getting started guide , NGINX Guide , and SMI Guide . Requirements \u00b6 Kubernetes cluster with Linkerd installed Kubernetes cluster with NGINX ingress controller installed and part of the mesh Tip See the environment setup guide for linkerd on how to setup a local minikube environment with linkerd and nginx. 1. Deploy the Rollout, Services, and Ingress \u00b6 When SMI is used as one the traffic routers, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : smi : {} When NGINX Ingress is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable ... A combination of both should have comply with each TrafficRouting requirements, in this case: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable smi : {} The Ingress referenced in canary.trafficRouting.nginx.stableIngress is required to have a host rule which has a backend targeting the Service referenced under canary.stableService . In our example, that stable Service is named: rollouts-demo-stable : apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rollouts-demo-stable annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : rollouts-demo.local http : paths : - path : / backend : # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field serviceName : rollouts-demo-stable servicePort : 80 Run the following commands to deploy: A Rollout with the Linkerd linkerd.io/inject: enabled annotation Two Services (stable and canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/mixed/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/mixed/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/mixed/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 2 1 2 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .111.69.188 <none> 80 /TCP 23m rollouts-demo-stable ClusterIP 10 .109.175.248 <none> 80 /TCP 23m $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 23m You should also see a TrafficSplit resource which is created automatically and owned by the rollout: $ kubectl get trafficsplit NAME SERVICE rollouts-demo rollouts-demo-stable When inspecting the generated TrafficSplit resource, the weights are automatically configured to send 100% traffic to the rollouts-demo-stable service, and 0% traffic to the rollouts-demo-canary . These values will be updated during an update. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"0\" - service : rollouts-demo-stable weight : \"100\" service : rollouts-demo-stable You should also notice a second ingress created by the rollouts controller, rollouts-demo-rollouts-demo-stable-canary . This ingress is the \"canary ingress\", which is a clone of the user-managed Ingress referenced under nginx.stableIngress . It is used by nginx ingress controller to achieve canary traffic splitting. The name of the generated ingress is formulated using <ROLLOUT-NAME>-<INGRESS-NAME>-canary . More details on the second Ingress are discussed in the following section. 2. Perform an update \u00b6 Now perform an update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary and 95% to the stable. When inspecting the TrafficSplit generated by the controller, we see that the weight has been updated to reflect the current setWeight: 5 step of the canary deploy. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"5\" - service : rollouts-demo-stable weight : \"95\" service : rollouts-demo-stable When inspecting the rollout controller generated Ingress copy, we see that it has the following changes over the original ingress: Two additional NGINX specific canary annotations are added to the annotations. The Ingress rules will have an rule which points the backend to the canary service. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : rollouts-demo-rollouts-demo-stable-canary annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/canary : \"true\" nginx.ingress.kubernetes.io/canary-weight : \"5\" spec : rules : - host : rollouts-demo.local http : paths : - backend : serviceName : rollouts-demo-canary servicePort : 80 As the Rollout progresses through steps, the weights in the TrafficSplit and Ingress resource will be adjusted to match the current setWeight of the steps.","title":"Getting Started - Multiple Providers (Service Mesh Interface and NGiNX Ingress)"},{"location":"getting-started/mixed/#getting-started-multiple-providers-service-mesh-interface-and-nginx-ingress","text":"Important Available since v1.2 This guide covers how Argo Rollouts integrates with multiple TrafficRoutings, using Linkerd and NGINX Ingress Controller for traffic shaping, but you should be able to produce any other combination between the existing trafficRouting options. This guide builds upon the concepts of the basic getting started guide , NGINX Guide , and SMI Guide .","title":"Getting Started - Multiple Providers (Service Mesh Interface and NGiNX Ingress)"},{"location":"getting-started/mixed/#requirements","text":"Kubernetes cluster with Linkerd installed Kubernetes cluster with NGINX ingress controller installed and part of the mesh Tip See the environment setup guide for linkerd on how to setup a local minikube environment with linkerd and nginx.","title":"Requirements"},{"location":"getting-started/mixed/#1-deploy-the-rollout-services-and-ingress","text":"When SMI is used as one the traffic routers, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : smi : {} When NGINX Ingress is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable ... A combination of both should have comply with each TrafficRouting requirements, in this case: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable smi : {} The Ingress referenced in canary.trafficRouting.nginx.stableIngress is required to have a host rule which has a backend targeting the Service referenced under canary.stableService . In our example, that stable Service is named: rollouts-demo-stable : apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rollouts-demo-stable annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : rollouts-demo.local http : paths : - path : / backend : # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field serviceName : rollouts-demo-stable servicePort : 80 Run the following commands to deploy: A Rollout with the Linkerd linkerd.io/inject: enabled annotation Two Services (stable and canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/mixed/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/mixed/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/mixed/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 2 1 2 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .111.69.188 <none> 80 /TCP 23m rollouts-demo-stable ClusterIP 10 .109.175.248 <none> 80 /TCP 23m $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 23m You should also see a TrafficSplit resource which is created automatically and owned by the rollout: $ kubectl get trafficsplit NAME SERVICE rollouts-demo rollouts-demo-stable When inspecting the generated TrafficSplit resource, the weights are automatically configured to send 100% traffic to the rollouts-demo-stable service, and 0% traffic to the rollouts-demo-canary . These values will be updated during an update. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"0\" - service : rollouts-demo-stable weight : \"100\" service : rollouts-demo-stable You should also notice a second ingress created by the rollouts controller, rollouts-demo-rollouts-demo-stable-canary . This ingress is the \"canary ingress\", which is a clone of the user-managed Ingress referenced under nginx.stableIngress . It is used by nginx ingress controller to achieve canary traffic splitting. The name of the generated ingress is formulated using <ROLLOUT-NAME>-<INGRESS-NAME>-canary . More details on the second Ingress are discussed in the following section.","title":"1. Deploy the Rollout, Services, and Ingress"},{"location":"getting-started/mixed/#2-perform-an-update","text":"Now perform an update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary and 95% to the stable. When inspecting the TrafficSplit generated by the controller, we see that the weight has been updated to reflect the current setWeight: 5 step of the canary deploy. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"5\" - service : rollouts-demo-stable weight : \"95\" service : rollouts-demo-stable When inspecting the rollout controller generated Ingress copy, we see that it has the following changes over the original ingress: Two additional NGINX specific canary annotations are added to the annotations. The Ingress rules will have an rule which points the backend to the canary service. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : rollouts-demo-rollouts-demo-stable-canary annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/canary : \"true\" nginx.ingress.kubernetes.io/canary-weight : \"5\" spec : rules : - host : rollouts-demo.local http : paths : - backend : serviceName : rollouts-demo-canary servicePort : 80 As the Rollout progresses through steps, the weights in the TrafficSplit and Ingress resource will be adjusted to match the current setWeight of the steps.","title":"2. Perform an update"},{"location":"getting-started/nginx/","text":"Getting Started - NGINX Ingress \u00b6 This guide covers how Argo Rollouts integrates with the NGINX Ingress Controller for traffic shaping. This guide builds upon the concepts of the basic getting started guide . Requirements \u00b6 Kubernetes cluster with NGINX ingress controller installed Tip See the environment setup guide for NGINX on how to setup a local minikube environment with nginx. 1. Deploy the Rollout, Services, and Ingress \u00b6 When NGINX Ingress is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable ... The Ingress referenced in canary.trafficRouting.nginx.stableIngress is required to have a host rule which has a backend targeting the Service referenced under canary.stableService . In our example, that stable Service is named: rollouts-demo-stable : apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rollouts-demo-stable annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : rollouts-demo.local http : paths : - path : / backend : # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field serviceName : rollouts-demo-stable servicePort : 80 Run the following commands to deploy: A Rollout Two Services (stable and canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .96.6.241 <none> 80 /TCP 33s rollouts-demo-stable ClusterIP 10 .102.229.83 <none> 80 /TCP 33s $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 36s rollouts-demo-rollouts-demo-stable-canary <none> rollouts-demo.local 192 .168.64.2 80 35s You should also notice a second ingress created by the rollouts controller, rollouts-demo-rollouts-demo-stable-canary . This ingress is the \"canary ingress\", which is a clone of the user-managed Ingress referenced under nginx.stableIngress . It is used by nginx ingress controller to achieve canary traffic splitting. The name of the generated ingress is formulated using <ROLLOUT-NAME>-<INGRESS-NAME>-canary . More details on the second Ingress are discussed in the following section. kubectl argo rollouts get rollout rollouts-demo 2. Perform an update \u00b6 Update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. One thing to note, is that the rollout is able to achieve a 5% canary weight despite only running two pods. This is able to be achieved since the traffic split happens at the ingress controller (as opposed to weighted replica counts and kube-proxy in the basic guide). When inspecting the rollout controller generated Ingress copy, we see that it has the following changes over the original ingress: Two additional NGINX specific canary annotations are added to the annotations. The Ingress rules will have an rule which points the backend to the canary service. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : rollouts-demo-rollouts-demo-stable-canary annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/canary : \"true\" nginx.ingress.kubernetes.io/canary-weight : \"5\" spec : rules : - host : rollouts-demo.local http : paths : - backend : serviceName : rollouts-demo-canary servicePort : 80 As the Rollout progresses through steps, the canary-weight annotation will be adjusted to match the current setWeight of the steps. The NGINX ingress controller examines the original Ingress, the canary Ingress, and the canary-weight annotation to determine what percentage of traffic to split between the two Ingresses.","title":"NGINX"},{"location":"getting-started/nginx/#getting-started-nginx-ingress","text":"This guide covers how Argo Rollouts integrates with the NGINX Ingress Controller for traffic shaping. This guide builds upon the concepts of the basic getting started guide .","title":"Getting Started - NGINX Ingress"},{"location":"getting-started/nginx/#requirements","text":"Kubernetes cluster with NGINX ingress controller installed Tip See the environment setup guide for NGINX on how to setup a local minikube environment with nginx.","title":"Requirements"},{"location":"getting-started/nginx/#1-deploy-the-rollout-services-and-ingress","text":"When NGINX Ingress is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : nginx : # Reference to an Ingress which has a rule pointing to the stable service (e.g. rollouts-demo-stable) # This ingress will be cloned with a new name, in order to achieve NGINX traffic splitting. stableIngress : rollouts-demo-stable ... The Ingress referenced in canary.trafficRouting.nginx.stableIngress is required to have a host rule which has a backend targeting the Service referenced under canary.stableService . In our example, that stable Service is named: rollouts-demo-stable : apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : rollouts-demo-stable annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : rollouts-demo.local http : paths : - path : / backend : # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field serviceName : rollouts-demo-stable servicePort : 80 Run the following commands to deploy: A Rollout Two Services (stable and canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 1 1 1 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .96.6.241 <none> 80 /TCP 33s rollouts-demo-stable ClusterIP 10 .102.229.83 <none> 80 /TCP 33s $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 36s rollouts-demo-rollouts-demo-stable-canary <none> rollouts-demo.local 192 .168.64.2 80 35s You should also notice a second ingress created by the rollouts controller, rollouts-demo-rollouts-demo-stable-canary . This ingress is the \"canary ingress\", which is a clone of the user-managed Ingress referenced under nginx.stableIngress . It is used by nginx ingress controller to achieve canary traffic splitting. The name of the generated ingress is formulated using <ROLLOUT-NAME>-<INGRESS-NAME>-canary . More details on the second Ingress are discussed in the following section. kubectl argo rollouts get rollout rollouts-demo","title":"1. Deploy the Rollout, Services, and Ingress"},{"location":"getting-started/nginx/#2-perform-an-update","text":"Update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. One thing to note, is that the rollout is able to achieve a 5% canary weight despite only running two pods. This is able to be achieved since the traffic split happens at the ingress controller (as opposed to weighted replica counts and kube-proxy in the basic guide). When inspecting the rollout controller generated Ingress copy, we see that it has the following changes over the original ingress: Two additional NGINX specific canary annotations are added to the annotations. The Ingress rules will have an rule which points the backend to the canary service. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : rollouts-demo-rollouts-demo-stable-canary annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/canary : \"true\" nginx.ingress.kubernetes.io/canary-weight : \"5\" spec : rules : - host : rollouts-demo.local http : paths : - backend : serviceName : rollouts-demo-canary servicePort : 80 As the Rollout progresses through steps, the canary-weight annotation will be adjusted to match the current setWeight of the steps. The NGINX ingress controller examines the original Ingress, the canary Ingress, and the canary-weight annotation to determine what percentage of traffic to split between the two Ingresses.","title":"2. Perform an update"},{"location":"getting-started/setup/","text":"Environment Set Up \u00b6 This guide shows how to set up a local environment for development, testing, learning, or demoing purposes. Helm \u00b6 Some dependencies are installable via the Helm stable repository: helm repo add stable https://charts.helm.sh/stable helm repo add grafana https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Minikube \u00b6 NGINX Ingress Controller Setup \u00b6 The following instructions describe how to configure NGINX Ingress Controller on minikube. For basic ingress support, only the \"ingress\" addon needs to be enabled: minikube addons enable ingress Optionally, Prometheus and Grafana can be installed to utilize progressive delivery functionality: # Install Prometheus kubectl create ns monitoring helm install prometheus prometheus-community/prometheus -n monitoring -f docs/getting-started/setup/values-prometheus.yaml # Patch the ingress-nginx-controller pod so that it has the required # prometheus annotations. This allows the pod to be scraped by the # prometheus server. kubectl patch deploy ingress-nginx-controller -n ingress-nginx -p \" $( cat docs/getting-started/setup/ingress-nginx-controller-metrics-scrape.yaml ) \" Note For Minikube version 1.18.1 or earlier , change the -n parameter value (namespace) to kube-system . # Install grafana along with nginx ingress dashboards helm install grafana grafana/grafana -n monitoring -f docs/getting-started/setup/values-grafana-nginx.yaml # Grafana UI can be accessed by running: minikube service grafana -n monitoring Istio Setup \u00b6 The following instructions describe how to configure Istio on minikube. # Istio on Minikube requires additional memory and CPU minikube start --memory = 8192mb --cpus = 4 # Install istio minikube addons enable istio-provisioner minikube addons enable istio # Label the default namespace to enable istio sidecar injection for the namespace kubectl label namespace default istio-injection = enabled Istio already comes with a Prometheus database ready to use. To visualize metrics about istio services, Grafana and Istio dashboards can be installed via Helm to leverage progressive delivery functionality: # Install Grafana and Istio dashboards helm install grafana grafana/grafana -n istio-system -f docs/getting-started/setup/values-grafana-istio.yaml # Grafana UI can be accessed by running minikube service grafana -n istio-system In order for traffic to enter the Istio mesh, the request needs to go through an Istio ingress gateway, which is simply a normal Kubernetes Deployment and Service. One convenient way to reach the gateway using minikube, is using the minikube tunnel command, which assigns Services a LoadBalancer. This command should be run in the background, usually in a separate terminal window: minikube tunnel While running minikube tunnel , the istio-ingressgateway Service will now have an external IP which can be retrieved via kubectl : $ kubectl get svc -n istio-system istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .100.136.45 10 .100.136.45 15020 :31711/TCP,80:31298/TCP.... 7d22h The LoadBalancer external IP (10.100.136.45 in this example) is now reachable to access services in the Istio mesh. Istio routes requests to the correct pod based on the Host HTTP header. Follow the guide on supplying host headers to learn how to configure your client environment to supply the proper request to reach the pod. Linkerd Setup \u00b6 Linkerd can be installed using the linkerd CLI. brew install linkerd linkerd install | kubectl apply -f - Linkerd does not provide its own ingress controller, choosing instead to work alongside your ingress controller of choice. On minikube, we can use the built-in NGINX ingress addon and reconfigure it to be part of the linkerd mesh. # Install the NGINX ingress controller addon minikube addons enable ingress # Patch the nginx-ingress-controller deployment to allow injection of the linkerd proxy to the # pod, so that it will be part of the mesh. kubectl patch deploy ingress-nginx-controller -n kube-system \\ -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"linkerd.io/inject\":\"enabled\"}}}}}' Supplying Host Headers \u00b6 Most ingress controllers and service mesh implementations rely on the Host HTTP request header being supplied in the request in order to determine how to route the request to the correct pod. Determining the hostname to IP mapping \u00b6 For the Host header to be set in the request, the hostname of the service should resolve to the public IP address of the ingress or service mesh. Depending on if you are using an ingress controller or a service mesh, use one of the following techniques to determine the correct hostname to IP mapping: Ingresses \u00b6 For traffic which is reaching the cluster network via a normal Kubernetes Ingress, the hostname should map to the IP of the ingress. We can retrieve the external IP of the ingress from the Ingress object itself, using kubectl : $ kubectl get ing rollouts-demo-stable NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 80m In the example above, the hostname rollouts-demo.local should be configured to resolve to the IP 192.168.64.2 . The next section describes various ways to configure your local system to resolve the hostname to the desired IP. Istio \u00b6 In the case of Istio, traffic enters the mesh through an Ingress Gateway , which simply is a load balancer sitting at the edge of mesh. To determine the correct hostname to IP mapping, it largely depends on what was configured in the VirtualService and Gateway . If you are following the Istio getting started guide , the examples use the \"default\" istio ingress gateway, which we can obtain from kubectl: $ kubectl get svc -n istio-system istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .100.136.45 10 .100.136.45 15020 :31711/TCP,80:31298/TCP.... 7d22h In the above example, the hostname rollouts-demo.local should be configured to resolve to the IP 10.100.136.45 . The next section describes various ways to configure your local system to resolve the hostname to the desired IP. Configuring local hostname resolution \u00b6 Now that you have determined the correct hostname to IP mapping, the next step involves configuring the system so that will resolve properly. There are different techniques to do this: DNS Entry \u00b6 In real, production environments, the Host header is typically achieved by adding a DNS entry for the hostname in the DNS server. However, for local development, this is typically not an easily accessible option. /etc/hosts Entry \u00b6 On local workstations, a local entry to /etc/hosts can be added to map the hostname and IP address of the ingress. For example, the following is an example of an /etc/hosts file which maps rollouts-demo.local to IP 10.100.136.45 . ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127 .0.0.1 localhost 255 .255.255.255 broadcasthost ::1 localhost 10 .100.136.45 rollouts-demo.local The advantages of using a host entry, are that it works for all clients (CLIs, browsers). On the other hand, it is harder to maintain if the IP address changes frequently. Supply Header in Curl \u00b6 Clients such as curl, have the ability to explicitly set a header (the -H flag in curl). For example: $ curl -I -H 'Host: rollouts-demo.local' http://10.100.136.45/color HTTP/1.1 200 OK content-type: text/plain ; charset = utf-8 x-content-type-options: nosniff date: Wed, 24 Jun 2020 08 :44:59 GMT content-length: 6 x-envoy-upstream-service-time: 1 server: istio-envoy Notice that the same request made without the header, fails with a 404 Not Found error. $ curl -I http://10.100.136.45/color HTTP/1.1 404 Not Found date: Wed, 24 Jun 2020 08 :46:07 GMT server: istio-envoy transfer-encoding: chunked Browser Extension \u00b6 Similar to curl's ability to explicitly set a header, browsers can also achieve this via browser extensions. One example of a browser extension which can do this, is ModHeader .","title":"Environment Setup"},{"location":"getting-started/setup/#environment-set-up","text":"This guide shows how to set up a local environment for development, testing, learning, or demoing purposes.","title":"Environment Set Up"},{"location":"getting-started/setup/#helm","text":"Some dependencies are installable via the Helm stable repository: helm repo add stable https://charts.helm.sh/stable helm repo add grafana https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update","title":"Helm"},{"location":"getting-started/setup/#minikube","text":"","title":"Minikube"},{"location":"getting-started/setup/#nginx-ingress-controller-setup","text":"The following instructions describe how to configure NGINX Ingress Controller on minikube. For basic ingress support, only the \"ingress\" addon needs to be enabled: minikube addons enable ingress Optionally, Prometheus and Grafana can be installed to utilize progressive delivery functionality: # Install Prometheus kubectl create ns monitoring helm install prometheus prometheus-community/prometheus -n monitoring -f docs/getting-started/setup/values-prometheus.yaml # Patch the ingress-nginx-controller pod so that it has the required # prometheus annotations. This allows the pod to be scraped by the # prometheus server. kubectl patch deploy ingress-nginx-controller -n ingress-nginx -p \" $( cat docs/getting-started/setup/ingress-nginx-controller-metrics-scrape.yaml ) \" Note For Minikube version 1.18.1 or earlier , change the -n parameter value (namespace) to kube-system . # Install grafana along with nginx ingress dashboards helm install grafana grafana/grafana -n monitoring -f docs/getting-started/setup/values-grafana-nginx.yaml # Grafana UI can be accessed by running: minikube service grafana -n monitoring","title":"NGINX Ingress Controller Setup"},{"location":"getting-started/setup/#istio-setup","text":"The following instructions describe how to configure Istio on minikube. # Istio on Minikube requires additional memory and CPU minikube start --memory = 8192mb --cpus = 4 # Install istio minikube addons enable istio-provisioner minikube addons enable istio # Label the default namespace to enable istio sidecar injection for the namespace kubectl label namespace default istio-injection = enabled Istio already comes with a Prometheus database ready to use. To visualize metrics about istio services, Grafana and Istio dashboards can be installed via Helm to leverage progressive delivery functionality: # Install Grafana and Istio dashboards helm install grafana grafana/grafana -n istio-system -f docs/getting-started/setup/values-grafana-istio.yaml # Grafana UI can be accessed by running minikube service grafana -n istio-system In order for traffic to enter the Istio mesh, the request needs to go through an Istio ingress gateway, which is simply a normal Kubernetes Deployment and Service. One convenient way to reach the gateway using minikube, is using the minikube tunnel command, which assigns Services a LoadBalancer. This command should be run in the background, usually in a separate terminal window: minikube tunnel While running minikube tunnel , the istio-ingressgateway Service will now have an external IP which can be retrieved via kubectl : $ kubectl get svc -n istio-system istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .100.136.45 10 .100.136.45 15020 :31711/TCP,80:31298/TCP.... 7d22h The LoadBalancer external IP (10.100.136.45 in this example) is now reachable to access services in the Istio mesh. Istio routes requests to the correct pod based on the Host HTTP header. Follow the guide on supplying host headers to learn how to configure your client environment to supply the proper request to reach the pod.","title":"Istio Setup"},{"location":"getting-started/setup/#linkerd-setup","text":"Linkerd can be installed using the linkerd CLI. brew install linkerd linkerd install | kubectl apply -f - Linkerd does not provide its own ingress controller, choosing instead to work alongside your ingress controller of choice. On minikube, we can use the built-in NGINX ingress addon and reconfigure it to be part of the linkerd mesh. # Install the NGINX ingress controller addon minikube addons enable ingress # Patch the nginx-ingress-controller deployment to allow injection of the linkerd proxy to the # pod, so that it will be part of the mesh. kubectl patch deploy ingress-nginx-controller -n kube-system \\ -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"linkerd.io/inject\":\"enabled\"}}}}}'","title":"Linkerd Setup"},{"location":"getting-started/setup/#supplying-host-headers","text":"Most ingress controllers and service mesh implementations rely on the Host HTTP request header being supplied in the request in order to determine how to route the request to the correct pod.","title":"Supplying Host Headers"},{"location":"getting-started/setup/#determining-the-hostname-to-ip-mapping","text":"For the Host header to be set in the request, the hostname of the service should resolve to the public IP address of the ingress or service mesh. Depending on if you are using an ingress controller or a service mesh, use one of the following techniques to determine the correct hostname to IP mapping:","title":"Determining the hostname to IP mapping"},{"location":"getting-started/setup/#ingresses","text":"For traffic which is reaching the cluster network via a normal Kubernetes Ingress, the hostname should map to the IP of the ingress. We can retrieve the external IP of the ingress from the Ingress object itself, using kubectl : $ kubectl get ing rollouts-demo-stable NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 80m In the example above, the hostname rollouts-demo.local should be configured to resolve to the IP 192.168.64.2 . The next section describes various ways to configure your local system to resolve the hostname to the desired IP.","title":"Ingresses"},{"location":"getting-started/setup/#istio","text":"In the case of Istio, traffic enters the mesh through an Ingress Gateway , which simply is a load balancer sitting at the edge of mesh. To determine the correct hostname to IP mapping, it largely depends on what was configured in the VirtualService and Gateway . If you are following the Istio getting started guide , the examples use the \"default\" istio ingress gateway, which we can obtain from kubectl: $ kubectl get svc -n istio-system istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .100.136.45 10 .100.136.45 15020 :31711/TCP,80:31298/TCP.... 7d22h In the above example, the hostname rollouts-demo.local should be configured to resolve to the IP 10.100.136.45 . The next section describes various ways to configure your local system to resolve the hostname to the desired IP.","title":"Istio"},{"location":"getting-started/setup/#configuring-local-hostname-resolution","text":"Now that you have determined the correct hostname to IP mapping, the next step involves configuring the system so that will resolve properly. There are different techniques to do this:","title":"Configuring local hostname resolution"},{"location":"getting-started/setup/#dns-entry","text":"In real, production environments, the Host header is typically achieved by adding a DNS entry for the hostname in the DNS server. However, for local development, this is typically not an easily accessible option.","title":"DNS Entry"},{"location":"getting-started/setup/#etchosts-entry","text":"On local workstations, a local entry to /etc/hosts can be added to map the hostname and IP address of the ingress. For example, the following is an example of an /etc/hosts file which maps rollouts-demo.local to IP 10.100.136.45 . ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127 .0.0.1 localhost 255 .255.255.255 broadcasthost ::1 localhost 10 .100.136.45 rollouts-demo.local The advantages of using a host entry, are that it works for all clients (CLIs, browsers). On the other hand, it is harder to maintain if the IP address changes frequently.","title":"/etc/hosts Entry"},{"location":"getting-started/setup/#supply-header-in-curl","text":"Clients such as curl, have the ability to explicitly set a header (the -H flag in curl). For example: $ curl -I -H 'Host: rollouts-demo.local' http://10.100.136.45/color HTTP/1.1 200 OK content-type: text/plain ; charset = utf-8 x-content-type-options: nosniff date: Wed, 24 Jun 2020 08 :44:59 GMT content-length: 6 x-envoy-upstream-service-time: 1 server: istio-envoy Notice that the same request made without the header, fails with a 404 Not Found error. $ curl -I http://10.100.136.45/color HTTP/1.1 404 Not Found date: Wed, 24 Jun 2020 08 :46:07 GMT server: istio-envoy transfer-encoding: chunked","title":"Supply Header in Curl"},{"location":"getting-started/setup/#browser-extension","text":"Similar to curl's ability to explicitly set a header, browsers can also achieve this via browser extensions. One example of a browser extension which can do this, is ModHeader .","title":"Browser Extension"},{"location":"getting-started/smi/","text":"Getting Started - SMI (Service Mesh Interface) \u00b6 Important Available since v0.9 This guide covers how Argo Rollouts integrates with the Service Mesh Interface (SMI), using Linkerd and NGINX Ingress Controller for traffic shaping. Since the SMI TrafficSplit resource is supported by multiple service mesh providers, the concepts taught here are applicable to other service mesh providers that support the interface. See the SMI Ecosystem for other projects that support SMI. This guide builds upon the concepts of the basic getting started guide . Requirements \u00b6 Kubernetes cluster with Linkerd installed Kubernetes cluster with NGINX ingress controller installed and part of the mesh Tip See the environment setup guide for linkerd on how to setup a local minikube environment with linkerd and nginx. 1. Deploy the Rollout, Services, and Ingress \u00b6 When SMI is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : smi : {} Run the following commands to deploy: A Rollout with the Linkerd linkerd.io/inject: enabled annotation Two Services (stable and canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/smi/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/smi/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/smi/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 2 1 2 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .111.69.188 <none> 80 /TCP 23m rollouts-demo-stable ClusterIP 10 .109.175.248 <none> 80 /TCP 23m $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 23m You should also see a TrafficSplit resource which is created automatically and owned by the rollout: $ kubectl get trafficsplit NAME SERVICE rollouts-demo rollouts-demo-stable When inspecting the generated TrafficSplit resource, the weights are automatically configured to send 100% traffic to the rollouts-demo-stable service, and 0% traffic to the rollouts-demo-canary . These values will be updated during an update. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"0\" - service : rollouts-demo-stable weight : \"100\" service : rollouts-demo-stable 2. Perform an update \u00b6 Now perform an update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. When inspecting the TrafficSplit generated by the controller, we see that the weight has been updated to reflect the current setWeight: 5 step of the canary deploy. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"5\" - service : rollouts-demo-stable weight : \"95\" service : rollouts-demo-stable As the Rollout progresses through steps, the weights in the TrafficSplit resource will be adjusted to match the current setWeight of the steps.","title":"SMI"},{"location":"getting-started/smi/#getting-started-smi-service-mesh-interface","text":"Important Available since v0.9 This guide covers how Argo Rollouts integrates with the Service Mesh Interface (SMI), using Linkerd and NGINX Ingress Controller for traffic shaping. Since the SMI TrafficSplit resource is supported by multiple service mesh providers, the concepts taught here are applicable to other service mesh providers that support the interface. See the SMI Ecosystem for other projects that support SMI. This guide builds upon the concepts of the basic getting started guide .","title":"Getting Started - SMI (Service Mesh Interface)"},{"location":"getting-started/smi/#requirements","text":"Kubernetes cluster with Linkerd installed Kubernetes cluster with NGINX ingress controller installed and part of the mesh Tip See the environment setup guide for linkerd on how to setup a local minikube environment with linkerd and nginx.","title":"Requirements"},{"location":"getting-started/smi/#1-deploy-the-rollout-services-and-ingress","text":"When SMI is used as the traffic router, the Rollout canary strategy must define the following mandatory fields: apiVersion : argoproj.io/v1alpha1 kind : Rollout metadata : name : rollouts-demo spec : strategy : canary : # Reference to a Service which the controller will update to point to the canary ReplicaSet canaryService : rollouts-demo-canary # Reference to a Service which the controller will update to point to the stable ReplicaSet stableService : rollouts-demo-stable trafficRouting : smi : {} Run the following commands to deploy: A Rollout with the Linkerd linkerd.io/inject: enabled annotation Two Services (stable and canary) An Ingress kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/smi/rollout.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/smi/services.yaml kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/smi/ingress.yaml After applying the manifests you should see the following rollout, services, and ingress resources in the cluster: $ kubectl get ro NAME DESIRED CURRENT UP-TO-DATE AVAILABLE rollouts-demo 1 2 1 2 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE rollouts-demo-canary ClusterIP 10 .111.69.188 <none> 80 /TCP 23m rollouts-demo-stable ClusterIP 10 .109.175.248 <none> 80 /TCP 23m $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE rollouts-demo-stable <none> rollouts-demo.local 192 .168.64.2 80 23m You should also see a TrafficSplit resource which is created automatically and owned by the rollout: $ kubectl get trafficsplit NAME SERVICE rollouts-demo rollouts-demo-stable When inspecting the generated TrafficSplit resource, the weights are automatically configured to send 100% traffic to the rollouts-demo-stable service, and 0% traffic to the rollouts-demo-canary . These values will be updated during an update. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"0\" - service : rollouts-demo-stable weight : \"100\" service : rollouts-demo-stable","title":"1. Deploy the Rollout, Services, and Ingress"},{"location":"getting-started/smi/#2-perform-an-update","text":"Now perform an update the rollout by changing the image, and wait for it to reached the paused state. kubectl argo rollouts set image rollouts-demo rollouts-demo = argoproj/rollouts-demo:yellow kubectl argo rollouts get rollout rollouts-demo At this point, both the canary and stable version of the Rollout are running, with 5% of the traffic directed to the canary. When inspecting the TrafficSplit generated by the controller, we see that the weight has been updated to reflect the current setWeight: 5 step of the canary deploy. apiVersion : split.smi-spec.io/v1alpha1 kind : TrafficSplit metadata : name : rollouts-demo namespace : default spec : backends : - service : rollouts-demo-canary weight : \"5\" - service : rollouts-demo-stable weight : \"95\" service : rollouts-demo-stable As the Rollout progresses through steps, the weights in the TrafficSplit resource will be adjusted to match the current setWeight of the steps.","title":"2. Perform an update"}]}